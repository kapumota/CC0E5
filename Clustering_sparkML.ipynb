{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clustering usando SparkML**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color: red'>El propósito de este cuaderno es mostrarte cómo usar SparkML para agrupar datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conjuntos de datos**\n",
    "\n",
    "En este cuaderno utilizaremos los siguientes conjuntos de datos:\n",
    "\n",
    " - Versión modificada del conjunto de datos de clientes mayoristas. Conjunto de datos original disponible en https://archive.ics.uci.edu/ml/datasets/Wholesale+customers \n",
    " - Conjunto de datos de semillas. Disponible en https://archive.ics.uci.edu/ml/datasets/seeds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuración**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este cuaderno, utilizaremos las siguientes librerías:\n",
    "\n",
    "*   [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01) para conectarse al clúster de Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instalación de librerías requeridas**\n",
    "\n",
    "Necesitaremos las librerías como pyspark y findspark para trabajar este cuaderno.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes librerías requeridas __no__ están preinstaladas. Necesitare,ps ejecutar la siguiente celda__ para instalarlas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importando librerías requeridas**\n",
    "\n",
    "_Recomendamos importar todas las bibliotecas requeridas en un solo lugar (aquí):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También puedes usar esta sección para suprimir advertencias generadas por tu código:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifica el proceso de uso de Apache Spark con Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Importar funciones y clases para SparkML\n",
    "\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans, GaussianMixture\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Crea una sesión de Spark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejemplos**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 1 - Crear una sesión de Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una sesión de Spark\n",
    "# Ignora cualquier advertencia generada por el comando SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Clustering usando SparkML\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 2 - Cargar los datos de un archivo csv en un dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga el archivo de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/customers.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga el conjunto de datos en el dataframe de Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la función spark.read.csv, cargamos los datos en un DataFrame.\n",
    "# El parámetro header=True indica que la primera fila del archivo CSV contiene los nombres de las columnas.\n",
    "# El parámetro inferSchema=True le indica a Spark que detecte automáticamente los tipos de datos de las columnas.\n",
    "\n",
    "# Cargar el conjunto de datos de clientes\n",
    "customer_data = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el esquema del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cada fila en este conjunto de datos representa un cliente. Las columnas indican los pedidos realizados por un cliente de:\n",
    "# Alimentos frescos (Fresh_food), Leche (Milk), Abarrotes (Grocery) y Comida congelada (Frozen_Food).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra las primeras 5 filas del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 3 - Crear un vector de características\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensambla las características en una única columna de vector\n",
    "feature_cols = ['Fresh_Food', 'Milk', 'Grocery', 'Frozen_Food']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transforma los datos del cliente para incluir la nueva columna de características\n",
    "customer_transformed_data = assembler.transform(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debes indicarle al algoritmo KMeans cuántos clusters crear a partir de tus datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 4 - Crear un modelo de clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un modelo de clustering KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(k = number_of_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrena/ajusta el modelo en el conjunto de datos<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = kmeans.fit(customer_transformed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea 5 - Imprimir detalles de los clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu modelo ya está entrenado. Es hora de evaluar el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza predicciones en el conjunto de datos\n",
    "predictions = modelo.transform(customer_transformed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra el resultado\n",
    "predictions.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra cuántos clientes hay en cada cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.groupBy('prediction').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paradigma Map-Reduce básico para clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "import random\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Parámetros generales\n",
    "k = number_of_clusters         # número de clusters deseados\n",
    "max_iters = 20                 # máximo de iteraciones\n",
    "tol = 1e-6                     # tolerancia para convergencia\n",
    "\n",
    "# 1) RDD de puntos: cada elemento es un vector\n",
    "points_rdd = customer_transformed_data.rdd.map(lambda row: row.features)\n",
    "\n",
    "# Funciones auxiliares\n",
    "def inicializar_centroides(rdd, k, seed=42):\n",
    "    \"\"\"\n",
    "    Inicializa k centroides escogiendo k muestras aleatorias del RDD de vectores.\n",
    "    \"\"\"\n",
    "    sample = rdd.takeSample(False, k, seed)\n",
    "    return [Vectors.dense(vec.toArray()) for vec in sample]\n",
    "\n",
    "def convergieron(old_centroids, new_centroids, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Comprueba si los centroides han dejado de moverse más allá de la tolerancia.\n",
    "    \"\"\"\n",
    "    distances = [float(old_centroids[i].squared_distance(new_centroids[i]))\n",
    "                 for i in range(len(old_centroids))]\n",
    "    return all(d <= tol for d in distances)\n",
    "\n",
    "def map_assign(point, centroids):\n",
    "    \"\"\"\n",
    "    Map: asigna un punto al centroide más cercano.\n",
    "    Devuelve (cluster_id, (vector, 1))\n",
    "    \"\"\"\n",
    "    idx = min(range(len(centroids)),\n",
    "              key=lambda i: float(point.squared_distance(centroids[i])))\n",
    "    return (idx, (point, 1))\n",
    "\n",
    "def reduce_avg(a, b):\n",
    "    \"\"\"\n",
    "    Reduce: suma vectores y cuentas.\n",
    "    a, b = (suma_vectores, cuenta)\n",
    "    \"\"\"\n",
    "    sum_a, cnt_a = a\n",
    "    sum_b, cnt_b = b\n",
    "    return (sum_a + sum_b, cnt_a + cnt_b)\n",
    "\n",
    "# 2) Inicializa centroides\n",
    "centroids = inicializar_centroides(points_rdd, k, seed=42)\n",
    "\n",
    "# 3) Bucle Map–Reduce\n",
    "for i in range(max_iters):\n",
    "    bc_centroids = sc.broadcast(centroids)\n",
    "\n",
    "    # Map\n",
    "    assigned = points_rdd.map(lambda p: map_assign(p, bc_centroids.value))\n",
    "\n",
    "    # Reduce\n",
    "    merged = assigned.reduceByKey(reduce_avg)\n",
    "\n",
    "    # Calcula nuevos centroides\n",
    "    new_centroids = (\n",
    "        merged\n",
    "        .mapValues(lambda x: x[0] / x[1])\n",
    "        .sortByKey()\n",
    "        .map(lambda x: x[1])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # Comprueba convergencia\n",
    "    if convergieron(centroids, new_centroids, tol):\n",
    "        print(f\"Convergió en la iteración {i}\")\n",
    "        centroids = new_centroids\n",
    "        break\n",
    "\n",
    "    centroids = new_centroids\n",
    "else:\n",
    "    print(\"No convergió tras el número máximo de iteraciones.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Map–Reduce K-Means en PySpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Asumimos centroids, map_assign y merge_counts ya definidos \n",
    "\n",
    "# 1) RDD de vectores\n",
    "points_rdd = customer_transformed_data.rdd.map(lambda row: row.features)\n",
    "\n",
    "# 2) Inicialización (ya en bloque 1)\n",
    "centroids = inicializar_centroides(points_rdd, k, seed=42)\n",
    "\n",
    "# 3) Iteraciones de K-Means\n",
    "for iteration in range(max_iters):\n",
    "    bc_centroids = sc.broadcast(centroids)\n",
    "\n",
    "    # a) Asigna cada punto a su cluster\n",
    "    clusters = points_rdd.map(lambda p: map_assign(p, bc_centroids.value))\n",
    "\n",
    "    # b) Reduce para sumar vectores y contar\n",
    "    stats = clusters.reduceByKey(reduce_avg)\n",
    "\n",
    "    # c) Calcula nuevos centroides\n",
    "    new_centroids = (\n",
    "        stats\n",
    "        .mapValues(lambda x: x[0] / x[1])\n",
    "        .sortByKey()\n",
    "        .map(lambda x: x[1])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # d) Verifica convergencia\n",
    "    if convergieron(centroids, new_centroids, tol):\n",
    "        print(f\"K-Means convergió en iteración {iteration}\")\n",
    "        centroids = new_centroids\n",
    "        break\n",
    "\n",
    "    centroids = new_centroids\n",
    "else:\n",
    "    print(\"K-Means alcanzó el máximo de iteraciones sin converger.\")\n",
    "\n",
    "# 4) Crea DataFrame de predicciones\n",
    "predictions_rdd = customer_transformed_data.rdd.map(\n",
    "    lambda row: Row(**row.asDict(),\n",
    "                    prediction=min(\n",
    "                        range(len(centroids)),\n",
    "                        key=lambda idx: float(row.features.squared_distance(centroids[idx]))\n",
    "                    ))\n",
    ")\n",
    "predictions_df = spark.createDataFrame(predictions_rdd)\n",
    "predictions_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Canopy Clustering (preprocesamiento para K-Means)**\n",
    "\n",
    "La idea de **Canopy Clustering** es agrupar rápidamente los puntos en \"canopies\" usando dos umbrales $T_1 > T_2$. Estos canopies luego se usan para inicializar centroides de K-Means de forma más efectiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "import math\n",
    "\n",
    "# Umbrales (ajustar según los datos)\n",
    "T1 = 5.0\n",
    "T2 = 3.0\n",
    "\n",
    "def canopy_step(points_rdd, T1, T2):\n",
    "    \"\"\"\n",
    "    Ejecuta una pasada de canopy clustering sobre el RDD de vectores.\n",
    "    Devuelve lista de canopies: [(centroid, [puntos_asignados]), ...]\n",
    "    \"\"\"\n",
    "    unassigned = points_rdd.collect()  # para ilustración; en datos muy grandes harías sampling\n",
    "    canopies = []\n",
    "\n",
    "    while unassigned:\n",
    "        # Elige un punto aleatorio como centro de canopy\n",
    "        c = unassigned.pop()\n",
    "        canopy = [c]\n",
    "        to_remove = []\n",
    "\n",
    "        for p in unassigned:\n",
    "            dist = float(p.squared_distance(c))**0.5\n",
    "            if dist < T1:\n",
    "                canopy.append(p)\n",
    "            if dist < T2:\n",
    "                to_remove.append(p)\n",
    "\n",
    "        # Elimina de unassigned los puntos con dist < T2\n",
    "        unassigned = [p for p in unassigned if p not in to_remove]\n",
    "        canopies.append((c, canopy))\n",
    "\n",
    "    return canopies\n",
    "\n",
    "# 1) RDD de puntos\n",
    "points_rdd = customer_transformed_data.rdd.map(lambda r: r.features)\n",
    "\n",
    "# 2) Canopy clustering\n",
    "canopies = canopy_step(points_rdd, T1, T2)\n",
    "\n",
    "# 3) Inicializar centroides de K-Means tomando uno por canopy\n",
    "initial_centroids = [canopy[0] for canopy in canopies[:k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MapReduce K-Means con canopy**\n",
    "\n",
    "Aprovechando los centroides iniciales de los canopies, lanzamos el bucle MapReduce de K-Means distribuido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Funciones map_assign, reduce_avg y convergieron definidas como antes...\n",
    "\n",
    "# 1) Broadcast de centroides iniciales\n",
    "centroids = initial_centroids\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    bc = sc.broadcast(centroids)\n",
    "\n",
    "    # Map: asigna cada punto al centroide más cercano\n",
    "    assignments = points_rdd.map(lambda p: map_assign(p, bc.value))\n",
    "\n",
    "    # Reduce: suma vectores y cuentas\n",
    "    stats = assignments.reduceByKey(reduce_avg)\n",
    "\n",
    "    # Computa nuevos centroides\n",
    "    new_centroids = (\n",
    "        stats\n",
    "        .mapValues(lambda x: x[0] / x[1])\n",
    "        .sortByKey()\n",
    "        .map(lambda x: x[1])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    if convergieron(centroids, new_centroids, tol):\n",
    "        print(f\"Convergió en iteración {iteration}\")\n",
    "        centroids = new_centroids\n",
    "        break\n",
    "\n",
    "    centroids = new_centroids\n",
    "else:\n",
    "    print(\"No convergió tras el máximo de iteraciones.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **K-Means con DataFrame y `mapPartitions` (paralelismo más fino)**\n",
    "\n",
    "En lugar de enviar cada punto individualmente al executor, podemos procesarlos por partición para reducir overhead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# 1) UDF para asignar un punto a su cluster usando broadcast\n",
    "@udf(IntegerType())\n",
    "def predict_cluster(features):\n",
    "    return int(min(\n",
    "        range(len(bc_cent.value)),\n",
    "        key=lambda i: float(features.squared_distance(bc_cent.value[i]))\n",
    "    ))\n",
    "\n",
    "# 2) Bucle MapPartitions con assign_partition redefinido para captar bc_cent\n",
    "for iteration in range(max_iters):\n",
    "    bc_cent = sc.broadcast(centroids)\n",
    "\n",
    "    # Redefinimos aquí la función para que capture bc_cent en su closure\n",
    "    def assign_partition(iterator):\n",
    "        cents = bc_cent.value\n",
    "        for v in iterator:\n",
    "            idx = min(range(len(cents)),\n",
    "                      key=lambda i: float(v.squared_distance(cents[i])))\n",
    "            yield (idx, (v, 1))\n",
    "\n",
    "    # mapPartitions recibe un iterador de vectores; usamos la versión local de assign_partition\n",
    "    mapped = points_rdd.mapPartitions(assign_partition)\n",
    "    stats = mapped.reduceByKey(reduce_avg)\n",
    "\n",
    "    new_centroids = (\n",
    "        stats\n",
    "        .mapValues(lambda x: x[0] / x[1])\n",
    "        .sortByKey()\n",
    "        .map(lambda x: x[1])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    if iteration > 0 and convergieron(centroids, new_centroids, tol):\n",
    "        print(f\"K-Means (mapPartitions) convergió en iteración {iteration}\")\n",
    "        centroids = new_centroids\n",
    "        break\n",
    "\n",
    "    centroids = new_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se detiene la sesión de spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1 - Crear una sesión de Spark**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea una sesión de Spark con el nombre de la aplicación \"Seed Clustering\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para una pista</summary>\n",
    "    \n",
    "Utiliza SparkSession.builder\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para ver la solución</summary>\n",
    "    \n",
    "```python\n",
    "spark = SparkSession.builder.appName(\"Seed Clustering\").getOrCreate()\n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2 - Cargar los datos de un archivo csv en un dataframe**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download seed dataset\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/seeds.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga el conjunto de datos de semillas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed_data =  #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para una pista</summary>\n",
    "    \n",
    "Utiliza spark.read.csv\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para ver la solución</summary>\n",
    "    \n",
    "```python\n",
    "seed_data = spark.read.csv(\"seeds.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime el esquema del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra las primeras 5 filas del conjunto de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_data.show(n=5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 3 - Crear un vector de características**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensambla todas las columnas en un solo vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols =  #TODO\n",
    "assembler =  #TODO\n",
    "seed_transformed_data =  #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para una pista</summary>\n",
    "    \n",
    "Consulta la tarea 3\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para ver la solución</summary>\n",
    "    \n",
    "```python\n",
    "feature_cols = ['area',\n",
    " 'perimeter',\n",
    " 'compactness',\n",
    " 'length of kernel',\n",
    " 'width of kernel',\n",
    " 'asymmetry coefficient',\n",
    " 'length of kernel groove']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "seed_transformed_data = assembler.transform(seed_data)\n",
    "\n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 4 - Crear un modelo de clustering**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea 7 clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters =  #TODO\n",
    "kmeans =  #TODO\n",
    "model =  #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para una pista</summary>\n",
    "    \n",
    "utiliza el método kmeans.fit()\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para ver la solución</summary>\n",
    "    \n",
    "```python\n",
    "number_of_clusters = 3\n",
    "kmeans = KMeans(k = number_of_clusters)\n",
    "model = kmeans.fit(seed_transformed_data)\n",
    "\n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ejercicio 5 - Imprimir detalles de los clusters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para una pista</summary>\n",
    "    \n",
    "utiliza el método transform()\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Haz clic aquí para ver la solución</summary>\n",
    "    \n",
    "```python\n",
    "predictions = model.transform(seed_transformed_data)\n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(n=5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Te animamos a crear diferentes números de clusters utilizando el mismo conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicios adicionales**\n",
    "\n",
    "1. **Diferencias de paradigma**\n",
    "\n",
    "   * Explica las principales diferencias entre un algoritmo de clustering paralelo (que aprovecha múltiples núcleos/procesos) y uno distribuido (que reparte datos y computación en varios nodos). ¿Qué ventajas y desafíos presenta cada enfoque?\n",
    "\n",
    "2. **Arquitectura de Spark**\n",
    "\n",
    "   * Describe cómo Spark gestiona RDDs y DataFrames en un clúster. ¿Qué papel juegan el *driver* y los *executors*, y cómo influye esto en la ejecución de un algoritmo de clustering?\n",
    "\n",
    "3. **Inicialización de centroides**\n",
    "\n",
    "   * En el enfoque Map-Reduce manual, ¿qué implicaciones tiene la estrategia de muestreo para inicializar centroides en datos muy grandes? Propón al menos dos alternativas y discute sus pros y contras.\n",
    "\n",
    "4. **Balanceo de carga**\n",
    "\n",
    "   * ¿Cómo podrías evaluar y mejorar el balanceo de carga entre particiones al ejecutar el bucle Map-Reduce de K-Means? Describe métricas y técnicas para detectar y corregir particiones desiguales.\n",
    "\n",
    "5. **Criterio de convergencia**\n",
    "\n",
    "   * El script usa tolerancia en la distancia cuadrática. Plantea un criterio de convergencia alternativo (por ejemplo, cambios en inercia global) y discute su viabilidad en un entorno distribuido.\n",
    "\n",
    "\n",
    "6. **Selección de umbrales**\n",
    "\n",
    "   * ¿Cómo influye la elección de los umbrales $T_1$ y $T_2$ en la formación de canopies? Diseña un protocolo experimental para ajustarlos automáticamente en un dataset desconocido.\n",
    "\n",
    "7. **Eficiencia y escalabilidad**\n",
    "\n",
    "   * El método `canopy_step` recoge todos los puntos en el driver. Propón un esquema que permita ejecutar canopy clustering de forma distribuida (sin `collect()`) y describe cómo sincronizar los centros de canopy.\n",
    "\n",
    "8. **Impacto en la calidad final**\n",
    "\n",
    "   * Diseña un experimento para comparar K-Means con inicialización aleatoria vs. K-Means con inicialización por canopies. ¿Qué métricas usarías y cómo interpretarías los resultados?\n",
    "\n",
    "\n",
    "9. **MapPartitions para reducir overhead**\n",
    "\n",
    "   * Explica cómo el uso de `mapPartitions` mejora la eficiencia con respecto a una asignación punto a punto. ¿En qué escenarios podría no ser beneficioso?\n",
    "\n",
    "10. **Streaming K-Means**\n",
    "\n",
    "    * El script importa `StreamingKMeans` pero no lo implementa. Describe cómo organizarías un pipeline de streaming (p. ej., datos de sensores) para actualizar los centroides en tiempo real. ¿Qué parámetros del modelo son críticos?\n",
    "\n",
    "11. **Clustering jerárquico y Mezcla Gaussiana**\n",
    "\n",
    "    * Además de K-Means, el script menciona `BisectingKMeans` y `GaussianMixture`. Para cada uno, plantea un caso de uso donde sea preferible sobre K-Means, y qué retos de paralelización o distribución conlleva.\n",
    "\n",
    "12. **Evaluación en Spark**\n",
    "\n",
    "    * Describe cómo calcularías métricas de calidad de clusters (Silhouette, Davies–Bouldin, etc.) en Spark de forma eficiente en un clúster, minimizando movimientos de datos.\n",
    "\n",
    "13. **Tolerancia a fallos**\n",
    "\n",
    "    * Imagina que un executor falla durante la fase de Map-Reduce. ¿Cómo maneja Spark esta situación y qué consideraciones de diseño debes tener para asegurar la integridad de tu clustering iterativo?\n",
    "\n",
    "14. **Benchmarking de rendimiento**\n",
    "\n",
    "    * Diseña un plan de benchmarking para comparar el rendimiento de las distintas implementaciones (SparkML vs. Map-Reduce manual vs. mapPartitions). Incluye tamaño de datos, número de particiones, recursos del clúster y métricas de tiempo/uso de CPU/memoria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "prev_pub_hash": "c863e5c6ade435b8be242605a8af91626c62366008224b92d7b875449bcd8516"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
