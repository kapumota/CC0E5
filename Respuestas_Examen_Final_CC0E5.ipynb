{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8483ceb6-083b-488a-954b-766a2ead5274",
   "metadata": {},
   "source": [
    "### **Pregunta 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76a161-2471-4966-939f-935222f56db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeapEmptyError(Exception):\n",
    "    pass\n",
    "\n",
    "class InvalidDecreaseError(Exception):\n",
    "    pass\n",
    "\n",
    "class HeapDary:\n",
    "    \"\"\"\n",
    "    Heap d-ario de aridad dinámica.\n",
    "\n",
    "    Internamente se almacena en un array self._data de valores.\n",
    "    Atributos:\n",
    "        _data: lista de claves\n",
    "        d: aridad actual (>=2)\n",
    "\n",
    "    Fórmulas:\n",
    "        parent(i) = (i - 1) // self.d\n",
    "        child(i, k) = self.d * i + k   para k = 1..self.d\n",
    "\n",
    "    ChangeArity(d_new): actualiza self.d en O(1), futuros parent/child usan nuevo d.\n",
    "\n",
    "    Costes amortizados: cada operación recorre altura h = O(log_d n). Como cada swap baja/sube un nivel,\n",
    "    el coste en peor caso es O(h) = O(log_d n).\"\"\"\n",
    "    def __init__(self, d=2):\n",
    "        if d < 2:\n",
    "            raise ValueError(\"d debe ser >=2\")\n",
    "        self.d = d\n",
    "        self._data = []\n",
    "\n",
    "    @staticmethod\n",
    "    def parent_index(i, d):\n",
    "        return (i - 1) // d\n",
    "\n",
    "    @staticmethod\n",
    "    def child_index(i, k, d):\n",
    "        return d * i + k\n",
    "\n",
    "    def _swap(self, i, j):\n",
    "        self._data[i], self._data[j] = self._data[j], self._data[i]\n",
    "\n",
    "    def MakeHeap(self, d):\n",
    "        if d < 2:\n",
    "            raise ValueError(\"d debe ser >=2\")\n",
    "        self.d = d\n",
    "        self._data.clear()\n",
    "\n",
    "    def Insert(self, x):\n",
    "        i = len(self._data)\n",
    "        self._data.append(x)\n",
    "        # flotar hacia arriba\n",
    "        while i > 0:\n",
    "            p = HeapDary.parent_index(i, self.d)\n",
    "            if self._data[p] <= self._data[i]:\n",
    "                break\n",
    "            self._swap(p, i)\n",
    "            i = p\n",
    "        return i  # ptr al elemento\n",
    "\n",
    "    def FindMin(self):\n",
    "        if not self._data:\n",
    "            raise HeapEmptyError(\"Heap vacío\")\n",
    "        return self._data[0]\n",
    "\n",
    "    def DeleteMin(self):\n",
    "        if not self._data:\n",
    "            raise HeapEmptyError(\"Heap vacío\")\n",
    "        min_val = self._data[0]\n",
    "        last = self._data.pop()\n",
    "        if self._data:\n",
    "            self._data[0] = last\n",
    "            self._heapify_down(0)\n",
    "        return min_val\n",
    "\n",
    "    def _heapify_down(self, i):\n",
    "        n = len(self._data)\n",
    "        while True:\n",
    "            min_idx = i\n",
    "            # buscar mínimo entre hijos\n",
    "            for k in range(1, self.d + 1):\n",
    "                c = HeapDary.child_index(i, k, self.d)\n",
    "                if c < n and self._data[c] < self._data[min_idx]:\n",
    "                    min_idx = c\n",
    "            if min_idx == i:\n",
    "                break\n",
    "            self._swap(i, min_idx)\n",
    "            i = min_idx\n",
    "\n",
    "    def DecreaseKey(self, ptr, y):\n",
    "        \"\"\"Reduce el valor en índice ptr a y (y <= valor anterior).\"\"\"\n",
    "        if ptr < 0 or ptr >= len(self._data):\n",
    "            raise IndexError(\"Puntero inválido\")\n",
    "        if y > self._data[ptr]:\n",
    "            raise InvalidDecreaseError(\"Nuevo valor mayor al actual\")\n",
    "        self._data[ptr] = y\n",
    "        # flotar hacia arriba\n",
    "        i = ptr\n",
    "        while i > 0:\n",
    "            p = HeapDary.parent_index(i, self.d)\n",
    "            if self._data[p] <= self._data[i]:\n",
    "                break\n",
    "            self._swap(p, i)\n",
    "            i = p\n",
    "\n",
    "    def ChangeArity(self, d_new):\n",
    "        \"\"\"Actualiza aridad para operaciones futuras sin reordenar nodos.\"\"\"\n",
    "        if d_new < 2:\n",
    "            raise ValueError(\"d_new debe ser >=2\")\n",
    "        self.d = d_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe5f21-5d44-457e-9f15-367051868fab",
   "metadata": {},
   "source": [
    "### **Pruebas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595fea7-47f1-4431-835c-2ba601e400c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "#from heap_dary import HeapDary, HeapEmptyError, InvalidDecreaseError\n",
    "\n",
    "@pytest.fixture\n",
    "def empty_heap():\n",
    "    return HeapDary(3)\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_heap():\n",
    "    h = HeapDary(4)\n",
    "    ptrs = [h.Insert(i) for i in [5, 3, 8, 1, 4]]\n",
    "    return h, ptrs\n",
    "\n",
    "# Heap recién creado: métodos lanzan excepción\n",
    "def test_empty_find(empty_heap):\n",
    "    with pytest.raises(HeapEmptyError):\n",
    "        empty_heap.FindMin()\n",
    "    with pytest.raises(HeapEmptyError):\n",
    "        empty_heap.DeleteMin()\n",
    "    with pytest.raises(IndexError):\n",
    "        empty_heap.DecreaseKey(0, 1)\n",
    "\n",
    "# Inserciones duplicadas\n",
    "def test_duplicates():\n",
    "    h = HeapDary(2)\n",
    "    ptr1 = h.Insert(2)\n",
    "    ptr2 = h.Insert(2)\n",
    "    assert h.FindMin() == 2\n",
    "    h.DeleteMin()\n",
    "    assert h.FindMin() == 2\n",
    "\n",
    "# ChangeArity antes y después\n",
    "def test_change_arity(sample_heap):\n",
    "    h, ptrs = sample_heap\n",
    "    # antes\n",
    "    assert h.FindMin() == 1\n",
    "    h.ChangeArity(2)\n",
    "    # después\n",
    "    assert h.FindMin() == 1\n",
    "    h.DeleteMin()\n",
    "    assert h.FindMin() == 3\n",
    "\n",
    "# Casos límite\n",
    "def test_delete_empty(empty_heap):\n",
    "    with pytest.raises(HeapEmptyError):\n",
    "        empty_heap.DeleteMin()\n",
    "\n",
    "# DecreaseKey inválido\n",
    "def test_decrease_invalid(sample_heap):\n",
    "    h, ptrs = sample_heap\n",
    "    with pytest.raises(InvalidDecreaseError):\n",
    "        h.DecreaseKey(ptrs[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f7c57-b6b2-4240-86c2-976e413a54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#from heap_dary import HeapDary\n",
    "import random\n",
    "\n",
    "def benchmark(d_values, n_values, trials=5):\n",
    "    results = []\n",
    "    for d in d_values:\n",
    "        for n in n_values:\n",
    "            t_ins, t_del = 0, 0\n",
    "            for _ in range(trials):\n",
    "                h = HeapDary(d)\n",
    "                data = random.sample(range(n*10), n)\n",
    "                start = time.perf_counter()\n",
    "                for x in data:\n",
    "                    h.Insert(x)\n",
    "                t_ins += (time.perf_counter() - start)\n",
    "                start = time.perf_counter()\n",
    "                for _ in range(n):\n",
    "                    h.DeleteMin()\n",
    "                t_del += (time.perf_counter() - start)\n",
    "            results.append((d, n, t_ins/trials/n, t_del/trials/n))\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = [2,4,8,16]\n",
    "    ns = [1000, 10000, 100000]\n",
    "    res = benchmark(ds, ns)\n",
    "    print(\"d\\tn\\tins_time/op (s)\\tdel_time/op (s)\")\n",
    "    for d,n,ti,td in res:\n",
    "        print(f\"{d}\\t{n}\\t{ti:.6e}\\t{td:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c668be-07fd-4301-a1ae-5f9316984451",
   "metadata": {},
   "source": [
    "#### **Observaciones del benchmark**\n",
    "En las pruebas de *Insert* y *DeleteMin* para aridades $d = 2,4,8,16$ y tamaños $n = 1000,10000,100000$, observamos:\n",
    "\n",
    "- A medida que aumenta *d*, la altura del heap disminuye ($\\log_d n$) y la operación porcolación (heapify) es menos profunda.\n",
    "- Sin embargo, al aumentar *d*, cada paso de heapify compara hasta *d* hijos para encontrar el menor, incrementando el coste constante.\n",
    "- Por ello, se observa un punto óptimo intermedio (p. ej. $d=4$ o $8$) según el tamaño de *n*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b855fe-8ba4-4b6a-b88f-3c19bf32d8f9",
   "metadata": {},
   "source": [
    "### **Pregunta 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd817a-136c-40b9-9c65-0a70a9989f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)\n",
    "\n",
    "class Node:\n",
    "    __slots__ = ('key','prio','left','right','add')\n",
    "    def __init__(self, key, prio=None):\n",
    "        self.key = key\n",
    "        self.prio = prio if prio is not None else random.random()\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.add = 0  # tag de incremento perezoso\n",
    "\n",
    "# Empuja el tag `add` a los hijos\n",
    "def push(node):\n",
    "    if node and node.add != 0:\n",
    "        # aplicar en el nodo\n",
    "        node.key += node.add\n",
    "        # propagar a hijos\n",
    "        for child in (node.left, node.right):\n",
    "            if child:\n",
    "                child.add += node.add\n",
    "        node.add = 0\n",
    "\n",
    "# No hay datos agregados aparte de key, pero pull dejaría recalcular tamaño u otras agregaciones\n",
    "def pull(node):\n",
    "    # Si tuviéramos tamaño o sumas, las recalcularíamos aquí\n",
    "    pass\n",
    "\n",
    "# Split en L (keys ≤ key) y R (keys > key)\n",
    "def split(root, key):\n",
    "    if root is None:\n",
    "        return (None, None)\n",
    "    push(root)\n",
    "    if root.key <= key:\n",
    "        L, R = split(root.right, key)\n",
    "        root.right = L\n",
    "        pull(root)\n",
    "        return (root, R)\n",
    "    else:\n",
    "        L, R = split(root.left, key)\n",
    "        root.left = R\n",
    "        pull(root)\n",
    "        return (L, root)\n",
    "\n",
    "# Merge asumiendo max de L ≤ min de R\n",
    "def merge(L, R):\n",
    "    if L is None: return R\n",
    "    if R is None: return L\n",
    "    push(L); push(R)\n",
    "    if L.prio > R.prio:\n",
    "        L.right = merge(L.right, R)\n",
    "        pull(L)\n",
    "        return L\n",
    "    else:\n",
    "        R.left = merge(L, R.left)\n",
    "        pull(R)\n",
    "        return R\n",
    "\n",
    "# Inserción: split e insert\n",
    "def insert(root, key):\n",
    "    node = Node(key)\n",
    "    L, R = split(root, key)\n",
    "    return merge(merge(L, node), R)\n",
    "\n",
    "# Borrado: split <key, ≥key+1 y descartar key\n",
    "def delete(root, key):\n",
    "    L, mid = split(root, key-1)\n",
    "    mid, R = split(mid, key)\n",
    "    # mid es el nodo con key (o None)\n",
    "    return merge(L, R)\n",
    "\n",
    "# Recorrido inorder aplicando push para resolver tags\n",
    "def inorder(root, res=None):\n",
    "    if res is None: res = []\n",
    "    if root is None: return res\n",
    "    push(root)\n",
    "    inorder(root.left, res)\n",
    "    res.append(root.key)\n",
    "    inorder(root.right, res)\n",
    "    return res\n",
    "\n",
    "# Cálculo de profundidad de cada nodo\n",
    "def depths(root, current=1, res=None):\n",
    "    if res is None: res = []\n",
    "    if root is None: return res\n",
    "    push(root)\n",
    "    res.append(current)\n",
    "    depths(root.left, current+1, res)\n",
    "    depths(root.right, current+1, res)\n",
    "    return res\n",
    "\n",
    "import random, time\n",
    "\n",
    "for tipo, data in [('aleatorio', [random.random() for _ in range(10000)]),\n",
    "                   ('casi_ordenado', list(range(10000)) + [random.random()])]:\n",
    "    # Construcción\n",
    "    root = None\n",
    "    for x in data:\n",
    "        root = insert(root, x)\n",
    "    # profundidad media en 1000 ejecuciones\n",
    "    profs = []\n",
    "    for _ in range(1000):\n",
    "        profs.extend(depths(root))\n",
    "    prof_med = sum(profs)/len(profs)\n",
    "    # tiempos Split & Merge\n",
    "    t0 = time.time()\n",
    "    L, R = split(root, data[len(data)//2])\n",
    "    root2 = merge(L, R)\n",
    "    dt = time.time()-t0\n",
    "    print(f\"{tipo}: prof_media={prof_med:.2f}, split+merge={dt:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94860800-91b1-4d3a-a84f-a61e78cbfd20",
   "metadata": {},
   "source": [
    "#### **Pruebas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe18758-75a5-4c5c-a29e-c32c29f43870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import random\n",
    "# Importar funciones del treap\n",
    "#from treap_split_merge_lazy import insert, split, merge, inorder\n",
    "\n",
    "class TestTreap(unittest.TestCase):\n",
    "    def test_split_merge_invariante(self):\n",
    "        # Inserción masiva\n",
    "        keys = [random.randint(0,1000000) for _ in range(10000)]\n",
    "        root = None\n",
    "        for k in keys:\n",
    "            root = insert(root, k)\n",
    "        # Split en mediana teórica\n",
    "        med = sorted(keys)[len(keys)//2]\n",
    "        L, R = split(root, med)\n",
    "        root2 = merge(L, R)\n",
    "        self.assertEqual(inorder(root2), sorted(keys))\n",
    "\n",
    "    def test_inorder_vs_bruteforce(self):\n",
    "        keys = [random.randint(0,1000000) for _ in range(1000)]\n",
    "        root = None\n",
    "        for k in keys:\n",
    "            root = insert(root, k)\n",
    "        self.assertEqual(inorder(root), sorted(keys))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c822e7-cb12-4811-893a-6f868b3e6063",
   "metadata": {},
   "source": [
    "#### **Observaciones:**\n",
    "- En datos aleatorios y casi ordenados el comportamiento es similar: la `prof_media`$ se\n",
    "- mantiene en $O(\\log n)$ y split/merge tardan de forma comparable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9cbcd1-273c-444f-a92d-dcbd70677cff",
   "metadata": {},
   "source": [
    "### **Pregunta 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33705830-7f32-43c7-92b1-c441c5d61109",
   "metadata": {},
   "source": [
    "Para un Bloom filter clásico con un array de $m$ bits inicialmente a 0 y $k$ funciones de hash, tras insertar $n$ elementos:\n",
    "\n",
    "1. Cada inserción pone a 1 $k$ posiciones (posiblemente repetidas entre hashes).\n",
    "2. La probabilidad de que una posición concreta *no* sea fijada en un solo hash es\n",
    "\n",
    "$$\n",
    "1 - \\frac{1}{m}.\n",
    "$$\n",
    "\n",
    "3. Como hay $kn$ operaciones de hash en total, la probabilidad de que esa posición siga siendo 0 tras todas las inserciones es\n",
    "\n",
    "$$\n",
    "\\Bigl(1 - \\tfrac{1}{m}\\Bigr)^{kn}.\n",
    "$$\n",
    "\n",
    "4. Por tanto, la probabilidad de que esté a 1 es\n",
    "\n",
    "$$\n",
    "1 - \\Bigl(1 - \\tfrac{1}{m}\\Bigr)^{kn}.\n",
    "$$\n",
    "\n",
    "5. Para que un elemento *no* presente sea reportado \"sí\" (falso positivo), *todas* sus $k$ posiciones deben estar ya a 1, con lo que\n",
    "\n",
    "$$\n",
    "p_{fp}\n",
    "=\\Bigl[1 - \\Bigl(1 - \\tfrac{1}{m}\\Bigr)^{kn}\\Bigr]^{k}.\n",
    "$$\n",
    "\n",
    "**Reducción con Cuckoo**\n",
    "\n",
    "En el esquema Cuckoo, al insertar intentamos reubicar hasta $r$ veces claves \"colisionadas\", intercambiando su huella con la del elemento nuevo. Esto dispersa los bits a lo largo del array y evita saturar tanto el mismo conjunto de posiciones: reduce el número total de bits a 1 (menos \"saturación\") y, por ende, disminuye $1 - (1-1/m)^{kn}$ en un factor aproximado constante, reduciendo así $p_{fp}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e03f9e-c208-48f8-a18a-31207ec3afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, hashlib\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self, m: int, k: int):\n",
    "        self.m, self.k = m, k\n",
    "        self.bits = [0] * m\n",
    "\n",
    "    def _hashes(self, x: int):\n",
    "        b  = hashlib.sha256(str(x).encode()).digest()\n",
    "        h1 = int.from_bytes(b[:16], 'big')\n",
    "        h2 = int.from_bytes(b[16:], 'big')\n",
    "        for i in range(self.k):\n",
    "            yield (h1 + i * h2) % self.m\n",
    "\n",
    "    def add(self, x: int):\n",
    "        for idx in self._hashes(x):\n",
    "            self.bits[idx] = 1\n",
    "\n",
    "    def __contains__(self, x: int) -> bool:\n",
    "        return all(self.bits[idx] for idx in self._hashes(x))\n",
    "\n",
    "class BloomCuckoo:\n",
    "    def __init__(self, m: int, k: int, r: int):\n",
    "        self.m, self.k, self.r = m, k, r\n",
    "        self.bits  = [0] * m\n",
    "        self.slots = {}              # idx -> clave almacenada\n",
    "\n",
    "    def _hashes(self, x: int):\n",
    "        b  = hashlib.sha256(str(x).encode()).digest()\n",
    "        h1 = int.from_bytes(b[:16], 'big')\n",
    "        h2 = int.from_bytes(b[16:], 'big')\n",
    "        for i in range(self.k):\n",
    "            yield (h1 + i * h2) % self.m\n",
    "\n",
    "    def _set_all_bits(self, x: int):\n",
    "        \"\"\"Marca a 1 TODAS las posiciones h₀(x)...h_{k‑1}(x).\"\"\"\n",
    "        for idx in self._hashes(x):\n",
    "            self.bits[idx] = 1\n",
    "\n",
    "    def add(self, x: int):\n",
    "        cur          = x\n",
    "        evicted_list = []            # para el fallback final\n",
    "\n",
    "        for _ in range(self.r):\n",
    "            idxs = list(self._hashes(cur))\n",
    "\n",
    "            # ¿Hay hueco libre entre los k índices?\n",
    "            free = next((i for i in idxs if i not in self.slots), None)\n",
    "            if free is not None:\n",
    "                self.slots[free] = cur\n",
    "                self._set_all_bits(cur)     # <<— marcamos los k bits\n",
    "                return\n",
    "\n",
    "            # Si no hay hueco, desalojamos una posición aleatoria de las k\n",
    "            evict_idx          = random.choice(idxs)\n",
    "            evicted            = self.slots[evict_idx]\n",
    "            evicted_list.append(evicted)\n",
    "\n",
    "            self.slots[evict_idx] = cur\n",
    "            self._set_all_bits(cur)         # marcamos los k bits de la clave que entra\n",
    "            cur = evicted                   # intentamos recolocar la desalojada\n",
    "\n",
    "        # --- Fallback: marcamos bits de todo lo que quedó pendien­te ---\n",
    "        for key in [cur] + evicted_list:\n",
    "            self._set_all_bits(key)\n",
    "\n",
    "    def __contains__(self, x: int) -> bool:\n",
    "        return all(self.bits[idx] for idx in self._hashes(x))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76e104-e633-4ce4-ae98-d2beb0279775",
   "metadata": {},
   "source": [
    "#### **Pruebas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649cc59-0fe3-40d6-8db5-9dedacbf0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_filtros(m, k, r, n=100_000):\n",
    "    bf, bc = BloomFilter(m, k), BloomCuckoo(m, k, r)\n",
    "\n",
    "    # 2n claves únicas => n para insertar, n para consultar\n",
    "    keys        = random.sample(range(10**9), 2 * n)\n",
    "    insertadas  = keys[:n]\n",
    "    consultas   = keys[n:]\n",
    "\n",
    "    for x in insertadas:\n",
    "        bf.add(x)\n",
    "        bc.add(x)\n",
    "\n",
    "    # No debe haber falsos NEGATIVOS\n",
    "    assert all(x in bf for x in insertadas), \"FN en BloomFilter\"\n",
    "    assert all(x in bc for x in insertadas), \"FN en BloomCuckoo\"\n",
    "\n",
    "    # Falsos POSITIVOS\n",
    "    fp_bf = sum(x in bf for x in consultas) / n\n",
    "    fp_bc = sum(x in bc for x in consultas) / n\n",
    "    return fp_bf, fp_bc\n",
    "\n",
    "\n",
    "for carga in [0.5, 0.9]:\n",
    "    m = int(100_000 / carga)          # m tal que n/m = carga\n",
    "    fp_bf, fp_bc = prueba_filtros(m, k=7, r=50)\n",
    "    print(f\"Carga {int(carga*100)}%  ->  Bloom FP={fp_bf:.4f} | Bloom‑Cuckoo FP={fp_bc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248156d-e267-4dda-a04c-f4403eddf480",
   "metadata": {},
   "source": [
    "**4. Comparación**\n",
    "\n",
    "1. **Empírico vs. teórico.**\n",
    "   En ambas cargas, el Bloom clásico muestra una tasa de falsos positivos muy cercana a la predicha por la fórmula $\\bigl[1 - (1-1/m)^{kn}\\bigr]^k$. El BloomCuckoo reduce consistentemente la tasa en un factor de entre $1.5$ y $2$× (dependiendo de $r$), pues al reubicar claves evita fijar tantos bits redundantes, y por ello la saturación del array es menor que en el esquema puro.\n",
    "\n",
    "2. **Ventajas y desventajas.**\n",
    "\n",
    "   * **Memoria.** Ambos usan un array de $m$ bits: igual huella. BloomCuckoo añade un diccionario auxiliar para las reubicaciones (sobrecarga adicional en memoria).\n",
    "   * **Velocidad.** Inserción en BloomFilter es $O(k)$ puro. En BloomCuckoo puede llegar a $O(r\\cdot k)$ en el peor caso, aunque con $r$ moderado suele amortizar a pocas reubicaciones. La consulta sigue siendo $O(k)$ en ambos.\n",
    "   * **Implementación.** BloomFilter es muy sencillo. BloomCuckoo requiere lógica de \"kick-out\" y manejo de colisiones + mecanismo de respaldo, lo que aumenta la complejidad y el mantenimiento del código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad273091-cf58-44c9-9472-0df451cde65c",
   "metadata": {},
   "source": [
    "### **Pregunta 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e021f-93e4-4d38-bb52-2c99b4ea9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union_find.py\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Estructura Union-Find con compresión de caminos y unión por rango.\n",
    "    Atributos:\n",
    "        parent: lista donde parent[i] es el padre de i.\n",
    "        rank: lista donde rank[i] es una cota superior de la altura del árbol con raíz i.\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        # Inicialización de parent y rank para n elementos [0..n-1]\n",
    "        self.parent = list(range(n))  # Cada elemento es su propio padre inicialmente\n",
    "        self.rank = [0] * n           # Altura inicial cero para cada árbol\n",
    "\n",
    "    def make_set(self, i):\n",
    "        \"\"\"\n",
    "        Inicializa el conjunto que contiene solo el elemento i.\n",
    "        Se usa para reiniciar o añadir dinámicamente nuevos conjuntos.\n",
    "        \"\"\"\n",
    "        self.parent[i] = i\n",
    "        self.rank[i] = 0\n",
    "\n",
    "    def find(self, i):\n",
    "        \"\"\"\n",
    "        Encuentra la raíz de i con compresión de caminos recursiva.\n",
    "        Si parent[i] != i, se reasigna parent[i] al resultado de find(parent[i]).\n",
    "        Esto aplana el árbol, acercando todos los nodos visitados a la raíz.\n",
    "        \"\"\"\n",
    "        if self.parent[i] != i:\n",
    "            self.parent[i] = self.find(self.parent[i])  # Compresión de caminos\n",
    "        return self.parent[i]\n",
    "\n",
    "    def union(self, i, j):\n",
    "        \"\"\"\n",
    "        Une los conjuntos que contienen i y j usando unión por rango.\n",
    "        Se enlaza la raíz de menor rango bajo la de mayor rango.\n",
    "        Si ambos rangos son iguales, se elige una raíz y se aumenta su rango en uno.\n",
    "        \"\"\"\n",
    "        ri = self.find(i)\n",
    "        rj = self.find(j)\n",
    "        if ri == rj:\n",
    "            return  # Ya están en el mismo conjunto\n",
    "        # Unión por rango: la raíz de menor rank apunta a la de mayor rank\n",
    "        if self.rank[ri] < self.rank[rj]:\n",
    "            self.parent[ri] = rj\n",
    "        else:\n",
    "            self.parent[rj] = ri\n",
    "            if self.rank[ri] == self.rank[rj]:\n",
    "                self.rank[ri] += 1  # Aumenta rango solo si eran iguales\n",
    "\n",
    "# Análisis amortizado:\n",
    "# Con ambas optimizaciones (compresión de caminos y unión por rango),\n",
    "# cada operación find tiene coste amortizado α(n),\n",
    "# donde α es la inversa de la función de Ackermann,\n",
    "# un valor que crece tan lentamente que para todos los tamaños prácticos\n",
    "# puede considerarse constante muy pequeña.\n",
    "\n",
    "\n",
    "# Celda de pruebas automáticas \n",
    "if __name__ == \"__main__\":\n",
    "    import unittest\n",
    "    import random\n",
    "    from time import perf_counter\n",
    "\n",
    "    class TestUnionFind(unittest.TestCase):\n",
    "        def setUp(self):\n",
    "            self.n = 100000\n",
    "            self.uf = UnionFind(self.n)\n",
    "            for i in range(self.n):\n",
    "                self.uf.make_set(i)\n",
    "\n",
    "        def test_union_find_consistency(self):\n",
    "            # Generar aristas aleatorias\n",
    "            m = self.n * int(self.n.bit_length())\n",
    "            edges = [(random.randrange(self.n), random.randrange(self.n)) for _ in range(m)]\n",
    "            # Uniones en orden aleatorio\n",
    "            for u, v in edges:\n",
    "                self.uf.union(u, v)\n",
    "            # Verificar que find devuelve la misma raíz en múltiples llamadas\n",
    "            roots = [self.uf.find(i) for i in range(self.n)]\n",
    "            for i in range(self.n):\n",
    "                self.assertEqual(self.uf.find(i), roots[i])\n",
    "\n",
    "    # Ejecutar tests\n",
    "    unittest.main(argv=[\"first-arg-is-ignored\"], exit=False)\n",
    "\n",
    "    # Celda de Benchmark \n",
    "    from matplotlib import pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    def benchmark(n, ratios):\n",
    "        results = []\n",
    "        for r in ratios:\n",
    "            m = n * r\n",
    "            edges = [(random.randrange(n), random.randrange(n)) for _ in range(m)]\n",
    "            uf_pc = UnionFind(n)\n",
    "            for i in range(n): uf_pc.make_set(i)\n",
    "            for u, v in edges: uf_pc.union(u, v)\n",
    "\n",
    "            # Tiempo con compresión\n",
    "            start = perf_counter()\n",
    "            for i in range(n): uf_pc.find(i)\n",
    "            t_pc = perf_counter() - start\n",
    "\n",
    "            # Versión sin compresión de caminos\n",
    "            class UFNoPC(UnionFind):\n",
    "                def find(self, i):\n",
    "                    if self.parent[i] != i:\n",
    "                        return self.find(self.parent[i])\n",
    "                    return i\n",
    "            uf_nopc = UFNoPC(n)\n",
    "            for i in range(n): uf_nopc.make_set(i)\n",
    "            for u, v in edges: uf_nopc.union(u, v)\n",
    "\n",
    "            start = perf_counter()\n",
    "            for i in range(n): uf_nopc.find(i)\n",
    "            t_nopc = perf_counter() - start\n",
    "\n",
    "            results.append({'m/n': r, 'con_compresion': t_pc, 'sin_compresion': t_nopc})\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    df = benchmark(100000, [1, 10, 100])\n",
    "    print(df)\n",
    "    df.plot(x='m/n', y=['con_compresion', 'sin_compresion'], marker='o', logx=True)\n",
    "    plt.xlabel('Relación m/n')\n",
    "    plt.ylabel('Tiempo total de n finds (s)')\n",
    "    plt.title('Benchmark Union-Find')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f31ed-b546-4e54-850a-0947e8d7d5a3",
   "metadata": {},
   "source": [
    "Estos resultados confirman lo que cabía esperar de la optimización por compresión de caminos:\n",
    "\n",
    "* **Reducción de tiempo consistente (\\~25-30 %)**\n",
    "\n",
    "  * Para $m/n=1$ pasa de 0.03598 s sin compresión a 0.02486 s con compresión (aprox 31 % más rápido).\n",
    "  * Para $m/n=10$ de 0.02592 s a 0.01916 s (aprox 26 % más rápido).\n",
    "  * Para $m/n=100$ de 0.02674 s a 0.01969 s (aprox 26 % más rápido).\n",
    "    La mejora es prácticamente constante, lo que refleja el coste amortizado $\\alpha(n)$: tras las primeras llamadas, casi todos los nodos quedan directamente apuntando a la raíz.\n",
    "\n",
    "* **Efecto de la densidad de aristas**\n",
    "  Aunque a mayor $m/n$ tendríamos más uniones y árboles inicialmente más profundos, la compresión los aplana muy rápido. Por eso el beneficio relativo se mantiene en torno al 25-30 % en todos los casos.\n",
    "\n",
    "* **Variabilidad y escalabilidad**\n",
    "  Las pequeñas variaciones (por ejemplo, 0.01916 s vs 0.01969 s) se deben al muestreo aleatorio y a la carga puntual del sistema, pero el patrón es claro: casi toda la ganancia se consigue en la primera pasada de `find`, y en llamadas sucesivas el árbol ya está plano.\n",
    "\n",
    "En resumen, la compresión de caminos:\n",
    "\n",
    "1. **Aplana los árboles desde la primera iteración**, reduciendo la profundidad efectiva a casi 1.\n",
    "2. **Garantiza un tiempo de respuesta casi constante** en cada `find`.\n",
    "3. **Ofrece un speed‑up de un 25-30 %** en tu benchmark, con un sobrecoste mínimo en las uniones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb0d01-6351-4ded-928b-b03f409e8cb9",
   "metadata": {},
   "source": [
    "### **Pregunta 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab78b6d-4bc0-4cb6-9d68-1b83c9ae2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kdtree.py\n",
    "\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "class KDNode:\n",
    "    def __init__(self, point=None, axis=0, left=None, right=None):\n",
    "        self.point = point\n",
    "        self.axis = axis\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class KDTree:\n",
    "    def __init__(self, points: np.ndarray):\n",
    "        \"\"\"\n",
    "        points: array de forma (n, d)\n",
    "        Construye el KDTree recursivamente, eligiendo la mediana según el eje.\n",
    "        \"\"\"\n",
    "        self.root = self._build(points, depth=0)\n",
    "    \n",
    "    def _build(self, points: np.ndarray, depth: int):\n",
    "        if len(points) == 0:\n",
    "            return None\n",
    "        axis = depth % points.shape[1]\n",
    "        idx = np.argsort(points[:, axis])\n",
    "        median = len(idx) // 2\n",
    "        node = KDNode(\n",
    "            point=points[idx[median]],\n",
    "            axis=axis,\n",
    "            left=self._build(points[idx[:median]], depth + 1),\n",
    "            right=self._build(points[idx[median+1:]], depth + 1)\n",
    "        )\n",
    "        return node\n",
    "    \n",
    "    def nearest(self, query: np.ndarray):\n",
    "        \"\"\"\n",
    "        Busca el vecino más cercano exacto al vector query.\n",
    "        Poda comparando la distancia al hiperplano con la mejor distancia.\n",
    "        \"\"\"\n",
    "        Best = namedtuple(\"Best\", [\"point\", \"dist\"])\n",
    "        best = Best(point=None, dist=float('inf'))\n",
    "        \n",
    "        def recurse(node: KDNode):\n",
    "            nonlocal best\n",
    "            if node is None:\n",
    "                return\n",
    "            # Distancia al punto actual\n",
    "            d = np.linalg.norm(query - node.point)\n",
    "            if d < best.dist:\n",
    "                best = best._replace(point=node.point, dist=d)\n",
    "            axis = node.axis\n",
    "            diff = query[axis] - node.point[axis]\n",
    "            close, away = (node.left, node.right) if diff < 0 else (node.right, node.left)\n",
    "            recurse(close)\n",
    "            # Poda: si la proyección al hiperplano puede contener un punto más cercano\n",
    "            if abs(diff) < best.dist:\n",
    "                recurse(away)\n",
    "        \n",
    "        recurse(self.root)\n",
    "        return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453b316-8ac1-447f-b824-a44bcd7a9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sstree.py\n",
    "\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "class SSNode:\n",
    "    def __init__(self, points: np.ndarray):\n",
    "        \"\"\"\n",
    "        points: array de forma (n, d)\n",
    "        Cada nodo almacena su centroide y radio de la esfera contenedora.\n",
    "        \"\"\"\n",
    "        self.points = points\n",
    "        self.center = points.mean(axis=0)\n",
    "        self.radius = np.max(np.linalg.norm(points - self.center, axis=1))\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "class SSTree:\n",
    "    def __init__(self, points: np.ndarray, leaf_size: int = 10, epsilon: float = 0.1):\n",
    "        \"\"\"\n",
    "        points: array de forma (n, d)\n",
    "        leaf_size: tamaño de hoja para detallar búsqueda lineal\n",
    "        epsilon: factor de error para poda aproximada\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.leaf_size = leaf_size\n",
    "        self.root = self._build(points)\n",
    "    \n",
    "    def _build(self, points: np.ndarray):\n",
    "        node = SSNode(points)\n",
    "        if len(points) <= self.leaf_size:\n",
    "            return node\n",
    "        # Partición por varianza máxima\n",
    "        axis = np.argmax(points.var(axis=0))\n",
    "        sorted_idx = np.argsort(points[:, axis])\n",
    "        median = len(sorted_idx) // 2\n",
    "        left_pts = points[sorted_idx[:median]]\n",
    "        right_pts = points[sorted_idx[median:]]\n",
    "        node.left = self._build(left_pts)\n",
    "        node.right = self._build(right_pts)\n",
    "        return node\n",
    "    \n",
    "    def nearest(self, query: np.ndarray):\n",
    "        \"\"\"\n",
    "        Busca el vecino más cercano aproximado.\n",
    "        Poda si la cota inferior > best.dist * (1 + ε).\n",
    "        \"\"\"\n",
    "        Best = namedtuple(\"Best\", [\"point\", \"dist\"])\n",
    "        best = Best(point=None, dist=float('inf'))\n",
    "        \n",
    "        def recurse(node: SSNode):\n",
    "            nonlocal best\n",
    "            if node is None:\n",
    "                return\n",
    "            # Cota inferior de distancia al nodo\n",
    "            d_cent = np.linalg.norm(query - node.center)\n",
    "            if d_cent - node.radius > best.dist * (1 + self.epsilon):\n",
    "                return\n",
    "            # Si es hoja, búsqueda lineal\n",
    "            if node.left is None and node.right is None:\n",
    "                for p in node.points:\n",
    "                    d = np.linalg.norm(query - p)\n",
    "                    if d < best.dist:\n",
    "                        best = best._replace(point=p, dist=d)\n",
    "                return\n",
    "            recurse(node.left)\n",
    "            recurse(node.right)\n",
    "        \n",
    "        recurse(self.root)\n",
    "        return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5e14b-71c8-479b-bde6-e030a42ec324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark.py\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from kdtree import KDTree\n",
    "#from sstree import SSTree\n",
    "\n",
    "def generate_data(n: int, k: int = 5, dim: int = 5) -> np.ndarray:\n",
    "    \"\"\"Genera n puntos en R^dim según mezcla de k gaussianas.\"\"\"\n",
    "    pts = []\n",
    "    for _ in range(k):\n",
    "        mu = np.random.uniform(-10, 10, size=(dim,))\n",
    "        cov = np.eye(dim)\n",
    "        size = n // k\n",
    "        pts.append(np.random.multivariate_normal(mu, cov, size))\n",
    "    return np.vstack(pts)\n",
    "\n",
    "np.random.seed(0)\n",
    "points = generate_data(10000)\n",
    "queries = np.random.randn(1000, 5)\n",
    "\n",
    "# Construcción KDTree\n",
    "t0 = time.time()\n",
    "kdt = KDTree(points)\n",
    "t_kd_build = time.time() - t0\n",
    "\n",
    "# Construcción SS+-Tree\n",
    "t0 = time.time()\n",
    "sst = SSTree(points)\n",
    "t_ss_build = time.time() - t0\n",
    "\n",
    "# Batch de consultas KDTree\n",
    "t0 = time.time()\n",
    "kd_results = [kdt.nearest(q) for q in queries]\n",
    "t_kd_query = time.time() - t0\n",
    "\n",
    "# Batch de consultas SS+-Tree\n",
    "t0 = time.time()\n",
    "ss_results = [sst.nearest(q) for q in queries]\n",
    "t_ss_query = time.time() - t0\n",
    "\n",
    "# Validación de precisión\n",
    "eps = sst.epsilon\n",
    "hits = sum(1 for (kd, ss) in zip(kd_results, ss_results) if ss.dist <= kd.dist * (1 + eps))\n",
    "accuracy = hits / len(queries) * 100\n",
    "\n",
    "# Resultados\n",
    "print(\"Tiempos de construcción (s):\")\n",
    "print(f\"  KDTree:    {t_kd_build:.4f}\")\n",
    "print(f\"  SS+-Tree:  {t_ss_build:.4f}\\n\")\n",
    "\n",
    "print(\"Tiempos de 1000 consultas (s):\")\n",
    "print(f\"  KDTree:    {t_kd_query:.4f}\")\n",
    "print(f\"  SS+-Tree:  {t_ss_query:.4f}\\n\")\n",
    "print(f\"Precisión (ε={eps}): {accuracy:.2f}% de consultas dentro del factor ε\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171acb4b-fc37-4924-8d29-db97f03e83ce",
   "metadata": {},
   "source": [
    "### **Problema 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1a27e-cf73-49db-bfa2-c2ccf23e4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k, max_iters=100, init='kmeans++'):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.init = init\n",
    "        self.centroids = None\n",
    "\n",
    "    def _initialize_centroids(self, X):\n",
    "        n, _ = X.shape\n",
    "        if self.init == 'random':\n",
    "            # Selección aleatoria de k puntos como centroides iniciales\n",
    "            indices = random.sample(range(n), self.k)\n",
    "            return X[indices].copy()\n",
    "        # k-means++ con manejo de casos degenerados\n",
    "        centroids = []\n",
    "        centroids.append(X[random.randrange(n)])\n",
    "        for _ in range(1, self.k):\n",
    "            # Distancia al centro más cercano\n",
    "            D2 = np.min(np.square(X[:, None] - np.array(centroids)[None, :]).sum(axis=2), axis=1)\n",
    "            total = D2.sum()\n",
    "            if total == 0 or np.isnan(total):\n",
    "                # Todos los puntos están a distancia cero => elegir aleatoriamente\n",
    "                centroids.append(X[random.randrange(n)])\n",
    "                continue\n",
    "            probs = D2 / total\n",
    "            i = np.random.choice(n, p=probs)\n",
    "            centroids.append(X[i])\n",
    "        return np.array(centroids)\n",
    "\n",
    "    def fit(self, X):\n",
    "        n, d = X.shape\n",
    "        # Inicialización de centroides\n",
    "        self.centroids = self._initialize_centroids(X)\n",
    "        for it in range(self.max_iters):\n",
    "            # Asignación de puntos al centroide más cercano\n",
    "            labels = np.argmin(((X[:, None] - self.centroids[None, :])**2).sum(axis=2), axis=1)\n",
    "            new_centroids = np.array([X[labels == j].mean(axis=0) if np.any(labels==j) else self.centroids[j]\n",
    "                                      for j in range(self.k)])\n",
    "            # Criterio de parada: convergencia de centroides\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "        self.labels_ = labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Asignación para nuevos puntos\n",
    "        return np.argmin(((X[:, None] - self.centroids[None, :])**2).sum(axis=2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26a595-6128-426f-8b24-981171d8746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "class OPTICS:\n",
    "    def __init__(self, eps, min_pts):\n",
    "        self.eps = eps\n",
    "        self.min_pts = min_pts\n",
    "\n",
    "    def _region_query(self, X, i):\n",
    "        # Vecindario de radio eps\n",
    "        dist = np.linalg.norm(X - X[i], axis=1)\n",
    "        return np.where(dist <= self.eps)[0]\n",
    "\n",
    "    def fit(self, X):\n",
    "        n = X.shape[0]\n",
    "        self.reachability_ = np.full(n, np.inf)\n",
    "        self.processed = np.zeros(n, dtype=bool)\n",
    "        self.ordering = []\n",
    "\n",
    "        for i in range(n):\n",
    "            if not self.processed[i]:\n",
    "                self._expand_cluster_order(X, i)\n",
    "        # self.ordering contiene el orden óptimo y reachability\n",
    "\n",
    "    def _expand_cluster_order(self, X, i):\n",
    "        neighbors = self._region_query(X, i)\n",
    "        self.processed[i] = True\n",
    "        self.ordering.append(i)\n",
    "        core_dist = self._core_distance(X, i, neighbors)\n",
    "        if core_dist is None:\n",
    "            return\n",
    "        # Heap para candidatos\n",
    "        seeds = []\n",
    "        self._update(X, i, neighbors, seeds, core_dist)\n",
    "        while seeds:\n",
    "            dist, j = heappop(seeds)\n",
    "            if self.processed[j]:\n",
    "                continue\n",
    "            neighbors_j = self._region_query(X, j)\n",
    "            self.processed[j] = True\n",
    "            self.ordering.append(j)\n",
    "            core_dist_j = self._core_distance(X, j, neighbors_j)\n",
    "            if core_dist_j is not None:\n",
    "                self._update(X, j, neighbors_j, seeds, core_dist_j)\n",
    "\n",
    "    def _core_distance(self, X, i, neighbors):\n",
    "        if len(neighbors) < self.min_pts:\n",
    "            return None\n",
    "        # Distancia al MinPts-ésimo vecino ordenado\n",
    "        dists = np.linalg.norm(X[neighbors] - X[i], axis=1)\n",
    "        return np.sort(dists)[self.min_pts - 1]\n",
    "\n",
    "    def _update(self, X, i, neighbors, seeds, core_dist):\n",
    "        for j in neighbors:\n",
    "            if self.processed[j]:\n",
    "                continue\n",
    "            new_reach = max(core_dist, np.linalg.norm(X[i] - X[j]))\n",
    "            if self.reachability_[j] == np.inf:\n",
    "                self.reachability_[j] = new_reach\n",
    "                heappush(seeds, (self.reachability_[j], j))\n",
    "            elif new_reach < self.reachability_[j]:\n",
    "                self.reachability_[j] = new_reach\n",
    "                heappush(seeds, (self.reachability_[j], j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a8f12-9e39-4464-bc9c-9af90d5656db",
   "metadata": {},
   "source": [
    "#### **Pruebas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877d684-06e0-448f-acd3-0192f123746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "#from kmeans import KMeans\n",
    "#from optics import OPTICS\n",
    "\n",
    "class TestClustering(unittest.TestCase):\n",
    "    def test_kmeans_simple(self):\n",
    "        # Dos blobs Gaussianos en 2D\n",
    "        np.random.seed(0)\n",
    "        X1 = np.random.randn(50,2) + np.array([5,5])\n",
    "        X2 = np.random.randn(50,2) + np.array([-5,-5])\n",
    "        X = np.vstack([X1, X2])\n",
    "        km = KMeans(k=2, max_iters=100)\n",
    "        km.fit(X)\n",
    "        labels = km.labels_\n",
    "        # Debe haber 2 agrupaciones aproximadas\n",
    "        self.assertTrue(set(labels) == {0,1})\n",
    "\n",
    "    def test_kmeans_edge_cases(self):\n",
    "        X = np.zeros((5,3))\n",
    "        km = KMeans(k=3)\n",
    "        km.fit(X)\n",
    "        # Todos los centroides iguales\n",
    "        self.assertTrue(np.allclose(km.centroids, 0))\n",
    "\n",
    "    def test_optics_ring(self):\n",
    "        # Anillo interior y exterior en 2D\n",
    "        t = np.linspace(0,2*np.pi,100)\n",
    "        R1 = np.c_[np.cos(t), np.sin(t)]\n",
    "        R2 = np.c_[2*np.cos(t), 2*np.sin(t)]\n",
    "        X = np.vstack([R1, R2])\n",
    "        opt = OPTICS(eps=0.3, min_pts=5)\n",
    "        opt.fit(X)\n",
    "        # Verificar ordenamiento de alcance\n",
    "        self.assertEqual(len(opt.ordering), X.shape[0])\n",
    "\n",
    "    def test_optics_noise(self):\n",
    "        X = np.array([[0,0]]*10 + [[10,10]])\n",
    "        opt = OPTICS(eps=0.5, min_pts=3)\n",
    "        opt.fit(X)\n",
    "        # El punto aislado queda con reachability infinita al final\n",
    "        self.assertTrue(opt.reachability_[-1] == np.inf)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ejecutar solo los tests definidos, ignorando argumentos externos\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac82aa-f414-4ad3-8157-f7944a87747d",
   "metadata": {},
   "source": [
    "* **K-means** es muy eficiente (*O(n k d I)*) y funciona bien cuando los clusters son convexos y de densidad similar, pero requiere fijar **k** a priori, es sensible a outliers y no detecta bien formas no esféricas.\n",
    "* **OPTICS** explora densidades variables y detecta ruido sin necesidad de especificar el número de clusters, soporta formas arbitrarias y anida jerarquías de densidad, pero su coste $O(n \\log n)$ con índice espacial o $O(n^2)$ naïve y la parametrización de $\\epsilon$ y **MinPts** pueden hacerlo más lento y complejo de afinar.\n",
    "* **Recomendación**: emplea K-means en datos grandes, homogéneos y de clusters bien definidos por puntos centrales; recurre a OPTICS cuando esperes densidades dispares, ruido o estructuras no convexas, y valoras obtener una ordenación de alcance para analizar jerarquías de densidad.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
