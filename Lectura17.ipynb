{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc8bbb2-b342-4184-a7ca-d5a0d02ddbae",
   "metadata": {},
   "source": [
    "### **Estimación de frecuencia y elementos mayoritarios**\n",
    "\n",
    "La **estimación de frecuencias** en flujos de datos (data streams) es un reto central en sistemas que procesan información de forma continua y en tiempo real. Cuando los elementos llegan uno a uno, potencialmente sin límite de longitud, resulta inviable almacenarlos todos para contar con exactitud. En su lugar, buscamos métodos que, en tiempo y espacio sublineales, permitan obtener estimaciones suficientemente precisas.\n",
    "\n",
    "**Elemento mayoritario**\n",
    "\n",
    "Se define **elemento mayoritario** a aquel que aparece más de $\\lfloor n/2\\rfloor$ veces en un flujo de $n$ elementos. Un enfoque clásico y elegante para hallarlo en tiempo $O(n)$ y espacio $O(1)$ es el algoritmo de *Boyer–Moore majority vote*. La idea es mantener un contador y un candidato, de modo que tras procesar todo el flujo, el candidato sea el único posible:\n",
    "\n",
    "1. **Fase de selección de candidato**  \n",
    "   Inicialmente, $count=0$. Por cada elemento $x$:\n",
    "   - Si $count=0$, asignamos $candidate\\leftarrow x$ y $count\\leftarrow1$.\n",
    "   - En caso contrario, si $x=candidate$ incrementamos $count\\leftarrow count+1$, y si $x\\neq candidate$ decrementamos $count\\leftarrow count-1$.\n",
    "\n",
    "2. **Fase de verificación (opcional)**  \n",
    "   Para asegurar que el candidato realmente supera $\\lfloor n/2\\rfloor$, se recorre de nuevo el flujo contando sus ocurrencias y comprobando\n",
    "   $$\\#\\{\\,x : x=candidate\\}\\;>\\;\\frac{n}{2}.$$\n",
    "\n",
    "El candidato final satisface:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\bigl[\\,a_i = \\mathit{candidate}\\bigr] \\;-\\;\\sum_{i=1}^n \\bigl[\\,a_i \\neq \\mathit{candidate}\\bigr]\\;>\\;0\n",
    "$$\n",
    "lo que garantiza su mayoría cuando existe un único elemento que cumple la condición. \n",
    "\n",
    "**Heavy hitters**\n",
    "\n",
    "Cuando extendemos el umbral de $\\frac{n}{2}$ a un valor arbitrario $\\varphi n$ (con $0<\\varphi<1$), surgen los **heavy hitters**: los elementos cuya frecuencia excede $\\varphi n$. El algoritmo de Misra–Gries (o _frequent_) mantiene a lo sumo $k=\\lfloor 1/\\varphi\\rfloor$ contadores y procede así:\n",
    "\n",
    "1. Se mantiene un conjunto $S$ de hasta $k$ pares $(elemento, contador)$.\n",
    "2. Al procesar $x$:\n",
    "   - Si $x\\in S$, incrementamos su contador.\n",
    "   - Si $|S|<k$, añadimos $(x,1)$ a $S$.\n",
    "   - Si $|S|=k$ y $x\\notin S$, decrementamos $1$ a todos los contadores y eliminamos aquellos que lleguen a cero.\n",
    "\n",
    "Al final, los elementos con contador positivo en $S$ son candidatos, que tras una segunda pasada pueden verificarse contando sus ocurrencias reales. Este enfoque usa espacio $O(1/\\varphi)$ y tiempo amortizado $O(1)$ por elemento, ofreciendo una cota $(\\varphi n)$ de error en la estimación de frecuencias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef80f5-e042-4887-b9a6-ec9eb4c15266",
   "metadata": {},
   "source": [
    "### **Count–Min Sketch: estructura y funcionamiento**\n",
    "\n",
    "El **Count–Min Sketch** (CMS) se basa en la combinación de varias estimaciones sesgadas con la operación de mínimo para obtener una cota superior ajustada de la frecuencia real. A continuación se describen con detalle sus componentes teóricos, variantes de actualización, garantías probabilísticas y aspectos prácticos de implementación.\n",
    "\n",
    "**Hashing y requerimientos de independencia**\n",
    "\n",
    "Cada función de hash $h_i$ debe pertenecer a una familia universal (o al menos 2-independiente) para garantizar que, para cualquier par de elementos distintos $x\\neq y$, \n",
    "$$\n",
    "\\Pr[h_i(x)=h_i(y)] \\;=\\; \\frac{1}{w}.\n",
    "$$\n",
    "Con independencia par a par basta para el análisis de error:\n",
    "\n",
    "- **Colisiones esperadas**: para un elemento $x$, el número de inserciones de otros elementos que caen en la misma celda $(i,h_i(x))$ tiene valor esperado\n",
    "  $$\n",
    "  \\mathbb{E}\\bigl[C[i,h_i(x)] - f(x)\\bigr]\n",
    "  = \\sum_{y\\neq x}\\Pr[h_i(y)=h_i(x)]\n",
    "  = \\frac{N - f(x)}{w}.\n",
    "  $$\n",
    "- **Acotación de cola**: usando la desigualdad de Markov,\n",
    "  $$\n",
    "  \\Pr\\bigl[C[i,h_i(x)] - f(x) \\ge \\varepsilon N\\bigr]\n",
    "  \\;\\le\\;\n",
    "  \\frac{\\mathbb{E}[C[i,h_i(x)] - f(x)]}{\\varepsilon N}\n",
    "  = \\frac{N/w}{\\varepsilon N}\n",
    "  = \\frac{1}{e},\n",
    "  $$\n",
    "  pues elegimos $w = \\lceil e/\\varepsilon\\rceil$.\n",
    "\n",
    "Repetir esta estimación en $d$ filas independientes y tomar el mínimo asegura:\n",
    "$$\n",
    "\\Pr\\bigl[\\min_{i\\in[d]}C[i,h_i(x)] - f(x) \\ge \\varepsilon N\\bigr]\n",
    "\\;\\le\\;\\Bigl(\\tfrac1e\\Bigr)^d\n",
    "\\;\\le\\;\\delta,\n",
    "$$\n",
    "si $d \\ge \\lceil \\ln(1/\\delta)\\rceil$.\n",
    "\n",
    "\n",
    "**Actualizaciones conservadoras**\n",
    "\n",
    "La **actualización conservadora** (conservative update) es una mejora empírica que reduce el sesgo al evitar incrementar todas las entradas:\n",
    "\n",
    "1. Para cada $i\\in\\{1,\\dots,d\\}$, calcular primero  \n",
    "   $\\widehat f_i = C[i,h_i(x)]$.\n",
    "2. Determinar $\\widehat f_{\\min} = \\min_i \\widehat f_i$.\n",
    "3. Solo incrementar aquellas celdas cuyo valor sea igual a $\\widehat f_{\\min}$:\n",
    "   $$\n",
    "   \\forall\\,i:\\; C[i,h_i(x)] \n",
    "   \\;+\\!=\\; \n",
    "   \\begin{cases}\n",
    "   1, & \\text{si }C[i,h_i(x)] = \\widehat f_{\\min},\\\\\n",
    "   0, & \\text{si }C[i,h_i(x)] > \\widehat f_{\\min}.\n",
    "   \\end{cases}\n",
    "   $$\n",
    "Con ello, se evita propagar conteos excesivos en todas las filas, reduciendo la sobrestimación promedio, aunque sigue preservando el mismo orden de complejidad:\n",
    "\n",
    "- **Tiempo por update**: $O(d)$.\n",
    "- **Error máximo**: aún acotado por $\\varepsilon N$ con probabilidad $1-\\delta$.\n",
    "\n",
    "\n",
    "**Fusión de sketches (merge/composability)**\n",
    "\n",
    "Los CMS son **linealmente composables**: dadas dos matrices $C^A$ y $C^B$ construidas con los mismos parámetros $(d,w,h_1,\\dots,h_d)$ sobre flujos disjuntos, la fusión se obtiene por suma elemento a elemento:\n",
    "\n",
    "$$\n",
    "C[i,j] \\;\\leftarrow\\; C^A[i,j] \\;+\\; C^B[i,j],\n",
    "\\quad\n",
    "\\forall\\,i\\in[1,d],\\;j\\in[0,w-1].\n",
    "$$\n",
    "\n",
    "La estimación tras mezclar satisface las mismas garantías estadísticas para el flujo combinado, pues\n",
    "\n",
    "$$\n",
    "\\widehat f_{\\text{global}}(x)\n",
    "= \\min_i\\bigl(C^A[i,h_i(x)] + C^B[i,h_i(x)]\\bigr).\n",
    "$$\n",
    "\n",
    "Esto es fundamental en entornos distribuidos: cada nodo procesa su parte, envía su CMS al agregador y se realiza un único `merge`, obteniendo un CMS global con costes $O(d\\,w)$.\n",
    "\n",
    "**Extensión a actualizaciones ponderadas y decrementos**\n",
    "\n",
    "- **Actualizaciones ponderadas**: si cada elemento $x$ viene con un peso $w_x$, basta reemplazar el incremento unitario por\n",
    "  $$\n",
    "  C[i,h_i(x)] \\;+\\!=\\; w_x,\n",
    "  $$\n",
    "  manteniendo intacta la garantía de error, ahora en proporción a $\\sum w_x$.\n",
    "\n",
    "- **Decrementos**: en aplicaciones de ventanas deslizantes (sliding windows), al retirar un evento con peso $w_x$ se podría hacer\n",
    "  $$\n",
    "  C[i,h_i(x)] \\;-\\!=\\; w_x\n",
    "  \\quad\\text{(limitando a 0 para evitar negativos).}\n",
    "  $$\n",
    "  Sin embargo, esto rompe las garantías pues las colisiones no son reversibles. En su lugar, se suelen emplear **sketches con contadores con tiempo** o esquemas de **exponentially decaying windows** que mantienen versiones antiguas y reconstruyen el CMS periódicamente.\n",
    "\n",
    "\n",
    "**Operaciones avanzadas: producto interno y estimación de joins**\n",
    "\n",
    "Dado un CMS para flujo $A$ con matriz $C^A$ y otro para flujo $B$ con $C^B$, se puede estimar el **producto interno** (join size):\n",
    "\n",
    "$$\n",
    "\\sum_{x} f_A(x)\\,f_B(x)\n",
    "$$\n",
    "\n",
    "mediante\n",
    "\n",
    "$$\n",
    "\\widehat{J} \\;=\\; \\sum_{j=0}^{w-1}\\;\\min\\bigl(C^A[i,j],\\,C^B[i,j]\\bigr)\n",
    "$$\n",
    "\n",
    "para cada fila $i$, y tomando la mínima de esas sumas. Sirve, por ejemplo, para estimar el tamaño de la intersección de conjuntos en bases de datos.\n",
    "\n",
    "**Implementación de bajo nivel y consideraciones prácticas**\n",
    "\n",
    "1. **Tipos de contadores**  \n",
    "   - Usar enteros de 32 bits con **saturación** (al desbordar, quedan en $\\mathtt{UINT32\\_MAX}$) para evitar wrap-around.\n",
    "   - Opción de contadores de 16 bits si la frecuencia máxima es baja.\n",
    "\n",
    "2. **Elección de hashes**  \n",
    "   - MurmurHash3 o CityHash con semillas distintas para cada fila.\n",
    "   - Asegurarse de independencia práctica: claves y semillas no deben solaparse.\n",
    "\n",
    "3. **Localidad de memoria**  \n",
    "   - Almacenar la matriz como un bloque contiguo:   `{uint32_t table[d][w];}`  \n",
    "     para mejorar la cacheabilidad, accediendo a $\\texttt{table[i*w + idx]}$.\n",
    "\n",
    "4. **Paralelismo**  \n",
    "   - En arquitecturas multihilo, cada hilo puede tener su CMS local y luego hacer un **reducer** que sume las matrices.\n",
    "   - Alternativamente, usar atomics en la tabla global, aunque con más contención.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd96d01-cf37-4250-a57c-f444f77e1e8f",
   "metadata": {},
   "source": [
    "#### **Casos de uso**\n",
    "\n",
    "**Identificación de \"restless sleepers\"**\n",
    "\n",
    "En aplicaciones de monitoreo del sueño, cada movimiento de usuario genera un evento. El objetivo es mantener actualizada, en tiempo real, la lista de los $k$ usuarios con mayor número de eventos (los _k restless sleepers_). Una estrategia práctica:\n",
    "\n",
    "1. Instanciar un CMS para contar eventos por usuario, con parámetros $(\\varepsilon,\\delta)$ adecuados al volumen.\n",
    "2. Cada vez que llega un evento para el usuario $u$, ejecutar `CMS.update(u)`.\n",
    "3. Mantener un **heap mínimo** de tamaño $k$ (heap) que almacene usuarios y sus frecuencias estimadas.\n",
    "4. Tras cada actualización, si la frecuencia estimada de $u$ supera el mínimo del heap, sustituirlo por $u$.\n",
    "\n",
    "Gracias al espacio sublineal del CMS y a la eficiencia del heap ($O(\\log k)$ por inserción), es factible procesar millones de identificadores con recursos controlados.\n",
    "\n",
    "**Similitud distribucional de palabras en grandes corpus**\n",
    "\n",
    "En procesamiento de lenguaje natural, la **similitud distribucional** de dos palabras $w_1,w_2$ se basa en contar sus contextos coocurrentes:\n",
    "$$\n",
    "\\text{PMI}(w_1,w_2)\n",
    "=\\log\\frac{f(w_1,w_2)\\,N}{f(w_1)\\,f(w_2)},\n",
    "$$\n",
    "donde $f(\\,\\cdot\\,)$ indica frecuencia y $N$ es el número total de pares. Con un CMS:\n",
    "\n",
    "1. Para cada par $(w,contexto)$ en el flujo de texto, llamar a `CMS.update(\"pair:\"+w+\"#\"+contexto)`.\n",
    "2. Para estimar $f(w_1,w_2)$, usar `CMS.estimate(\"pair:\"+w_1+\"#\"+w_2)`; para $f(w_i)$, otro CMS dedicado a frecuencias marginales.\n",
    "3. Calcular $\\widehat{\\text{PMI}}(w_1,w_2)$ sustituyendo las estimaciones en la fórmula.\n",
    "\n",
    "Este método escala a vocabularios de decenas de millones de términos, reduciendo drásticamente la memoria comparado con tablas hash exactas.\n",
    "\n",
    "**Detección de ataques DDoS en tiempo real**\n",
    "\n",
    "En redes, un ataque DDoS genera ráfagas de paquetes desde múltiples orígenes. Se puede:\n",
    "\n",
    "1. Contar paquetes por dirección IP fuente mediante CMS.\n",
    "2. Definir un umbral de frecuencia (por ejemplo, $\\varphi N$) para detectar IPs sospechosas.\n",
    "3. Identificar rápidamente heavy hitters y activar defensas automáticas.\n",
    "\n",
    "Al combinar CMS con estructuras de heavy hitters (Misra–Gries), se consigue un sistema ligero y reactivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b87b34-d369-4bc0-b5a4-3b922dd31031",
   "metadata": {},
   "source": [
    "#### **Error vs. espacio en Count–Min Sketch**\n",
    "\n",
    "El diseño de un CMS implica un compromiso entre precisión y memoria. La tabla comparativa muestra la relación:\n",
    "\n",
    "\n",
    "| Parámetro | Dimensión                         | Garantía de error                                                     |\n",
    "|-----------|-----------------------------------|-----------------------------------------------------------------------|\n",
    "| $w$       | $\\lceil e / \\varepsilon \\rceil$   | $\\lvert \\widehat{f}(x) - f(x) \\rvert \\le \\varepsilon N$              |\n",
    "| $d$       | $\\lceil \\ln(1 / \\delta) \\rceil$   | $P(\\text{error} \\ge \\varepsilon N) \\le \\delta$                      |\n",
    "| Total     | $d \\times w$                      | $O\\left( \\frac{1}{\\varepsilon} \\ln\\left( \\frac{1}{\\delta} \\right) \\right)$ |\n",
    "\n",
    "- Si elegimos un $\\varepsilon$ menor, aumentamos $w\\propto1/\\varepsilon$, lo que incrementa el espacio linealmente.\n",
    "- Para reducir la probabilidad de fallo $\\delta$, crece $d\\propto\\ln(1/\\delta)$, impactando el tiempo por operación.\n",
    "\n",
    "Este análisis permite adaptar el CMS a las limitaciones de memoria y al nivel de confianza requerido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b379e0-8ba4-42c5-8ab2-56d6e203b43d",
   "metadata": {},
   "source": [
    "#### **Implementación sencilla en Python**\n",
    "\n",
    "A continuación se presenta una versión minimalista de Count–Min Sketch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50100818-5b6d-427b-acfd-b2343195eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3\n",
    "from array import array\n",
    "from math import ceil, e, log\n",
    "from typing import Union\n",
    "\n",
    "class CountMinSketch:\n",
    "    \"\"\"\n",
    "    Estructura probabilística Count–Min Sketch para estimar frecuencias.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        Fracción de error permitido (error aditivo máximo = epsilon * total_eventos).\n",
    "    delta : float\n",
    "        Probabilidad máxima de que el error exceda epsilon * total_eventos.\n",
    "    conservative : bool, opcional\n",
    "        Si es True, usa actualización conservadora para reducir la sobreestimación.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon: float,\n",
    "        delta: float,\n",
    "        conservative: bool = False,\n",
    "    ):\n",
    "        # Validar rangos de parámetros\n",
    "        if not (0 < epsilon < 1):\n",
    "            raise ValueError(f\"epsilon debe estar en (0,1), pero vino {epsilon}\")\n",
    "        if not (0 < delta < 1):\n",
    "            raise ValueError(f\"delta debe estar en (0,1), pero vino {delta}\")\n",
    "\n",
    "        # Guardar parámetros\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.conservative = conservative\n",
    "\n",
    "        # Dimensiones: ancho w y número de filas d\n",
    "        self.w = ceil(e / epsilon)\n",
    "        self.d = ceil(log(1 / delta))\n",
    "        self.total = 0                   # contador total de todos los eventos\n",
    "\n",
    "        # Matriz d×w inicializada a ceros\n",
    "        self.table = [array('L', [0] * self.w) for _ in range(self.d)]\n",
    "\n",
    "        # Semillas de 32 bits para cada función de hash\n",
    "        self.seeds = [((i * 0xFBA4C795 + 1) & 0xFFFFFFFF) for i in range(self.d)]\n",
    "\n",
    "    def update(self, key: Union[str, bytes], count: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Incrementa la cuenta de `key` en `count` (debe ser >=1).\n",
    "        \"\"\"\n",
    "        if count < 1:\n",
    "            raise ValueError(f\"count debe ser >=1, pero vino {count}\")\n",
    "\n",
    "        # Calcular los índices en cada fila\n",
    "        indices = [mmh3.hash(key, seed) % self.w for seed in self.seeds]\n",
    "\n",
    "        if self.conservative:\n",
    "            # Actualización conservadora: sólo incrementa las celdas con el valor mínimo actual\n",
    "            valores = [self.table[i][idx] for i, idx in enumerate(indices)]\n",
    "            minimo = min(valores)\n",
    "            for i, idx in enumerate(indices):\n",
    "                if self.table[i][idx] == minimo:\n",
    "                    self.table[i][idx] += count\n",
    "        else:\n",
    "            # Actualización normal: incrementa todas las filas\n",
    "            for i, idx in enumerate(indices):\n",
    "                self.table[i][idx] += count\n",
    "\n",
    "        self.total += count\n",
    "\n",
    "    def estimate(self, key: Union[str, bytes]) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve la estimación de frecuencia de `key`.\n",
    "        \"\"\"\n",
    "        return min(\n",
    "            self.table[i][mmh3.hash(key, seed) % self.w]\n",
    "            for i, seed in enumerate(self.seeds)\n",
    "        )\n",
    "\n",
    "    def merge(self, other: 'CountMinSketch') -> 'CountMinSketch':\n",
    "        \"\"\"\n",
    "        Fusiona otro CountMinSketch en este (modifica en lugar).\n",
    "\n",
    "        Ambos sketches deben tener los mismos d, w y seeds.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, CountMinSketch):\n",
    "            raise TypeError(\"Sólo se puede fusionar con otro CountMinSketch\")\n",
    "        if (self.w, self.d, self.seeds) != (other.w, other.d, other.seeds):\n",
    "            raise ValueError(\"Dimensiones o semillas de hash diferentes\")\n",
    "\n",
    "        for i in range(self.d):\n",
    "            for j in range(self.w):\n",
    "                self.table[i][j] += other.table[i][j]\n",
    "        self.total += other.total\n",
    "        return self\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el conteo total de todos los elementos procesados.\n",
    "        \"\"\"\n",
    "        return self.total\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"CountMinSketch(epsilon={self.epsilon:.4f}, \"\n",
    "            f\"delta={self.delta:.4e}, total={self.total}, \"\n",
    "            f\"d={self.d}, w={self.w})\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df2868-014f-43d3-b222-d9e6776e7806",
   "metadata": {},
   "source": [
    "**Ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99fb26-b2e8-4b44-876b-0b3ed11cd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un CMS con error máximo del 1% y probabilidad de fallo 0.1%\n",
    "cms = CountMinSketch(epsilon=0.01, delta=0.001)\n",
    "# Crear otro CMS idéntico para demostrar merge\n",
    "cms2 = CountMinSketch(epsilon=0.01, delta=0.001, conservative=True)\n",
    "\n",
    "# Actualizar frecuencias\n",
    "palabras = [\"gato\", \"perro\", \"gato\", \"ratón\", \"perro\", \"gato\"]\n",
    "for p in palabras:\n",
    "    cms.update(p)\n",
    "    cms2.update(p, count=2)  # en cms2 usamos actualización ponderada\n",
    "\n",
    "# Estimar frecuencias\n",
    "for p in {\"gato\", \"perro\", \"ratón\"}:\n",
    "    print(f\"{p}: estimación cms={cms.estimate(p)}, cms2={cms2.estimate(p)}\")\n",
    "\n",
    "# Fusionar cms2 en cms\n",
    "cms.merge(cms2)\n",
    "print(\"\\nTras fusionar cms2 en cms:\")\n",
    "for p in {\"gato\", \"perro\", \"ratón\"}:\n",
    "    print(f\"{p}: estimación combinada={cms.estimate(p)}\")\n",
    "\n",
    "# Mostrar contador total\n",
    "print(f\"\\nTotal de eventos procesados: {len(cms)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226353e-1ad8-49fc-a163-340d993dff21",
   "metadata": {},
   "source": [
    "#### **Consultas por rangos con Count–Min Sketch**\n",
    "\n",
    "Sea un dominio ordenado de enteros $[0, U-1]$. Queremos responder a la consulta  \n",
    "$$\n",
    "F(L,R)\\;=\\;\\sum_{x=L}^{R} f(x),\n",
    "$$  \n",
    "donde $f(x)$ es la frecuencia (o peso) del ítem $x$. El CMS estándar no soporta directamente sumas sobre rangos—pero podemos hacerlo con **descomposición diádica** y mantener un CMS por cada \"nivel\" de potencias de dos.\n",
    "\n",
    "**Niveles y bloques diádicos**\n",
    "\n",
    "- Definimos $k = \\lceil\\log_{2} U\\rceil$.  \n",
    "- Para cada nivel $i=0,1,\\dots,k$, los bloques diádicos de longitud $2^i$ son  \n",
    "  $$\n",
    "  B_{i,j} = \\bigl[j\\cdot 2^i,\\;(j+1)\\cdot2^i -1\\bigr],\\quad j=0,1,\\dots,\\lfloor U/2^i\\rfloor.\n",
    "  $$\n",
    "- Hay a lo sumo $\\tfrac{U}{2^i}+1$ bloques en el nivel $i$ (muchos de ellos vacíos si $U$ no es múltiplo).\n",
    "\n",
    "Cada bloque $B_{i,j}$ representamos su contador con la clave de hash  \n",
    "```text\n",
    "key = f\"{i}:{j}\"\n",
    "```\n",
    "en un CMS independiente $\\mathrm{CMS}_i$.\n",
    "\n",
    "**Fase de actualización (\"update\")**\n",
    "\n",
    "**Objetivo:** cuando llega un elemento $x$, debemos incrementar *todos* los bloques diádicos que lo contienen, es decir, uno por cada nivel.\n",
    "\n",
    "```python\n",
    "def update(x, count=1):\n",
    "    for i in range(0, k+1):\n",
    "        # bloque diádico que contiene x en nivel i\n",
    "        j = x >> i           # equivale a floor(x / 2^i)\n",
    "        CMS[i].update(f\"{i}:{j}\", count)\n",
    "```\n",
    "\n",
    "- **Complejidad**  \n",
    "  - Coste de hash y escritura: $O(d)$ por cada CMS,  \n",
    "  - hay $k+1 = O(\\log U)$ niveles,  \n",
    "  - ⇒ **Actualización en** $O(d\\,\\log U)$.\n",
    "\n",
    "- **Memoria**  \n",
    "  - Cada CMS_i ocupa $d\\times w$ contadores,  \n",
    "  - Total: $(k+1)\\,d\\,w = O\\bigl(d\\,w\\,\\log U\\bigr)$.\n",
    "\n",
    "**Fase de consulta de rango (\"query\")**\n",
    "\n",
    "Para obtener $\\widehat F(L,R)$:\n",
    "\n",
    "1. **Descomposición diádica**: calculamos un conjunto de pares $\\mathcal B\\subseteq\\{0,\\dots,k\\}\\times\\mathbb N$ que cubren exactamente $[L,R]$ sin solaparse.  \n",
    "2. **Suma de estimaciones**:\n",
    "   $$\n",
    "   \\widehat F(L,R)\n",
    "   = \\sum_{(i,j)\\in\\mathcal B} \\mathrm{CMS}_i.\\mathrm{estimate}(i \\mathbin{:} j)\n",
    "   $$\n",
    "\n",
    "donde $i \\mathbin{:} j$ representa una codificación única del intervalo que corresponde al bloque $(i, j)$, por ejemplo mediante la cadena de caracteres $\\texttt{\"i:j\"}$, usada internamente por CMS como clave.\n",
    "\n",
    "**¿Por qué $O(\\log U)$ bloques?**\n",
    "\n",
    "- Cada descomposición diádica elige siempre el bloque más grande alineado con $L$ que quepa dentro de $[L,R]$.  \n",
    "- Tras consumir ese bloque, el nuevo inicio se mueve al final del mismo y volvemos a elegir el bloque más grande posible.  \n",
    "- Este \"greedy\" garantiza a lo sumo $2\\log_2 U$ bloques:\n",
    "  - **Parte izquierda**: a lo sumo $\\log_2 U$ bloques (uno por cada bit en la representación de $L$).  \n",
    "  - **Parte derecha**: análogo para la longitud $(R-L+1)$.  \n",
    "\n",
    "Por ejemplo, para $[3,14]$ con $U\\ge16$:\n",
    "```\n",
    "[3,14] = [3,3] ∪ [4,7] ∪ [8,15]∩[8,14]\n",
    "        = B_{0,3} ∪ B_{2,1} ∪ (B_{3,1} truncated)\n",
    "```\n",
    "que corresponden a 3 bloques, mucho menos que $U$.\n",
    "\n",
    "**Pseudocódigo completo**\n",
    "\n",
    "```python\n",
    "def dyadic_intervals(L: int, R: int) -> list[tuple[int,int]]:\n",
    "    \"\"\"Devuelve lista de (nivel i, bloque j) que cubren [L,R].\"\"\"\n",
    "    intervals = []\n",
    "    while L <= R:\n",
    "        # Máxima potencia de dos que divide a L\n",
    "        max_block = L & -L\n",
    "        # Longitud restante\n",
    "        rem = R - L + 1\n",
    "        # Ajustar block al tamaño que quepa en rem\n",
    "        block = max_block if max_block <= rem else 1 << (rem.bit_length() - 1)\n",
    "        lvl = block.bit_length() - 1\n",
    "        j = L >> lvl\n",
    "        intervals.append((lvl, j))\n",
    "        L += block\n",
    "    return intervals\n",
    "\n",
    "def range_query(L: int, R: int) -> int:\n",
    "    \"\"\"Estimación de frecuencia en [L,R] usando k+1 CountMinSketch.\"\"\"\n",
    "    total = 0\n",
    "    for i, j in dyadic_intervals(L, R):\n",
    "        key = f\"{i}:{j}\"\n",
    "        total += CMS[i].estimate(key)\n",
    "    return total\n",
    "```\n",
    "\n",
    "- **Coste**  \n",
    "  - Construir $\\mathcal B$: $O(\\log U)$.  \n",
    "  - Por cada bloque, una consulta CMS en $O(d)$.  \n",
    "  - => **Complejidad**: $O(d\\,\\log U) = O(\\ln(1/\\delta)\\,\\log U)$.\n",
    "\n",
    "**Garantías de error para rango**\n",
    "\n",
    "Cada bloque incursiona un error **aditivo** $\\le \\varepsilon N$ con probabilidad $1-\\delta$. Al sumar $|\\mathcal B|$ bloques:\n",
    "\n",
    "- El error global $\\le |\\mathcal B|\\cdot\\varepsilon N \\le 2\\varepsilon\\,N\\log_2U$.  \n",
    "- Para mantener un **error relativo** pequeño, conviene elegir un $\\varepsilon'$ tal que  \n",
    "  $$\n",
    "  \\varepsilon' \\;=\\;\\frac{\\varepsilon}{2\\log_2U},\n",
    "  $$\n",
    "  y crear los CMS con ese parámetro para que la suma de errores siga $\\le \\varepsilon N$.\n",
    "\n",
    "**Variantes y optimizaciones**\n",
    "\n",
    "1. **Segment tree de sketches**  \n",
    "   - En lugar de niveles diádicos fijos, construir un árbol binario completo sobre $[0,U-1]$, con un CMS en cada nodo.  \n",
    "   - La descomposición y consulta usan el mismo principio de árbol, pero ocupa $O(U)$ nodos: más memoria, pero consultas en $O(d\\log U)$ igual.\n",
    "\n",
    "2. **Descomposición dinámica**  \n",
    "   - Si el flujo usa sólo un subconjunto de $[0, U-1]$, podemos llevar un mapeo dinámico de bloques activos y mantener CMS solo para ellos, ahorrando espacio.\n",
    "\n",
    "3. **Ajuste de parámetros por nivel**  \n",
    "   - Podríamos dedicar más ancho $w$ a niveles donde esperamos mayor densidad de elementos (niveles bajos), y menos a los altos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa09ae-d433-40f8-a8e2-4c16fc13ab38",
   "metadata": {},
   "source": [
    "### **Estimación de cardinalidad y hyperloglog**\n",
    "\n",
    "En aplicaciones de **big data**, bases de datos distribuidas y procesado de flujos en tiempo real, es común la necesidad de **contar elementos distintos** (cardinalidad) en un conjunto de datos muy grande o en un flujo continuo. El método exacto por ejemplo, mantener un _hash set_ de todos los valores vistos, es inviable cuando:\n",
    "\n",
    "- El volumen de datos supera la capacidad de memoria disponible.  \n",
    "- Se requiere una respuesta en tiempo real, con latencias muy bajas.  \n",
    "- Se procesan datos distribuidos en varias máquinas y luego debe agregarse el resultado.\n",
    "\n",
    "Los algoritmos de **conteo probabilístico** resuelven este problema entregando una estimación de la cardinalidad con un error controlado y usando **espacio sublineal**. Entre ellos, la familia **hyperloglog (HLL)** se ha consolidado como uno de los más precisos y eficientes, ofreciendo un **error relativo** del orden de $1\\%$ usando apenas unos pocos kilobytes de memoria.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed03f7a-3da3-4f33-902e-79ee6f9b6d1c",
   "metadata": {},
   "source": [
    "#### **Conteo de elementos distintos en bases de datos**\n",
    "\n",
    "**Enfoque exacto**\n",
    "\n",
    "El método tradicional para contar distintos en SQL es:\n",
    "\n",
    "```sql\n",
    "SELECT COUNT(DISTINCT columna) FROM tabla;\n",
    "```\n",
    "\n",
    "Este enfoque requiere escanear la tabla completa y almacenar en memoria o en disco la lista de valores únicos (o su índice), con un coste en espacio y tiempo de $O(n)$. Para tablas de decenas o centenas de millones de filas, el rendimiento y la escalabilidad son problemáticos.\n",
    "\n",
    "\n",
    "En entornos de **streaming**, los datos llegan de forma continua y no pueden almacenarse en su totalidad. Además, el requisito de latencia impide:\n",
    "\n",
    "- Realizar dos pasadas sobre los datos.  \n",
    "- Almacenar todas las claves vistas.  \n",
    "\n",
    "Por ello, se necesitan estructuras que:\n",
    "\n",
    "1. **Procesen cada elemento** en tiempo amortizado muy bajo ($O(1)$ o $O(\\log\\log n)$).  \n",
    "2. **Usen espacio sublineal** respecto al número de elementos distintos.  \n",
    "3. **Entreguen estimaciones** con error relativo bajo y ajustable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2bd40d-9239-4d08-82da-e039e780055a",
   "metadata": {},
   "source": [
    "#### **Diseño incremental de HyperLogLog**\n",
    "\n",
    "La evolución de HyperLogLog parte del algoritmo clásico de Flajolet–Martin y pasa por dos etapas intermedias antes de llegar a la fórmula definitiva basada en media armónica. A continuación se presenta cada paso con texto fluido y las ecuaciones principales.\n",
    "\n",
    "**Probabilistic counting (Flajolet–Martin)**\n",
    "\n",
    "En el enfoque original se emplea una única función de hash $h$ que mapea cada elemento $x$ a una palabra de $L$ bits, interpretada como una secuencia de ceros y unos. A partir de ese hash, se define\n",
    "$$\n",
    "\\rho(h(x)) = \\min\\{\\,i \\ge 1 : \\text{el $i$-ésimo bit de }h(x)\\text{ es }1\\},\n",
    "$$\n",
    "es decir, la posición del primer uno (contando ceros iniciales). Se mantiene un solo registro\n",
    "$$\n",
    "R = \\max_{x\\in\\text{stream}} \\rho\\bigl(h(x)\\bigr),\n",
    "$$\n",
    "y la cardinalidad aproximada del conjunto es\n",
    "$$\n",
    "\\hat{n} = 2^R.\n",
    "$$\n",
    "Aunque simple, este método adolece de una varianza muy elevada ($\\sigma\\approx n$) y un error relativo cercano al 100 %, dado que depende de un único máximo.\n",
    "\n",
    "**Promedio estocástico**\n",
    "\n",
    "Para atenuar la varianza se introducen $k$ funciones de hash independientes $h_1,\\dots,h_k$ y se calcula un registro $R_i$ para cada una. La estimación se define como la media aritmética de sus potencias:\n",
    "$$\n",
    "\\bar{Z}\n",
    "= \\frac{1}{k}\\sum_{i=1}^k 2^{R_i}.\n",
    "$$\n",
    "La varianza de $\\bar{Z}$ cae a $O(n^2/k)$, de modo que el error relativo pasa a ser $O(1/\\sqrt{k})$. Sin embargo, este enfoque multiplica por $k$ tanto el espacio requerido como el coste de computar cada elemento.\n",
    "\n",
    "**LogLog**\n",
    "\n",
    "LogLog consigue un efecto similar con un único vector de $m$ registros, donde $m=2^b$. Se aprovechan los primeros $b$ bits del hash para escoger un bucket $j\\in\\{0,\\dots,m-1\\}$ y el resto de bits para calcular $\\rho$. Para cada bucket $j$ se mantiene\n",
    "$$\n",
    "R_j \\;=\\; \\max_{x:\\,\\text{bucket}(x)=j} \\rho\\bigl(\\text{suffix}(h(x))\\bigr).\n",
    "$$\n",
    "La estimación global se formula como\n",
    "$$\n",
    "\\hat{n}\n",
    "= \\alpha_m \\,m \\,2^{\\frac{1}{m}\\sum_{j=0}^{m-1}R_j},\n",
    "$$\n",
    "donde $\\alpha_m$ es una constante de corrección de sesgo. Así, la varianza baja a $O(n^2/m)$ y el error relativo a $O(1/\\sqrt{m})$, pero la media de potencias deja un sesgo residual.\n",
    "\n",
    "**HyperLogLog: media armónica y correcciones**\n",
    "\n",
    "HyperLogLog sustituye la media de potencias por una media armónica que reduce drásticamente el sesgo. Definiendo\n",
    "\n",
    "$$\n",
    "Z \\;=\\;\\Bigl(\\sum_{j=0}^{m-1}2^{-R_j}\\Bigr)^{-1},\n",
    "$$\n",
    "\n",
    "la estimación principal es\n",
    "\n",
    "$$\n",
    "\\hat{n}\n",
    "=\\frac{\\alpha_m\\,m^2}{\\sum_{j=0}^{m-1}2^{-R_j}},\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "\\alpha_m\n",
    "=\n",
    "\\begin{cases}\n",
    "0.673 & m=16,\\\\\n",
    "0.697 & m=32,\\\\\n",
    "0.709 & m=64,\\\\\n",
    "\\frac{0.7213}{1 + 1.079/m} & m\\ge128.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Además, para mejorar la precisión en los extremos se aplican dos correcciones:\n",
    "\n",
    "1. **Rango bajo** ($\\hat n \\le 2.5\\,m$): usar _linear counting_\n",
    "   $$\n",
    "     \\hat{n}_{\\mathrm{LC}}\n",
    "     = m\\;\\ln\\!\\Bigl(\\tfrac{m}{V}\\Bigr),\n",
    "   $$\n",
    "   donde $V$ es el número de registros con $R_j=0$.\n",
    "\n",
    "2. **Rango alto** ($\\hat n > 2^L/30$, con $L$ bits de hash):\n",
    "   $$\n",
    "     \\hat{n}_{\\mathrm{HR}}\n",
    "     = -2^{L}\\;\\ln\\!\\Bigl(1 - \\tfrac{\\hat{n}}{2^{L}}\\Bigr).\n",
    "   $$\n",
    "\n",
    "Con estas mejoras, HyperLogLog alcanza un error relativo teórico de\n",
    "$$\n",
    "\\sigma \\approx \\frac{1.04}{\\sqrt{m}},\n",
    "$$\n",
    "logrando, por ejemplo con $m=2^{14}=16384$, una precisión cercana al 0.8 %."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7383c1f-fb3e-4ccd-af66-12c8409057ce",
   "metadata": {},
   "source": [
    "#### **Implementación de HyperLogLog en Python**\n",
    "\n",
    "A continuación presentamos una **implementación simplificada** de HyperLogLog en Python, que incluye:\n",
    "\n",
    "- Cálculo de buckets por los primeros $b$ bits.  \n",
    "- Registro del máximo de ceros iniciales.  \n",
    "- Estimación y correcciones de rango bajo.  \n",
    "- Fusión (_merge_) de dos estructuras HLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723c9b6-761c-47f6-892c-3ffe7182689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3        # MurmurHash3\n",
    "import math\n",
    "from typing import Callable, Optional\n",
    "\n",
    "class HyperLogLog:\n",
    "    \"\"\"\n",
    "    HyperLogLog para estimación de cardinalidad de un stream de elementos.\n",
    "    Soporta:\n",
    "      - Parámetro b: log2(m) buckets\n",
    "      - Corrección de rango bajo (linear counting)\n",
    "      - Corrección de rango alto (opcional)\n",
    "      - Fusión de múltiples HLL\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        b: int,\n",
    "        hash_fn: Optional[Callable[[bytes], int]] = None,\n",
    "        register_type: type = list\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param b: número de bits para indexar (m = 2^b buckets).\n",
    "        :param hash_fn: función que tome bytes y devuelva un entero (por defecto mmh3.hash64).\n",
    "        :param register_type: tipo de contenedor para los registros; por defecto list.\n",
    "        \"\"\"\n",
    "        if b < 4 or b > 16:\n",
    "            raise ValueError(\"b debe estar entre 4 y 16 para garantizar precisión y eficiencia\")\n",
    "        self.b = b\n",
    "        self.m = 1 << b\n",
    "        self.registers = register_type([0]) * self.m\n",
    "\n",
    "        # Selección de función de hash de 64 bits (más uniforme en volúmenes grandes)\n",
    "        self._hash = hash_fn or (lambda v: mmh3.hash64(v, signed=False)[0])\n",
    "\n",
    "        # Constante α_m\n",
    "        if   self.m == 16:   self.alpha = 0.673\n",
    "        elif self.m == 32:   self.alpha = 0.697\n",
    "        elif self.m == 64:   self.alpha = 0.709\n",
    "        else:                self.alpha = 0.7213 / (1 + 1.079 / self.m)\n",
    "\n",
    "        # Número de bits para ρ()\n",
    "        self._w_bits = 64 - self.b\n",
    "\n",
    "    def _rho(self, w: int) -> int:\n",
    "        \"\"\"\n",
    "        Cuenta ceros iniciales en w (de longitud _w_bits) + 1.\n",
    "        Usa bit_length() para velocidad.\n",
    "        \"\"\"\n",
    "        if w == 0:\n",
    "            return self._w_bits + 1\n",
    "        # posición del bit más alto (1-based)\n",
    "        l = w.bit_length()\n",
    "        # ceros a la izquierda: (_w_bits - l)\n",
    "        return (self._w_bits - l) + 1\n",
    "\n",
    "    def add(self, value: str) -> None:\n",
    "        \"\"\"\n",
    "        Inserta un elemento (string) en el HLL.\n",
    "        Convierte a bytes UTF-8 antes de hashear.\n",
    "        \"\"\"\n",
    "        x = self._hash(value.encode('utf-8'))\n",
    "        idx = x >> self._w_bits        # primeros b bits\n",
    "        w   = x & ((1 << self._w_bits) - 1)\n",
    "        self.registers[idx] = max(self.registers[idx], self._rho(w))\n",
    "\n",
    "    def estimate(self) -> float:\n",
    "        \"\"\"\n",
    "        Retorna la estimación corregida de cardinalidad.\n",
    "        Aplica:\n",
    "          1. Estimación cruda E\n",
    "          2. Linear counting si E es pequeño\n",
    "          3. Corrección de rango alto si E es muy grande\n",
    "        \"\"\"\n",
    "        inv_sum = sum(2.0 ** -r for r in self.registers)\n",
    "        E = (self.alpha * self.m * self.m) / inv_sum\n",
    "\n",
    "        # Corrección rango bajo\n",
    "        if E <= 2.5 * self.m:\n",
    "            V = self.registers.count(0)\n",
    "            if V:\n",
    "                E = self.m * math.log(self.m / V)\n",
    "\n",
    "        # Corrección rango alto (para cardinalidades cercanas al espacio de hash)\n",
    "        elif E > (1/30) * (1 << 64):\n",
    "            E = - (1 << 64) * math.log(1 - E / (1 << 64))\n",
    "\n",
    "        return E\n",
    "\n",
    "    def merge(self, other: 'HyperLogLog') -> 'HyperLogLog':\n",
    "        \"\"\"\n",
    "        Fusiona self con otro HLL (de mismo b) y retorna uno nuevo.\n",
    "        \"\"\"\n",
    "        if self.b != other.b:\n",
    "            raise ValueError(\"No es posible fusionar HLLs con distinto parámetro b\")\n",
    "        h = HyperLogLog(self.b, hash_fn=self._hash, register_type=type(self.registers))\n",
    "        h.registers = [max(r1, r2) for r1, r2 in zip(self.registers, other.registers)]\n",
    "        return h\n",
    "\n",
    "    def __or__(self, other: 'HyperLogLog') -> 'HyperLogLog':\n",
    "        \"\"\"Permite usar operador '|' como alias de merge().\"\"\"\n",
    "        return self.merge(other)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Longitud aproximada de elementos insertados.\"\"\"\n",
    "        return int(self.estimate())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<HyperLogLog b={self.b} m={self.m} est={self.estimate():.2f}>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501e6b4-66fa-4ee4-941a-d07985cbfff0",
   "metadata": {},
   "source": [
    "**Ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2e09e-5604-4dcc-bf35-9493bb122da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejemplo_básico():\n",
    "    # 1) Creamos un HLL con b = 10 -> m = 2^10 = 1024 buckets\n",
    "    hll = HyperLogLog(b=10)\n",
    "\n",
    "    # 2) Insertamos 10000 elementos únicos (como strings)\n",
    "    for i in range(10000):\n",
    "        hll.add(f\"user_{i}\")\n",
    "\n",
    "    # 3) Estimación de cardinalidad\n",
    "    print(\"Estimación cruda:    \", hll.estimate())\n",
    "    print(\"Estimación entera:   \", len(hll))\n",
    "    print(\"Representación .repr:\", hll)\n",
    "\n",
    "def ejemplo_merge():\n",
    "    # Creamos dos HLL independientes\n",
    "    h1 = HyperLogLog(b=10)\n",
    "    h2 = HyperLogLog(b=10)\n",
    "\n",
    "    # h1 recibe los elementos 0–4999, h2 los 5000–9999\n",
    "    for i in range(5000):\n",
    "        h1.add(f\"item_{i}\")\n",
    "    for i in range(5000, 10000):\n",
    "        h2.add(f\"item_{i}\")\n",
    "\n",
    "    # Cada uno estima ~5000\n",
    "    print(\"h1 ≈\", len(h1), \" h2 ≈\", len(h2))\n",
    "\n",
    "    #  - Fusionamos con el método merge()\n",
    "    h_merged = h1.merge(h2)\n",
    "    print(\"Merge():\", len(h_merged))       # ≈10000\n",
    "\n",
    "    #  - O usando el operador |\n",
    "    h_union = h1 | h2\n",
    "    print(\"Operator | :\", len(h_union))   # ≈10000\n",
    "\n",
    "def ejemplo_hash_personalizado():\n",
    "    # Podemos pasar nuestra propia función de hash\n",
    "    import hashlib\n",
    "\n",
    "    def sha1_hash(v: bytes) -> int:\n",
    "        # Tomamos los 8 bytes más bajos de SHA1 para obtener un entero de 64 bits\n",
    "        h = hashlib.sha1(v).digest()[-8:]\n",
    "        return int.from_bytes(h, \"big\")\n",
    "\n",
    "    # Usamos SHA1 en lugar de mmh3\n",
    "    h_sha1 = HyperLogLog(b=12, hash_fn=sha1_hash)\n",
    "\n",
    "    # Insertamos algunos valores repetidos\n",
    "    datos = [\"apple\", \"banana\", \"cherry\", \"apple\", \"banana\"] * 100\n",
    "    for fruta in datos:\n",
    "        h_sha1.add(fruta)\n",
    "\n",
    "    print(\"Únicos esperados: 3\")\n",
    "    print(\"Estimación SHA1 :\", len(h_sha1))  # ≈3\n",
    "\n",
    "# Ejecutar todos los ejemplos de una vez\n",
    "print(\"Ejemplo básico\")\n",
    "ejemplo_básico()\n",
    "print(\"\\nEjemplo de merge\")\n",
    "ejemplo_merge()\n",
    "print(\"\\nEjemplo con hash personalizado\")\n",
    "ejemplo_hash_personalizado()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1b8ce-8b3f-4dd0-95d0-f93eac8b235b",
   "metadata": {},
   "source": [
    "#### **Caso de uso: \"Catching Worms\" con HyperLogLog**\n",
    "\n",
    "En un entorno de **detección de gusanos de red** (network worms), el volumen de paquetes maliciosos puede ser inmenso y los recursos de memoria y CPU escasos. HyperLogLog (HLL) encaja perfectamente como estructura de conteo aproximado de **hosts únicos**, gracias a su bajo consumo de memoria y alta velocidad en la inserción de cada fingerprint.\n",
    "\n",
    "**Arquitectura general**\n",
    "\n",
    "1. **Captura de paquetes**  \n",
    "   Un sniffer o IDS (p. ej. Zeek, Suricata) inspecciona tráfico en tiempo real y, ante la detección de un payload sospechoso, extrae un _fingerprint_ (hash del payload, campos de cabecera, etc.).\n",
    "2. **Pipeline de ingestión**  \n",
    "   Cada fingerprint se envía a un componente de conteo de cardinalidad basado en HLL, que vive en memoria o en un datastore en memoria (p. ej. Redis con módulo HyperLogLog).\n",
    "3. **Procesamiento por intervalos**  \n",
    "   - Se crea un HLL con `b=14` (m=16 384 registros -> ~13 KB).  \n",
    "   - Cada paquete malicioso se inserta con `hll.add(f)`.  \n",
    "   - Al acabar el intervalo (p. ej. cada 60 s), se llama `n = hll.estimate()`.\n",
    "4. **Generación de alertas**  \n",
    "   - Si `n > umbral_absoluto` (p. ej. T = 1 000 hosts), dispara alerta crítica.  \n",
    "   - Si `n / n_prev > umbral_relativo` (p. ej. >1.5× respecto al minuto anterior), alerta de crecimiento explosivo.\n",
    "5. **Ventana deslizante**  \n",
    "   Para suavizar picos, en vez de reiniciar, se mantienen k HLL circulares o se usan técnicas de \"HLL decay\" donde cada registro se reduce lentamente.\n",
    "\n",
    "\n",
    "**Parámetros y precisión**\n",
    "\n",
    "- **Elección de b=14**  \n",
    "  - Memoria: $2^{14}$ registros → 16 384 enteros de 1 byte → ~16 KB.  \n",
    "  - Error estándar $\\sigma ≈ 1.04/\\sqrt{m} \\approx 1.04/128 \\approx 0.8 %$.  \n",
    "- **Umbrales**  \n",
    "  - **Absoluto (T)**: depende de la capacidad de la red y del volumen normal de hosts clientes. Se calibra midiendo tráfico benigno.  \n",
    "  - **Relativo**: detecta \"brotes\" que no alcanzarían T pero sí un rápido incremento.\n",
    "- **Error en conteo**  \n",
    "  - A menor cardinalidad (n < 2.5 m), se aplica linear counting y el sesgo es aún menor.  \n",
    "  - A cardinalidades muy altas, se puede añadir corrección de rango alto.\n",
    "\n",
    "\n",
    "**Ventanas temporales y \"decay\"**\n",
    "\n",
    "1. **Ventanas fijas**  \n",
    "   - Cada minuto se lanza un nuevo HLL; el anterior se descarta.  \n",
    "   - Simple, pero no permite medias móviles.\n",
    "2. **Ventana deslizante (sliding window)**  \n",
    "   - Mantén un arreglo circular de `W` HLLs (p. ej. W = 5 para ventana de 5 min).  \n",
    "   - Al insertarse un fingerprint, se añade a todos los HLLs activos.  \n",
    "   - La estimación del total único en la ventana es la fusión de los W HLLs ($h_{total} = h_0 | h_1 | ... | h_{W-1}$).\n",
    "3. **Decay exponencial**  \n",
    "   - Cada registro $R_i$ se multiplica por un factor $\\alpha <1$ al acabar cada intervalo, facilitando que los eventos antiguos \"se desvanezcan\".\n",
    "\n",
    "**Integración con SIEM y alerta**\n",
    "\n",
    "- Los valores `n` por intervalo se envían a la plataforma de monitoreo (Elastic, Splunk, Grafana).  \n",
    "- Un job de alerta compara `n` contra umbrales históricos y disparadores definidos (por ejemplo, Elastic Watcher).  \n",
    "- En dashboards, se grafica la serie temporal de estimaciones junto con la tasa de crecimiento minuto a minuto.\n",
    "\n",
    "\n",
    "**Pseudocódigo simplificado**\n",
    "\n",
    "```python\n",
    "# Parámetros\n",
    "b           = 14\n",
    "intervalo_s = 60\n",
    "T_absoluto  = 1000\n",
    "T_relativo  = 1.5\n",
    "\n",
    "# Buffers para ventana deslizante de 5 intervalos\n",
    "W = 5\n",
    "hlls = [HyperLogLog(b) for _ in range(W)]\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    t_inicio = time.time()\n",
    "    hlls[i] = HyperLogLog(b)          # reset del HLL actual\n",
    "\n",
    "    # Ingestión de paquetes durante intervalo\n",
    "    for pkt in sniff_interval(intervalo_s):\n",
    "        f = extract_fingerprint(pkt)\n",
    "        for h in hlls:\n",
    "            h.add(f)\n",
    "\n",
    "    # Estimación de unión de ventana\n",
    "    h_total = hlls[0]\n",
    "    for j in range(1, W):\n",
    "        h_total |= hlls[j]\n",
    "    n = len(h_total)\n",
    "\n",
    "    # Alerta absoluta\n",
    "    if n > T_absoluto:\n",
    "        alert(f\"Hosts únicos infectados: {n} > {T_absoluto}\")\n",
    "\n",
    "    # Alerta relativa\n",
    "    if i > 0:\n",
    "        n_prev = medidas[i-1]\n",
    "        if n_prev and (n / n_prev) > T_relativo:\n",
    "            alert(f\"Crecimiento rápido: {n_prev}→{n} (>×{T_relativo})\")\n",
    "\n",
    "    medidas[i] = n\n",
    "    i = (i + 1) % W\n",
    "\n",
    "    # Esperar fin de intervalo\n",
    "    time.sleep(max(0, intervalo_s - (time.time() - t_inicio)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9f3c2-119e-4f6b-b7ac-e072bcc55760",
   "metadata": {},
   "source": [
    "#### **Agregación distribuida con HyperLogLog**\n",
    "\n",
    "En un entorno distribuido, el reto de contar elementos únicos , ya sean usuarios activos, eventos distintos o claves de partición— puede convertirse en un cuello de botella si intentamos enviar todos los ítems brutos al nodo central. Aquí es donde brilla HyperLogLog: cada nodo procesa localmente su porción de datos y construye un pequeño \"sketch\" de cardinalidad, que luego se fusiona de forma trivial, sin necesidad de reenviar millones de registros.\n",
    "\n",
    "Imaginemos un clúster de procesamiento de logs de acceso web. Cada servidor recibe decenas de miles de peticiones por segundo y necesita estimar cuántos visitantes únicos ha atendido. Con HyperLogLog, cada servidor mantiene un objeto `hll = HyperLogLog(b=14)` en memoria. A medida que llegan URLs, cabeceras o identificadores de sesión, el servidor hace:\n",
    "\n",
    "```python\n",
    "hll.add(user_id_hash)\n",
    "```\n",
    "\n",
    "Al finalizar el intervalo, por ejemplo, un minuto el sketch resultante ocupa apenas unos 16 KB ($2^{14}$ registros) y se envía al agregador central. Este, en lugar de recibir una lista de millones de hashes, recibe un array contiguo de enteros. La fusión es una operación $O(m)$ en la que para cada posición del vector se toma el máximo:\n",
    "\n",
    "```python\n",
    "hll_global = hll_shard1\n",
    "for hll_shard in [hll_shard2, hll_shard3, ...]:\n",
    "    hll_global |= hll_shard\n",
    "```\n",
    "\n",
    "El método `__or__` internamente invoca `merge()`, garantizando que `hll_global.estimate()` devuelva la cardinalidad aproximada total de todo el clúster.\n",
    "\n",
    "**Ejemplo en Redis**\n",
    "\n",
    "Redis incorpora este patrón de manera nativa con los comandos `PFADD`, `PFMERGE` y `PFCOUNT`. Cada instancia de tu aplicación puede hacer:\n",
    "\n",
    "```redis\n",
    "PFADD visits:shard1 user_1 user_2 user_3\n",
    "PFADD visits:shard2 user_4 user_5 user_2\n",
    "```\n",
    "\n",
    "y luego, en el extremo central:\n",
    "\n",
    "```redis\n",
    "PFMERGE visits:global visits:shard1 visits:shard2\n",
    "PFCOUNT visits:global\n",
    "# → (integer) 5\n",
    "```\n",
    "\n",
    "Redis maneja la serialización del sketch y la fusión en segundo plano, de modo que tu único trabajo es emitir esos comandos.\n",
    "\n",
    "**Integración en PostgreSQL y BigQuery**\n",
    "\n",
    "En PostgreSQL, la extensión `hll` nos permite incluir HyperLogLog directamente en consultas SQL. Por ejemplo:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  region,\n",
    "  hll_cardinality(hll_union_agg(hll_hash_text(user_id))) AS usuarios_unicos\n",
    "FROM access_logs\n",
    "GROUP BY region;\n",
    "```\n",
    "\n",
    "Aquí cada fragmento del cluster ejecuta su agregación local, y PostgreSQL fusiona los sketches antes de calcular la cardinalidad.\n",
    "\n",
    "De forma similar, en Google BigQuery basta con:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  country,\n",
    "  APPROX_COUNT_DISTINCT(user_id, 0.01) AS usuarios_unicos\n",
    "FROM `dataset.events`\n",
    "GROUP BY country;\n",
    "```\n",
    "\n",
    "BigQuery descompone el trabajo en shards, calcula un HLL por fragmento y fusiona todo automáticamente, todo de manera interna.\n",
    "\n",
    "\n",
    "**Spark y Flink: conteos a escala de streaming**\n",
    "\n",
    "En Apache Spark, la función de DataFrame `approx_count_distinct(col, rsd)` usa internamente HLL++:\n",
    "\n",
    "```python\n",
    "df.groupBy(\"session_id\") \\\n",
    "  .agg(F.approx_count_distinct(\"user_id\", rsd=0.01).alias(\"uniq_users\"))\n",
    "```\n",
    "\n",
    "Spark mapea cada partición a un HLL, envía los sketches a los reducers y fusiona sin mover los datos individuales.\n",
    "\n",
    "Análogamente, en Flink un `AggregateFunction` puede mantener un HLL por ventana de tiempo. Cuando se redistribuye el trabajo o reequilibra el paralelismo, Flink fusiona automáticamente los sketches de cada subtask usando el mismo operador de \"máximo por bucket\".\n",
    "\n",
    "Con este modelo, HyperLogLog se convierte en la piedra angular de cualquier sistema que necesite contar alto volumen de ítems de forma eficiente, manteniendo un uso de memoria constante, operaciones de red ligeras y precisión suficiente (error típico <1 %) para toma de decisiones en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2f9f5-7efb-4741-accd-ba254af259a9",
   "metadata": {},
   "source": [
    "#### **Ejercicios**\n",
    "\n",
    "**1. Estimación de frecuencia y elementos mayoritarios**\n",
    "\n",
    "1. **Elemento mayoritario**  \n",
    "   - Dado un flujo de $n$ elementos en el que sabes que existe uno que aparece más de $\\lfloor n/2\\rfloor$ veces, explica paso a paso cómo el algoritmo de Boyer–Moore encuentra ese elemento con espacio $O(1)$.  \n",
    "   - Justifica por qué basta una sola pasada (más la verificación opcional) para garantizar la corrección.\n",
    "\n",
    "2. **Heavy Hitters generales (Misra–Gries)**  \n",
    "   - Para un umbral $\\varphi$, razona por qué con $k=1/\\varphi$ contadores se capturan todos los elementos cuya frecuencia supera $\\varphi n$.  \n",
    "   - Describe un escenario (por ejemplo, conteo de palabras en un log de servidor) y diseña cómo asignarías $\\varphi$ para encontrar los \"top-10\" más frecuentes.\n",
    "\n",
    "\n",
    "**2. Count–Min Sketch: funcionamiento interno**\n",
    "\n",
    "1. **Fase de actualización**  \n",
    "   - Ilustra con un ejemplo numérico (números pequeños) qué sucede en la matriz de contadores al insertar un mismo elemento tres veces seguidas.  \n",
    "   - Compara el comportamiento de la **actualización normal** vs. la **conservadora** y discute en qué situaciones la segunda mejora la precisión.\n",
    "\n",
    "2. **Fase de estimación**  \n",
    "   - Explica por qué la estimación toma el **mínimo** de las $d$ celdas y no, por ejemplo, la media.  \n",
    "   - Analiza cómo influyen las colisiones en cada celda y cómo el mínimo atenúa ese sesgo.\n",
    "\n",
    "3. **Casos de uso**  \n",
    "   - **Top-k restless sleepers**: propone un diseño de cómo mantiene el heap de tamaño $k$ junto al CMS, detallando la lógica de inserción y extracción de candidatos.  \n",
    "   - **Similitud distribucional de palabras**: plantea cómo combinar dos CMS (marginal y conjunto) para estimar el PMI de un par de palabras.\n",
    "\n",
    "4. **Error vs. espacio**  \n",
    "   - A partir de los parámetros $\\varepsilon$ y $\\delta$, formula explícitamente $w$ y $d$.  \n",
    "   - Para un flujo de $10^8$ eventos y un error relativo máximo del 0.5%, calcula $w$ y $d$ para $\\delta=0.01$.  \n",
    "   - Debate el impacto de doblar $w$ o doblar $d$ sobre el error y la memoria.\n",
    "\n",
    "5. **Implementación conceptual**  \n",
    "   - Describe con tus propias palabras la estructura de datos interna (matriz, semillas, total).  \n",
    "   - Explica qué validaciones harías en el constructor para garantizar parámetros válidos y cómo documentarías la clase para un equipo de desarrollo.\n",
    "\n",
    "6. **Intuición matemática**  \n",
    "   - Deriva el límite de Markov que da origen a la probabilidad de colisión $\\Pr[\\text{error} \\ge \\varepsilon N]\\le 1/e$.  \n",
    "   - Extiende el razonamiento al caso de $d$ filas independientes y obtén la condición sobre $d$.\n",
    "\n",
    "**3. Consultas por rangos con Count–Min Sketch**\n",
    "\n",
    "1. **Intervalos diádicos**  \n",
    "   - Explica por qué todo intervalo $[L,R]$ se puede cubrir con a lo sumo $2\\log_2U$ bloques diádicos.  \n",
    "   - Para $U=32$ y $[L,R]=[5,26]$, descompón manualmente en bloques de la forma $(i,j)$.\n",
    "\n",
    "2. **Fase de actualización por niveles**  \n",
    "   - Diseña el flujo de datos que llega (valores enteros) y detalla cómo actualizas cada CMS de nivel $i$.  \n",
    "   - Reflexiona sobre el coste en tiempo y memoria cuando $U$ crece de $10^3$ a $10^6$.\n",
    "\n",
    "3. **Fase de estimación de rango**  \n",
    "   - Tras la descomposición diádica, argumenta cómo la suma de estimaciones mantiene la propiedad de cota superior.  \n",
    "   - Discute cómo calibrar $\\varepsilon$ para que el **error aditivo total** siga siendo aceptable en consultas largas.\n",
    "\n",
    "4. **Cálculo de intervalos diádicos**  \n",
    "   - Describe el algoritmo \"greedy\" paso a paso para encontrar el bloque más grande alineado con el inicio.  \n",
    "   - Explica por qué la operación bitwise `L & -L` devuelve la mayor potencia de dos que divide a $L$.\n",
    "\n",
    "\n",
    "**Estimación de cardinalidad y HyperLogLog**\n",
    "\n",
    "1. **Conteo de elementos distintos**  \n",
    "   - Compara el enfoque de **Flajolet–Martin** con un conteo exacto: ¿qué información almacena cada bit del registro?  \n",
    "   - Justifica por qué basta con $O(\\log\\log N)$ bits para aproximar $\\log N$.\n",
    "\n",
    "2. **Diseño incremental de HyperLogLog**  \n",
    "   - **Primer corte (probabilistic counting)**: explica la estadística de la posición del primer '1' en el hash y cómo se traduce a $\\hat n = 2^R$.  \n",
    "   - **Stochastic averaging**: describe el \"truco de los limones\" (varios registros y promedio) y por qué reduce la varianza.  \n",
    "   - **LogLog vs. HyperLogLog**: analiza la diferencia clave entre usar promedio aritmético y promedio armónico.\n",
    "\n",
    "3. **Experimento práctico**  \n",
    "   - Propón un diseño de mini-experimento variando el número de buckets $m$, escribe los pasos de evaluación (sin código) y qué métricas registrarías (error medio, desviación).\n",
    "\n",
    "4. **Caso de uso: Catching worms**  \n",
    "   - Imagina un sistema de detección de gusanos en logs de red: define qué representa cada elemento y cómo interpreta el HLL para alertar a operaciones.\n",
    "\n",
    "5. **Agregación distribuida con HLL**  \n",
    "   - Detalla cómo fusionar múltiples estructuras HLL de diferentes nodos y por qué la operación de \"máximo por bucket\" mantiene la corrección.  \n",
    "   - Discute el coste de comunicación y memoria en comparación con un conteo de cardinalidad exacto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8babc-28db-4585-89a1-5ee0c6e122c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
