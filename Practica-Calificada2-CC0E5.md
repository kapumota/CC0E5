### **Práctica calificada 2 CC0E5**

**Consideraciones generales para todos los proyectos:**

* **Alcance:** Se requiere una implementación funcional y eficiente del núcleo de la estructura o algoritmo, incluyendo optimizaciones clave discutidas en la literatura relevante o variantes que ofrezcan mejoras de rendimiento significativas. Las optimizaciones extremas o todas las variantes posibles pueden quedar fuera del alcance si se justifica, pero se espera una exploración más allá de la implementación básica.
* **Video (mínimo 7-10 minutos):** Deberá contener:
    * Una explicación teórica profunda del concepto, incluyendo una comparativa con estructuras o algoritmos alternativos y una justificación clara de su eficiencia, especialmente en términos de cotas asintóticas.
    * Una descripción detallada de la estructura de datos o algoritmo implementado, especificando las invariantes clave y cómo la implementación las mantiene a lo largo de las operaciones.
    * Un recorrido por las partes más complejas o críticas del código, explicando las decisiones de diseño tomadas y por qué se eligieron ciertas aproximaciones sobre otras.
    * Una demostración en vivo de la funcionalidad, que debe incluir escenarios típicos y escenarios que pongan a prueba la robustez y correctitud de la estructura.
    * Una discusión honesta de los desafíos encontrados durante el desarrollo, las soluciones implementadas para superarlos, y posibles trabajos futuros o extensiones de la implementación actual.
    * Un análisis breve, pero conciso, de los resultados obtenidos del profiling o los benchmarks realizados, conectándolos con las expectativas teóricas.
* **Repositorio:** Deberá ser público y contener:
    * El código fuente completo, que debe ser claro, estar bien comentado (explicando la lógica detrás de secciones complejas) y estructurado modularmente para facilitar su comprensión y mantenimiento. Los lenguajes permitidos son Python, C++ o Rust.
    * Un archivo `README.md` exhaustivo que incluya:
        * Una descripción del proyecto y la motivación teórica detrás de la estructura o algoritmo elegido.
        * Instrucciones claras y precisas para la compilación y ejecución del código, incluyendo todas las dependencias necesarias.
        * Una explicación de la estructura del proyecto (organización de archivos y directorios) y del diseño de la API pública de la estructura implementada.
        * Una documentación detallada de la API, especificando parámetros de entrada, valores de retorno, y el manejo de excepciones o errores para cada función pública.
    * Archivos de demostración o *drivers* que muestren el uso avanzado y variado de la estructura de datos, ilustrando sus capacidades.
    * Una suite exhaustiva de pruebas unitarias, buscando una alta cobertura del código, y scripts dedicados para realizar profiling y benchmarking comparativo de la implementación.
* **Documentación:** Además del `README.md`, se entregará un breve informe técnico (aproximadamente 3-5 páginas) en formato PDF, explicando:
    * La teoría subyacente en profundidad, incluyendo pruebas de correctitud o eficiencia para los aspectos más críticos si son relevantes y manejables dentro del alcance del proyecto.
    * Las decisiones de diseño más importantes tomadas durante la implementación, discutiendo las alternativas consideradas y por qué se optó por la solución final.
    * Un análisis empírico detallado de la complejidad y el rendimiento observado, comparándolo con las expectativas teóricas. Siempre que sea posible, este análisis debe incluir una comparación con otras implementaciones (por ejemplo, las disponibles en bibliotecas estándar o implementaciones de referencia).
    * Una discusión sobre las limitaciones de la implementación actual y posibles extensiones o mejoras futuras.
* **Integrantes**: Los proyectos se realizarán en grupos de a lo más dos integrantes.

> Condición especial: si no entregan el video de manera conjunta, la calificación máxima en el proyecto será 5 puntos de un total de 20.


### **Propuestas de proyectos**

**Proyecto 1: Deques de Kaplan y Tarjan**

* **Tema(s) principal(es):** Deques concatenables rápidos de Kaplan y Tarjan, con énfasis en la comprensión y aplicación de la técnica subyacente (por ejemplo, árboles de nivel enlazados o estructuras similares que fundamentan su eficiencia).
* **Descripción:** Los deques (colas de doble extremo) son estructuras de datos fundamentales en ciencias de la computación. Haim Kaplan y Robert Tarjan presentaron una estructura de datos notable que permite la concatenación de dos deques, además de otras operaciones estándar de deque, en tiempo amortizado `O(1)` o `O(log N)` dependiendo de la variante específica implementada. Esta eficiencia es significativamente superior a las implementaciones ingenuas que requerirían reconstrucciones costosas.
* **Objetivos de implementación:**
    -  Implementar una deque utilizando fielmente la representación subyacente propuesta por Kaplan y Tarjan. Esto requiere investigar y comprender cuál es la estructura específica (a menudo basada en árboles balanceados especializados o listas con punteros "dedo" y metadatos adicionales) que permite alcanzar las cotas de rendimiento teóricas.
    -  Implementar las operaciones básicas de una deque: `push_front`, `pop_front`, `push_back`, `pop_back`, asegurando su correcta funcionalidad.
    -  Implementar la operación clave: `concatenate(deque1, deque2)`, asegurando que la cota de tiempo amortizado esperada según la teoría de Kaplan y Tarjan se cumpla o se aproxime razonablemente.
    -  Implementar la operación `split(deque, position)` que divide una deque en dos en una posición dada, logrando la cota de tiempo eficiente que la estructura subyacente de Kaplan y Tarjan permita.
    -  Analizar teórica y empíricamente el coste amortizado de las operaciones de concatenación y división, así como de las operaciones básicas.
* **Desafíos potenciales:** El principal desafío radica en comprender y reproducir correctamente la estructura interna compleja y los algoritmos que garantizan la eficiencia. El manejo preciso de punteros, referencias, la actualización consistente de metadatos durante la concatenación y la división, y la gestión de casos borde son críticos para una implementación funcional y eficiente.
* **Ideas para el video y repositorio:**
    * Video: Explicar la ineficiencia de concatenar deques de forma ingenua y la motivación detrás de la estructura de Kaplan y Tarjan. Mostrar detalladamente la estructura interna de la implementación. Demostrar la creación de varias deques, su llenado, la concatenación y la división de las mismas. Presentar un benchmark simple que compare el rendimiento de la concatenación de la implementación con una alternativa ingenua (por ejemplo, basada en listas enlazadas simples o arrays dinámicos) para evidenciar la mejora.
    * Repo: Incluir una suite de pruebas exhaustiva que verifique la correcta funcionalidad de la deque después de secuencias complejas y mixtas de operaciones (push, pop, concatenate, split), incluyendo casos borde. Incluir un análisis de rendimiento que intente validar las cotas amortizadas, por ejemplo, midiendo el tiempo total de N operaciones y graficando los resultados para diferentes tamaños de N y tipos de secuencias.

**Proyecto 2: Sequence heaps**

* **Tema(s) principal(es):** Heaps de secuencia (Sequence heaps), con un enfoque en la optimización de la localidad de caché y la medición experimental de su impacto.
* **Descripción:** Los Sequence Heaps son una estructura de cola de prioridad que busca optimizar el rendimiento aprovechando la jerarquía de memoria, específicamente las cachés del procesador. A diferencia de los heaps binarios tradicionales que pueden dispersar elementos relacionados en memoria, los Sequence Heaps intentan mantener elementos que probablemente se accedan secuencialmente (o en proximidad temporal) más juntos en memoria, reduciendo así los fallos de caché.
* **Objetivos de implementación:**
    -  Investigar a fondo e implementar la estructura de un Sequence Heap, o una variante similar como un k-heap cache-alineado, justificando la elección del diseño específico en términos de sus beneficios esperados para la localidad de datos.
    -  Implementar las operaciones de cola de prioridad: `insert`, `extract_min`. Además, es obligatorio implementar `decrease_key` y `delete` si la literatura sobre Sequence Heaps o la variante elegida las soporta de manera eficiente.
    -  Diseñar y ejecutar un esquema de benchmarking cuidadoso (utilizando herramientas como `perf` en Linux, Intel VTune, o contadores de rendimiento de hardware accesibles programáticamente si es posible) para intentar medir el impacto real en métricas relacionadas con la caché, como la tasa de fallos de caché (cache miss rate) o ciclos por instrucción (CPI). Comparar estos resultados con un heap binario estándar y, opcionalmente, un d-ary heap.
* **Desafíos potenciales:** La gestión eficiente de los bloques de elementos, la reorganización cuando los bloques se llenan o vacían, y especialmente, lograr un beneficio de caché medible y atribuible directamente al diseño de la estructura. Esto puede requerir un control fino sobre la alocación de memoria, el alineamiento de datos, y la creación de patrones de acceso específicos en los benchmarks que exploten las fortalezas del Sequence Heap.
* **Ideas para el video y repositorio:**
    * Video: Comparar conceptualmente el Sequence Heap con un heap binario, enfocándose en las diferencias de layout en memoria. Explicar cómo las operaciones específicas (inserción, extracción, manejo de bloques) están diseñadas para mejorar la localidad de caché. Presentar de forma clara los resultados del benchmark de caché, discutiendo su significancia.
    * Repo: Implementación de la estructura. Scripts de benchmark detallados, incluyendo el código para generar las cargas de trabajo. Discusión de los resultados de los benchmarks, incluyendo posibles factores de confusión, limitaciones del entorno de prueba, y la reproducibilidad de los resultados.

**Proyecto 3: Ham sandwich trees**

* **Tema(s) principal(es):** Árboles de sándwich de jamón (Ham Sandwich Trees), con énfasis en el algoritmo de búsqueda de hiperplanos bisectores y la capacidad de realizar consultas con polígonos más generales, además de una visualización interactiva.
* **Descripción:** El teorema del sándwich de jamón establece que dadas *n* regiones (masas) en un espacio *n*-dimensional, es siempre posible encontrar un hiperplano que divida cada una de esas regiones exactamente por la mitad en términos de su volumen (o medida). Los árboles de sándwich de jamón son una estructura de datos jerárquica basada en este principio, utilizada para realizar búsquedas de rango eficientes, especialmente en 2D o dimensiones superiores, y son particularmente útiles cuando los rangos de consulta no están alineados con los ejes coordenados.
* **Objetivos de implementación:**
    -  Implementar la construcción de un árbol de sándwich de jamón para un conjunto de puntos 2D. Esto implicará investigar e implementar un algoritmo robusto para encontrar líneas de corte (bisectrices) que se aproximen a una bisección equiparticionada de los conjuntos de puntos relevantes en cada paso recursivo. No es necesario que la bisección sea perfecta si el algoritmo para ello es excesivamente complejo, pero la elección del método de bisección debe estar bien justificada.
    -  Implementar una función de búsqueda de puntos que residan dentro de una región poligonal convexa arbitraria especificada por el usuario (no limitarse solo a rectángulos no alineados con los ejes).
    -  **(Reto adicional opcional)** Investigar y discutir la extensión teórica de la estructura a 3D, detallando los desafíos adicionales que esto implicaría, aunque la implementación completa en 3D podría ser demasiado extensa para el alcance del proyecto.
    -  Implementar una visualización interactiva (por ejemplo, utilizando Pygame, Matplotlib con widgets interactivos, o una librería de JavaScript como D-js o P-js si se opta por una demo web) que muestre el conjunto de puntos, el árbol construido (las líneas de partición) y permita al usuario definir interactivamente la región poligonal de consulta, resaltando los puntos encontrados.
* **Desafíos potenciales:** La matemática y los algoritmos para encontrar buenas líneas de corte (o planos en 3D) que logren bisecciones razonables pueden ser complejos. La implementación de la lógica de búsqueda de puntos dentro de polígonos convexos arbitrarios requiere un manejo cuidadoso de la geometría. La robustez numérica, debido a cálculos con puntos flotantes en geometría computacional, puede ser un problema a considerar.
* **Ideas para el video y repositorio:**
    * Video: Explicar el teorema del sándwich de jamón de forma intuitiva. Mostrar cómo se construye el árbol paso a paso con un pequeño conjunto de puntos y las líneas de corte elegidas. Demostrar búsquedas con diferentes polígonos convexos definidos por el usuario, incluyendo casos donde el polígono de consulta cruza múltiples regiones del árbol. Mostrar la visualización interactiva en acción.
    * Repo: Código fuente de la estructura y la visualización. Incluir una discusión detallada de la heurística o algoritmo de bisección utilizado y una evaluación (aunque sea cualitativa) de su efectividad. Proveer ejemplos de conjuntos de datos y polígonos de consulta.

**Proyecto 4: Weak AVL trees (WAVL)**

* **Tema(s) principal(es):** Árboles AVL débiles (Weak AVL trees), con un análisis profundo de las invariantes de rango, las reglas de rebalanceo y una comparación empírica detallada del número de operaciones de rebalanceo frente a otros árboles balanceados.
* **Descripción:** Los WAVL trees son una variante de los árboles binarios de búsqueda auto-balanceables que ofrecen una alternativa más simple en términos de reglas de rebalanceo y, en algunos casos, más eficiente que los árboles Rojo-Negro. Mantienen el equilibrio utilizando rangos (ranks) asociados a los nodos y un conjunto de reglas de rebalanceo más sencillas que los AVL clásicos, permitiendo ciertas diferencias de altura que un AVL estricto no permitiría, pero aun así garantizando una altura logarítmica.
* **Objetivos de implementación:**
    -  Implementar un WAVL tree con las operaciones fundamentales: `insert`, `delete`, y `search`.
    -  Implementar correctamente y de forma precisa todas las rotaciones (simples y dobles, si aplican según la definición de WAVL que se siga) y las reglas de promoción y democión de rangos para mantener estrictamente las invariantes del WAVL tree. Es crucial entender cómo los rangos definen el balance del árbol.
    -  Realizar un análisis empírico comparativo del número de operaciones de rebalanceo (específicamente, contar rotaciones, promociones de rango, y demociones de rango por separado) y la altura promedio y máxima del árbol. Esta comparación debe hacerse frente a implementaciones de árboles AVL estándar y Árboles Rojo-Negro, utilizando diversas secuencias de operaciones (por ejemplo, inserciones aleatorias, inserciones secuenciales, inserciones seguidas de eliminaciones aleatorias, secuencias que induzcan muchos rebalanceos).
    -  **(Opcional avanzado pero muy valioso)** Investigar e implementar operaciones de `join` (unión de dos WAVL trees) y `split` (división de un WAVL tree por una clave) si la literatura describe estas operaciones de forma clara y eficiente para WAVL trees.
* **Desafíos potenciales:** El principal desafío es entender y aplicar correctamente todas las sutilezas de las invariantes de los rangos y las reglas de rebalanceo, que aunque más simples que las de los árboles Rojo-Negro, deben ser implementadas sin errores. Diseñar un conjunto exhaustivo de pruebas unitarias para validar todos los casos de rebalanceo, especialmente los más complejos que surgen de eliminaciones, es fundamental.
* **Ideas para el video y repositorio:**
    * Video: Explicar las diferencias conceptuales y las invariantes de los WAVL trees en comparación con los AVL estándar o los Árboles Rojo-Negro. Mostrar paso a paso, de forma visual, inserciones y eliminaciones que provoquen múltiples tipos de rebalanceos (promociones, demociones, diferentes tipos de rotaciones), explicando las reglas específicas del WAVL tree que se aplican en cada situación. Presentar gráficos comparativos claros del análisis de rendimiento (número de rotaciones, altura del árbol) obtenidos en los benchmarks.
    * Repo: Implementación completa y robusta del WAVL tree. Scripts de benchmark detallados para generar los datos comparativos contra AVL y RBT, así como los gráficos resultantes. Incluir una discusión sobre la eficiencia relativa observada.

**Proyecto 5: Estructura RMQ de Durocher (sin método de los cuatro Rusos)**

* **Tema(s) principal(es):** Estructura RMQ (Range Minimum Query) de Durocher, enfocándose en la implementación precisa que logra `O(n)` de tiempo de preprocesamiento y `O(1)` por consulta sin recurrir a la complejidad del "Método de los Cuatro Rusos". El objetivo es compararla con la tabla dispersa (sparse table) estándar.
* **Descripción:** El problema de la consulta de rango mínimo (RMQ) consiste en preprocesar un array `A` de elementos comparables de tal manera que se pueda encontrar rápidamente la posición o el valor del elemento mínimo en cualquier subsegmento `A[i...j]`. La estructura de Durocher (o enfoques similares como el de Fischer-Heun para RMQ) es una de las que logra un tiempo de preprocesamiento de `O(n)` y un tiempo de consulta de `O(1)`. Esto se consigue, típicamente, combinando una tabla dispersa para bloques grandes con una técnica más eficiente para manejar las consultas dentro de bloques pequeños, como la precomputación de todas las respuestas posibles para patrones de bloques pequeños, evitando así la dependencia del `log log n` del método de los cuatro rusos para esos bloques.
* **Objetivos de implementación:**
    -  Investigar y entender en detalle la variante específica de la estructura RMQ que evita el método de los cuatro rusos para los bloques pequeños. Esto usualmente implica una descomposición del array en bloques de tamaño `k` (por ejemplo, `k = (log n)/2`), una tabla dispersa sobre los mínimos de estos bloques, y una forma de responder consultas dentro de los bloques o que cruzan pocos bloques en `O(1)` (a menudo mediante precomputación de respuestas para todos los posibles "tipos" o "patrones" de bloques pequeños, donde el tipo se puede codificar en pocos bits).
    -  Implementar la fase de preprocesamiento en tiempo `O(n)`. Esto incluye la construcción de la tabla dispersa para los mínimos de los bloques y la estructura auxiliar (por ejemplo, tablas de lookup o precomputación) para las consultas dentro de los bloques pequeños.
    -  Implementar la función `query(i, j)` que devuelve el índice o valor del mínimo en `A[i...j]` en tiempo `O(1)`.
    -  Comparar empíricamente el tiempo de preprocesamiento, el uso de memoria (constante de espacio) y el tiempo de consulta de esta implementación con una implementación de Sparse Table estándar (que tiene `O(n log n)` de preprocesamiento y `O(1)` de consulta).
* **Desafíos potenciales:** Comprender cómo se combinan las diferentes ideas (la tabla dispersa sobre los representantes de bloques, la división en bloques, la caracterización de los patrones de bloques pequeños, y la precomputación de respuestas para estos patrones) para lograr la consulta en `O(1)` y el preprocesamiento en `O(n)`. La manipulación de bits para codificar los patrones de bloques pequeños y para indexar en las tablas precalculadas puede ser necesaria y requiere cuidado.
* **Ideas para el video y repositorio:**
    * Video: Explicar claramente el problema RMQ. Describir la estructura de la Sparse Table ingenua y luego detallar cómo la estructura implementada (Durocher/Fischer-Heun) optimiza el preprocesamiento o el manejo de bloques pequeños para lograr `O(n)` global. Demostrar el preprocesamiento y luego realizar varias consultas rápidas. Presentar los resultados clave de la comparación de rendimiento y uso de memoria con la Sparse Table estándar.
    * Repo: Implementación de la estructura RMQ avanzada y la Sparse Table estándar para comparación. Incluir tests unitarios que verifiquen la correctitud de ambas contra una búsqueda lineal ingenua. Proveer scripts para el benchmark comparativo y una discusión de los resultados, resaltando las ventajas y posibles desventajas (como constantes de espacio mayores) de la estructura `O(n)`/`O(1)`.

**Proyecto 6: Algoritmo de árbol de sufijos de Farach (o Ukkonen como alternativa viable)**

* **Tema(s) principal(es):** Algoritmo de construcción de árbol de sufijos en tiempo lineal. Dada la complejidad intrínseca, se permite elegir entre la implementación del algoritmo de Farach o el algoritmo de Ukkonen, ambos capaces de construir árboles de sufijos en tiempo lineal.
* **Descripción:** Los árboles de sufijos son estructuras de datos fundamentales y extremadamente potentes para el procesamiento de cadenas, con aplicaciones en bioinformática, búsqueda de texto, compresión de datos y más. El algoritmo de Farach es un elegante algoritmo de "divide y vencerás" para construir un árbol de sufijos en tiempo lineal `O(n)`. Es conocido por su belleza teórica, aunque su implementación completa puede ser compleja. El algoritmo de Ukkonen es otro método popular que construye el árbol de sufijos en línea, también en tiempo `O(n)`, utilizando "punteros de sufijo" y una serie de optimizaciones ingeniosas.
* **Objetivos de implementación:**
    -  Comprender en profundidad los pasos clave del algoritmo elegido:
        * Si se elige Farach: la construcción recursiva para sufijos impares, luego para sufijos pares utilizando la información de los impares, y finalmente la fusión de los dos árboles de sufijos resultantes.
        * Si se elige Ukkonen: la construcción implícita del árbol sufijo a sufijo, las extensiones de sufijos (reglas 1, 2, 3), el uso de punteros de sufijo para acelerar las transiciones, y la regla de "mostrar y parar" (show-and-stop trick) o "skip/count".
    -  Intentar una implementación completa del algoritmo elegido que apunte a la eficiencia `O(n)` (o `O(n log |Σ|)` si se utiliza una estructura como un mapa o árbol balanceado para las transiciones de los nodos en lugar de arrays de tamaño fijo o tablas hash perfectas, donde `|Σ|` es el tamaño del alfabeto). Si no se logra estrictamente la linealidad pura (`O(n)`), se debe justificar claramente dónde reside el cuello de botella que incrementa la complejidad.
    -  Una vez construido el árbol de sufijos, implementar al menos dos aplicaciones no triviales sobre él. Ejemplos incluyen: encontrar la subcadena común más larga de k cadenas, encontrar el palíndromo más largo dentro de una cadena, contar el número de subcadenas distintas, o resolver el problema de la supercadena más corta para un conjunto pequeño de cadenas.
* **Desafíos potenciales:** Este proyecto sigue siendo, probablemente, el más complejo algorítmicamente de la lista. La gestión correcta de los punteros de sufijo y las múltiples optimizaciones en Ukkonen, o la recursión compleja y el proceso de fusión en Farach, son extremadamente delicados y propensos a errores sutiles. Lograr la linealidad real requiere una atención meticulosa a muchos detalles de implementación.
* **Ideas para el video y repositorio:**
    * Video: Explicar qué es un árbol de sufijos y por qué es una estructura de datos tan útil. Detallar los pasos del algoritmo elegido (Farach o Ukkonen) con claridad. Visualizar la construcción del árbol paso a paso para una cadena de ejemplo, mostrando el estado de la estructura, los punteros clave (como los punteros de sufijo en Ukkonen), y las decisiones tomadas en cada fase. Demostrar las aplicaciones implementadas sobre el árbol construido.
    * Repo: Implementación del algoritmo elegido. Documentar exhaustivamente las partes más complejas del algoritmo, explicando la lógica y las invariantes. Incluir pruebas unitarias robustas para la construcción del árbol y para las aplicaciones desarrolladas. Analizar el rendimiento (tiempo de construcción, uso de memoria) y compararlo con las expectativas teóricas, discutiendo cualquier desviación.

**Proyecto 7: Dinamización de Bentley-Saxe aplicada a una estructura de datos geométrica**

* **Tema(s) principal(es):** Técnica de dinamización de Bentley-Saxe, aplicada a una estructura de datos geométrica estática donde las actualizaciones (inserciones y/o eliminaciones) son inherentemente costosas o complejas de manejar incrementalmente.
* **Descripción:** La técnica de Bentley-Saxe es un método general y poderoso para convertir estructuras de datos que solo funcionan eficientemente en conjuntos estáticos de elementos (o que son muy costosas de actualizar) en estructuras de datos dinámicas que pueden soportar inserciones y, a veces, eliminaciones de manera eficiente (en un sentido amortizado). Funciona manteniendo múltiples versiones de la estructura estática, usualmente de tamaños crecientes exponencialmente (por ejemplo, $S_0, S_1, S_2, \dots$ donde $S_i$ opera sobre $2^i$ elementos). Las inserciones provocan reconstrucciones de estas sub-estructuras, y las consultas se realizan interrogando a todas las sub-estructuras relevantes y combinando sus resultados.
* **Objetivos de implementación:**
    -  Elegir una estructura de datos estática geométrica interesante para la cual las actualizaciones directas son difíciles. Algunas opciones podrían ser:
        * **Construcción de la envolvente convexa (convex hull) estática 2D:** Por ejemplo, utilizando el algoritmo de Monotone Chain (`O(n log n)` para construir). La consulta dinámica sería "¿El punto P está dentro de la envolvente convexa del conjunto actual de puntos?".
        * **(Muy ambicioso)** **diagrama de Voronoi estático 2D:** Para un conjunto de sitios, la construcción es `O(n log n)`. La consulta dinámica sería "¿Cuál es el sitio (del conjunto actual) más cercano al punto de consulta P?".
        * **Árbol k-d (k-dimensional tree) estático:** Utilizado para búsquedas de rango o vecinos más cercanos. La reconstrucción es `O(n log n)`.
        La elección debe justificarse en términos de por qué la dinamización es apropiada.
    -  Implementar la técnica de Bentley-Saxe para la estructura estática elegida:
        * Manejar una colección de estructuras estáticas $S_0, S_1, S_2, \dots$, donde cada $S_i$ almacena un conjunto disjunto de los elementos actualmente activos y tiene un tamaño máximo (usualmente $2^i$).
        * Implementar la operación `insert(elemento)`. Esto implicará agregar el nuevo elemento a $S_0$. Si $S_0$ se llena, sus elementos se fusionan con los de $S_1$, y así sucesivamente, provocando reconstrucciones en cascada de las estructuras estáticas.
        * Implementar las consultas relevantes para la estructura base elegida (por ejemplo, `is_inside_convex_hull(punto)`, `find_nearest_neighbor(punto)`). Esto requerirá consultar todas las estructuras $S_i$ activas y combinar adecuadamente sus resultados.
    -  Implementar la operación `delete(elemento)`. Esto es más complejo con Bentley-Saxe. Una aproximación es utilizar la técnica de reconstrucción global periódica cuando el número de elementos eliminados alcanza un umbral. Otra es mantener listas de elementos "marcados como eliminados" por cada sub-estructura $S_i$ y filtrar los resultados de las consultas, o incluso reconstruir las $S_i$ afectadas de forma más perezosa.
    -  Analizar el coste amortizado de las operaciones de inserción y eliminación, así como el coste de las consultas en la estructura dinamizada.
* **Desafíos potenciales:** La complejidad intrínseca de la estructura de datos geométrica estática base elegida. La correcta implementación de la lógica de reconstrucción y fusión de las sub-estructuras $S_i$ cuando se insertan elementos, asegurando que los invariantes se mantengan. La gestión eficiente de las eliminaciones es particularmente desafiante y requiere una estrategia bien pensada para evitar una degradación severa del rendimiento.
* **Ideas para el video y repositorio:**
    * Video: Explicar el concepto general de dinamización y la idea específica de Bentley-Saxe. Describir la estructura estática base elegida y por qué es un buen candidato para esta técnica. Mostrar visualmente cómo las inserciones y eliminaciones provocan la creación, fusión o reconstrucción de las sub-estructuras $S_i$. Demostrar cómo se realizan las consultas combinando la información de las múltiples estructuras.
    * Repo: Implementación de la estructura estática base y el framework de Bentley-Saxe sobre ella. Incluir pruebas exhaustivas que demuestren la correctitud de la estructura dinámica tras secuencias mixtas de inserciones, eliminaciones y consultas. Presentar un análisis (teórico o empírico) de los costos amortizados de las operaciones.

**Proyecto 8: Árboles B-ε (B-epsilon) con simulación de I/O y políticas de flushing variadas**

* **Tema(s) principal(es):** Árboles B-ε (B-epsilon trees), con un enfoque en la modelización del costo de I/O (simulado), la exploración y comparación de diferentes estrategias de "flushing" (vaciado) de buffers, y la gestión de eliminaciones.
* **Descripción:** Los Árboles B-ε son una variante de los Árboles B (o B+) diseñados específicamente para entornos donde las operaciones de escritura a memoria persistente (especialmente a disco duro o memoria flash NAND) son significativamente más costosas que las lecturas o las operaciones en memoria RAM. Introducen un "buffer" o "epsilon" ($\epsilon$) de elementos pendientes en cada nodo interno (y a veces en las hojas). Las inserciones (y a veces las eliminaciones) se dirigen primero a estos buffers, y los datos solo se propagan ("flushean") a los nodos hijos o se integran en la estructura principal del nodo cuando los buffers se llenan o bajo ciertas políticas, agrupando así múltiples escrituras lógicas en menos operaciones de escritura física.
* **Objetivos de implementación:**
    -  Implementar la estructura de un nodo de Árbol B-ε, incluyendo el buffer para elementos pendientes (mensajes de inserción/eliminación). Definir claramente el parámetro `B` (orden del árbol) y `ε` (tamaño del buffer).
    -  Implementar las operaciones de `insert` y `search`. La inserción deberá gestionar el flujo de elementos hacia los buffers de los nodos apropiados. La búsqueda deberá considerar tanto los elementos en la estructura principal de los nodos como los elementos en los buffers.
    -  Implementar la operación `delete`, considerando cómo los mensajes de eliminación interactúan con los buffers y la estructura principal del árbol. Esto puede implicar agregar "mensajes de borrado" a los buffers.
    -  Implementar y comparar al menos dos políticas de "flushing" (vaciado) de buffers desde los nodos padres a los nodos hijos. Por ejemplo:
        * Política de "buffer lleno": Flushear el buffer de un nodo solo cuando esté completamente lleno.
        * Política de "flush parcial oportunista": Flushear parte del buffer de un nodo padre a un hijo cuando ese hijo es accedido (leído o escrito) por otra razón.
    -  Simular y contar de forma diferenciada las operaciones que implicarían I/O costoso. Esto incluye "lecturas de nodo desde disco", "escrituras de nodo a disco" (cuando la estructura principal de un nodo se modifica) y "escrituras a buffer" (que se asumen más baratas, quizás en una RAM persistente o como parte de una escritura de nodo agrupada). Comparar estas métricas con un Árbol B estándar (que puede modelarse como un Árbol B-ε con $\epsilon=0$) bajo el mismo modelo de simulación de I/O y para las mismas cargas de trabajo.
* **Desafíos potenciales:** Gestionar la lógica compleja de los buffers, especialmente con diferentes políticas de flushing y durante las operaciones de división de nodos (node splits) y fusión de nodos (node merges) que son inherentes a los Árboles B. Asegurar la sincronización correcta del estado de los buffers con la estructura principal del árbol y mantener las invariantes del Árbol B subyacente es crucial y propenso a errores. El diseño de la simulación de I/O debe ser razonable.
* **Ideas para el video y repositorio:**
    * Video: Explicar la motivación detrás de los árboles B-ε (el alto costo de escritura a disco/flash). Mostrar la estructura de un nodo con su buffer. Demostrar inserciones y eliminaciones, y cómo se llenan los buffers antes de que los cambios se propaguen. Comparar visualmente o con contadores las diferentes políticas de flushing y su impacto en el conteo simulado de operaciones de I/O.
    * Repo: Código de la implementación del Árbol B-ε y del Árbol B estándar para comparación. Un driver que permita configurar la política de flushing, el tamaño del árbol `B` y el tamaño del buffer `ε`. El driver debe ejecutar secuencias de operaciones y reportar las métricas de I/O simuladas. Incluir un análisis de qué política de flushing funciona mejor bajo qué tipo de carga de trabajo (ej. intensiva en inserciones, mixta, etc.).

**Proyecto 9: Árboles binarios de búsqueda desamortizados (aplicado a Splay trees o Scapegoat trees)**

* **Tema(s) principal(es):** Desamortización de árboles binarios de búsqueda (BST), aplicando técnicas formales de desamortización a un BST amortizado conocido (como Splay Trees o Scapegoat Trees) para garantizar cotas de peor caso por operación, en lugar de solo cotas amortizadas.
* **Descripción:** Algunas operaciones en árboles BST con análisis de costo amortizado (como los Splay Trees o los Scapegoat Trees) pueden, en raras ocasiones, tener un costo muy alto en el peor caso para una operación individual, aunque el costo promedio a lo largo de una secuencia de operaciones sea bueno. La desamortización es una técnica que busca convertir estas cotas amortizadas en cotas de peor caso más estrictas para cada operación individual. Esto usualmente se logra realizando pequeñas cantidades de trabajo de rebalanceo o mantenimiento de forma incremental y distribuida con cada operación, en lugar de concentrarlo todo en una sola. El término "Pop Tarts" es un nombre coloquial para algunas de estas ideas, pero el objetivo aquí es una aproximación más formal.
* **Objetivos de implementación:**
    -  Implementar un BST amortizado bien conocido como base. Las opciones más adecuadas son:
        * **Splay tree:** Con su operación `splay` que lleva el nodo accedido a la raíz mediante rotaciones.
        * **Scapegoat tree:** Que detecta desbalance mediante un factor $\alpha$ y reconstruye subárboles enteros para rebalancear.
    -  Investigar y aplicar una técnica de desamortización específica y reconocida para el tipo de árbol elegido.
        * Para Splay Trees: Esto podría implicar técnicas como la "reconstrucción parcial" de caminos durante el splay o alguna forma de "splaying incremental" para asegurar que ninguna secuencia de rotaciones se vuelva excesivamente larga en una sola operación.
        * Para Scapegoat Trees: La desamortización típicamente implica distribuir el trabajo de reconstrucción de un subárbol identificado como "chivo expiatorio" (scapegoat) a lo largo de un número de operaciones posteriores, utilizando un "presupuesto de trabajo" o un contador de desbalance.
    -  El objetivo primordial es lograr una cota de **peor caso** explícita y mejorada para cada operación individual (por ejemplo, `O(log^2 N)` o incluso `O(log N)` si la técnica lo permite y se implementa correctamente), en contraste con la cota amortizada original que podría esconder un peor caso de `O(N)` para una operación aislada.
    -  Analizar empíricamente la distribución de los tiempos de operación individuales. Se deben presentar gráficos o estadísticas que muestren una reducción significativa (o eliminación) de los picos de alta latencia en la versión desamortizada en comparación con la versión puramente amortizada.
* **Desafíos potenciales:** Comprender e implementar correctamente las técnicas de desamortización formales. Estas suelen implicar mantener estado adicional en los nodos o en la estructura global (por ejemplo, un "potencial" como en los análisis con el método del potencial, o un "trabajo pendiente") y realizar pequeñas porciones de trabajo de rebalanceo de manera sistemática y disciplinada. Probar formalmente que la cota de peor caso se cumple puede ser teóricamente difícil, pero se debe argumentar sólidamente y, fundamentalmente, demostrar empíricamente la mejora en la predictibilidad del rendimiento.
* **Ideas para el video y repositorio:**
    * Video: Explicar claramente el concepto de análisis amortizado y la motivación para la desamortización. Describir el BST amortizado base y su comportamiento de rebalanceo que puede llevar a picos de latencia. Luego, detallar la técnica de desamortización específica implementada. Presentar gráficos convincentes de tiempos de operación individuales (por ejemplo, un scatter plot o un histograma de tiempos) que contrasten la versión amortizada (con picos notables) y la versión desamortizada (con un comportamiento más suave y predecible).
    * Repo: Implementaciones tanto del BST amortizado original como de su versión desamortizada. Scripts y herramientas para realizar pruebas que midan y grafiquen los tiempos de N operaciones individuales, diseñadas para evidenciar la reducción (o eliminación) de picos de latencia en la versión desamortizada.

**Proyecto 10: RMQ Sucinto con implementación de estructuras de bits de bajo nivel**

* **Tema(s) principal(es):** RMQ (Range Minimum Query) sucinto, con una implementación que no solo logre la compacidad espacial teórica sino que también involucre la implementación manual (o el uso bien comprendido y justificado de bibliotecas especializadas) de las primitivas sobre secuencias de bits necesarias, como `rank` y `select`.
* **Descripción:** Mientras que algunas estructuras RMQ, como la Sparse Table, se centran en lograr la velocidad de consulta `O(1)` con un preprocesamiento razonable (aunque `O(n log n)` para Sparse Table), las estructuras RMQ sucintas buscan adicionalmente minimizar drásticamente el espacio utilizado para el preprocesamiento. El objetivo es acercarse al mínimo teórico de bits necesario para representar la información que permite responder las consultas RMQ (alrededor de $2n + o(n)$ bits para un array de $n$ elementos), mientras se mantienen consultas muy rápidas (a menudo `O(1)`).
* **Objetivos de implementación:**
    -  Investigar a fondo una técnica de RMQ sucinto que logre un espacio de almacenamiento cercano a $2n + o(n)$ bits. Un enfoque común se basa en la reducción del problema RMQ al problema LCA (Lowest Common Ancestor) sobre el Árbol Cartesiano del array de entrada, y luego utilizar una representación sucinta para el Árbol Cartesiano (como LOUDS - Level Order Unary Degree Sequence, o BP - Balanced Parentheses) junto con operaciones de `rank` y `select` sobre bitvectors.
    -  Implementar (o si se usa una biblioteca, demostrar una comprensión profunda y la capacidad de replicar su funcionalidad básica) las operaciones de `rank(bitvector, index)` (contar bits '1' hasta index) y `select(bitvector, count)` (encontrar el índice del `count`-ésimo bit '1') sobre secuencias de bits. Estas son fundamentales para navegar y decodificar muchas estructuras de datos sucintas. Considerar implementaciones que logren `O(1)` tiempo para `rank` y `select` usando tablas auxiliares.
    -  Implementar la construcción completa de la estructura RMQ sucinta. Esto incluirá la construcción del Árbol Cartesiano, su codificación en una forma sucinta (ej. LOUDS o BP), y la creación de las estructuras auxiliares para `rank` y `select`.
    -  Implementar la función `query(i, j)` que opera sobre la representación sucinta para devolver el mínimo en el rango original.
    -  Medir y reportar el espacio real en bits o bytes utilizado por la estructura de datos implementada. Comparar este espacio con el objetivo teórico de $2n + o(n)$ bits y también con el espacio utilizado por una estructura RMQ menos compacta como una Sparse Table estándar.
* **Desafíos potenciales:** La manipulación a nivel de bits es inherentemente detallista y propensa a errores. La correcta implementación y la combinación sinérgica de varias ideas teóricas complejas (Árboles Cartesianos, su mapeo a secuencias de bits balanceadas o LOUDS, y las operaciones eficientes de `rank` y `select` sobre estas secuencias) es el mayor desafío. Lograr un espacio verdaderamente sucinto requiere cuidado en cada paso.
* **Ideas para el video y repositorio:**
    * Video: Explicar la importancia del espacio en el contexto de RMQ y grandes conjuntos de datos. Describir la técnica sucinta elegida, haciendo hincapié en cómo se logra la compresión (por ejemplo, cómo la estructura del Árbol Cartesiano se captura en pocos bits) y cómo funcionan las consultas sobre esta estructura comprimida, destacando el rol de `rank` y `select`. Mostrar el análisis de espacio comparativo.
    * Repo: Implementación de la estructura RMQ sucinta. Si se implementan `rank`/`select` manualmente, incluir pruebas exhaustivas para estas primitivas. Incluir un análisis detallado del espacio utilizado, desglosando si es posible el consumo de bits para cada componente de la estructura sucinta (ej. bitvector del árbol, estructuras auxiliares para rank/select).

**Proyecto 11: Hashing por tabulación k-independiente y su aplicación en filtros de Bloom o Cuckoo hashing**

* **Tema(s) principal(es):** Hashing por tabulación, enfocándose en lograr un grado de k-independencia teóricamente deseable (idealmente k ≥ 3) y aplicando esta función hash robusta en la implementación de una estructura de datos probabilística avanzada como Filtros de Bloom o Cuckoo Hashing.
* **Descripción:** El hashing por tabulación es una técnica de hashing simple en su concepción pero sorprendentemente poderosa en la práctica, capaz de ofrecer garantías teóricas fuertes sobre la distribución de los hashes. Funciona dividiendo la clave de entrada en varios trozos (bytes o palabras cortas). Cada trozo se utiliza para indexar una tabla separada de valores precalculados (generalmente aleatorios). Los valores obtenidos de cada tabla se combinan (usualmente mediante XOR) para producir el hash final. La variante más simple (tabulación simple) es 3-independiente. Variantes más elaboradas pueden alcanzar mayor k-independencia.
* **Objetivos de implementación:**
    -  Implementar una función de hash utilizando hashing por tabulación que garantice al menos 3-independencia (esto se logra con la tabulación simple estándar). Opcionalmente, investigar e implementar variantes más avanzadas como "twisted tabulation" (que puede lograr 4-independencia) o composición de funciones de tabulación para intentar alcanzar una k-independencia mayor. La elección de la clave (ej. enteros de 32 o 64 bits) y el tamaño de los trozos (ej. 8 bits por trozo) debe ser justificada.
    -  Inicializar las tablas de consulta con valores aleatorios de buena calidad, asegurando que sean lo suficientemente grandes para el dominio de cada trozo de la clave.
    -  Utilizar esta función de hash por tabulación (o múltiples instancias de ella con diferentes tablas/seeds) en la implementación de una de las siguientes estructuras de datos probabilísticas:
        * **Filtro de Bloom:** Implementar un filtro de Bloom estándar. Se necesitarán `k` funciones hash; estas pueden ser `k` instancias de la función de tabulación con diferentes conjuntos de tablas aleatorias.
        * **Cuckoo Hashing:** Implementar una tabla hash utilizando la técnica de Cuckoo Hashing, que típicamente requiere dos o más funciones hash. Cada una de estas funciones hash debe ser una instancia de la función de tabulación implementada.
    -  Evaluar la calidad de la función de hash por tabulación mediante tests estadísticos de uniformidad y colisiones (ej. Chi-cuadrado, o midiendo la longitud promedio de las cadenas en una tabla hash con encadenamiento). Para la estructura de datos avanzada elegida (Filtro de Bloom o Cuckoo Hashing), analizar su rendimiento específico:
        * Para Filtros de Bloom: Medir la tasa de falsos positivos empírica y compararla con la predicción teórica.
        * Para Cuckoo Hashing: Analizar la carga máxima alcanzable antes de fallos de inserción frecuentes, y medir el rendimiento promedio de inserción y búsqueda.
        Comparar estos resultados con lo que se esperaría teóricamente dado el uso de funciones hash k-independientes.
* **Desafíos potenciales:** Comprender las propiedades teóricas de la k-independencia y cómo el hashing por tabulación las logra. Implementar correctamente y analizar de forma significativa una estructura de datos como Cuckoo Hashing (con su lógica de reubicación de elementos) o un Filtro de Bloom eficiente (con la correcta gestión de las múltiples funciones hash y el bit array). La generación y gestión de las tablas de valores aleatorios también requiere atención.
* **Ideas para el video y repositorio:**
    * Video: Explicar el concepto de hashing por tabulación y la noción de k-independencia, resaltando por qué es deseable. Mostrar cómo se inicializan las tablas y cómo se calcula un valor hash. Demostrar la estructura de datos elegida (Filtro de Bloom o Cuckoo Hashing) en acción, insertando elementos y realizando consultas. Presentar los resultados clave de su evaluación (por ejemplo, la tasa de falsos positivos observada vs. la teórica para el Filtro de Bloom, o la eficiencia de las inserciones en Cuckoo Hashing).
    * Repo: Implementación de la(s) función(es) de hashing por tabulación y la estructura de datos avanzada que la utiliza. Incluir scripts para realizar tests de calidad de la función hash (uniformidad, colisiones) y para evaluar el rendimiento y las propiedades de la estructura de datos avanzada (Filtro de Bloom o Cuckoo Hashing).

**Proyecto 12: Árboles multisplay: Explorando una variante específica y su análisis**

* **Tema(s) principal(es):** Árboles multisplay, con una investigación y posterior implementación de una variante específica bien definida de la literatura académica, junto con un análisis de su rendimiento y aplicabilidad.
* **Descripción:** Los árboles Multisplay son una generalización o variante de los conocidos árboles splay (autoajustables). Mientras un árbol splay estándar realiza una operación de "splay" para llevar un único nodo accedido a la raíz del árbol (mejorando así la eficiencia de accesos futuros a ese nodo o nodos cercanos), un árbol multisplay busca extender esta idea. Podría implicar realizar la operación de splay sobre múltiples nodos simultáneamente, o de una manera que optimice secuencias de accesos a un conjunto específico de nodos, o incluso para facilitar operaciones concurrentes.
* **Objetivos de implementación:**
    -  Implementar un árbol splay estándar de manera robusta y correcta, ya que servirá de base. Esto incluye las operaciones de `insert`, `delete`, `search` (todas las cuales realizan un splay), y la propia operación `splay(nodo)`.
    -  Investigar a fondo la literatura académica para seleccionar una variante o aplicación concreta y bien documentada de los árboles multisplay. Algunas ideas podrían ser:
        * Splaying un rango de claves (llevar los nodos correspondientes a un cierto nivel o estructura favorable).
        * Una operación que intente llevar un conjunto de k nodos clave a posiciones prominentes en el árbol, quizás optimizando la altura de su ancestro común más bajo.
        * Técnicas para realizar operaciones de splay de forma concurrente o en paralelo (esto sería muy avanzado).
        La elección debe estar bien justificada.
    -  Implementar la funcionalidad "multi-splay" específica de la variante elegida, construyéndola sobre la base del árbol splay estándar.
    -  Analizar teóricamente (si la literatura original provee cotas o análisis) y/o empíricamente el rendimiento de esta operación multisplay. Esto debe incluir una comparación con la realización de la misma tarea (si es posible) mediante una secuencia de operaciones splay individuales en el árbol splay estándar. El objetivo es identificar escenarios o cargas de trabajo donde la operación multisplay ofrezca ventajas claras.
* **Desafíos potenciales:** La teoría de los árboles multisplay es menos estándar y más fragmentada que la de los splay trees. Encontrar una variante que esté bien especificada, sea manejable de implementar en el tiempo disponible, y ofrezca beneficios interesantes es el primer desafío crucial. La coreografía de las rotaciones y las interacciones para múltiples nodos o para operaciones concurrentes puede ser significativamente más intrincada que en un splay simple.
* **Ideas para el video y repositorio:**
    * Video: Breve repaso de los árboles splay y su funcionamiento. Explicar en detalle la variante específica de multisplay elegida, su motivación teórica y los algoritmos involucrados. Demostrar su operación con ejemplos claros y, si es posible, visualizaciones. Presentar el análisis comparativo de rendimiento, destacando cuándo y por qué el multisplay es superior (o no) a los splays secuenciales.
    * Repo: Implementación del árbol splay estándar y la extensión multisplay elegida. Incluir una justificación clara y detallada de la variante seleccionada, con referencias a la literatura pertinente. Desarrollar escenarios de prueba y benchmarks que estén específicamente diseñados para mostrar la utilidad (o las limitaciones) de la operación multisplay implementada.

**Proyecto 13: Estructura unificada de Iacono: Implementación y verificación de propiedades**

* **Tema(s) principal(es):** Estructura unificada de John Iacono, con un enfoque en la correcta implementación de su compleja jerarquía interna y la verificación empírica de sus propiedades de localidad de referencia y eficiencia en ciertos patrones de acceso.
* **Descripción:** La estructura unificada de John Iacono es una estructura de datos teóricamente sofisticada que busca lograr un rendimiento óptimo o cercano al óptimo para una amplia gama de patrones de acceso, especialmente aquellos que exhiben localidad de referencia (temporal o espacial). Intenta unificar varias propiedades deseables de otras estructuras de datos autoajustables y ha sido discutida en el contexto de la (aún no resuelta) conjetura del acceso dinámico óptimo para árboles binarios de búsqueda. Su diseño suele implicar una jerarquía de listas o árboles más simples, interconectados de manera ingeniosa.
* **Objetivos de implementación:**
    -  Investigar en detalle la estructura propuesta por John Iacono. Es importante centrarse en una versión o simplificación que sea manejable dentro del alcance del proyecto, ya que existen generalizaciones y variantes complejas. Se debe comprender la jerarquía de estructuras que la componen (por ejemplo, listas de elementos, árboles que agrupan estas listas) y cómo interactúan durante las operaciones.
    -  Implementar los componentes clave de esta estructura jerárquica y la lógica de su interconexión.
    -  Implementar las operaciones fundamentales: `access` (o `search`) e `insert`. La operación de acceso es crucial, ya que debe desencadenar las reorganizaciones específicas de la estructura de Iacono, moviendo elementos entre niveles de la jerarquía para que futuros accesos a elementos "cercanos" (en algún sentido definido por la estructura) sean más rápidos.
    -  Implementar la operación `delete`. Esta puede ser particularmente compleja en estructuras jerárquicas como la de Iacono. Se debe considerar una estrategia viable, como el marcado perezoso de elementos eliminados con reconstrucciones periódicas parciales o globales, o una propagación más activa de los mensajes de borrado.
    -  Diseñar y ejecutar un conjunto de experimentos con patrones de acceso cuidadosamente seleccionados. Estos patrones deben estar diseñados para verificar empíricamente las propiedades teóricas que la estructura de Iacono busca satisfacer, como la propiedad de "working set" (acceder eficientemente a un conjunto de elementos recientemente activos) o la propiedad "dynamic finger" (acceder eficientemente a elementos cercanos a uno accedido previamente). Comparar su rendimiento en estos patrones de acceso con el de Splay Trees u otras estructuras de datos autoajustables relevantes.
* **Desafíos potenciales:** La complejidad conceptual de la estructura jerárquica de Iacono y sus intrincadas reglas de reorganización. La implementación requiere un manejo muy cuidadoso de punteros o referencias y de las invariantes de cada nivel de la jerarquía. Demostrar de manera convincente sus propiedades teóricas mediante experimentos puede ser difícil y requiere un diseño experimental meticuloso y un análisis estadístico apropiado de los resultados.
* **Ideas para el video y repositorio:**
    * Video: Explicar la motivación de la estructura de Iacono (localidad de referencia, búsqueda de optimalidad dinámica). Describir la organización interna y las reglas de reorganización de la estructura de la forma más clara y visual posible. Presentar los resultados de los experimentos con los diferentes patrones de acceso, idealmente utilizando gráficos comparativos que muestren el rendimiento de la estructura de Iacono frente a otras estructuras como Splay Trees en dichos escenarios.
    * Repo: Implementación de la estructura de Iacono. Incluir un conjunto de pruebas unitarias y pruebas basadas en propiedades (si es posible). Proveer los scripts para generar los patrones de acceso utilizados en los experimentos y herramientas para medir y visualizar el costo de acceso o el estado de la estructura bajo diferentes cargas. Discutir los resultados empíricos en relación con las propiedades teóricas esperadas.

**Proyecto 14: Dinamismo en grafos con Top trees (Versión simplificada pero funcional para árboles)**

* **Tema(s) principal(es):** Top trees (o árboles de topología), con una implementación simplificada pero funcional aplicada al mantenimiento de propiedades de caminos o subárboles en árboles dinámicos. Las operaciones dinámicas podrían incluir cambios en el peso de las aristas, y la adición o eliminación de nodos hoja.
* **Descripción:** Los Top trees son una estructura de datos extremadamente poderosa y general para mantener información sobre grafos dinámicos, permitiendo actualizaciones estructurales (como añadir o quitar aristas/nodos) y consultas sobre diversas propiedades del grafo (como conectividad, componentes biconectados, caminos mínimos en un árbol, etc.). Son notoriamente complejos de entender e implementar en su totalidad.
    **Simplificación para el proyecto:** En lugar de un Top Tree genérico para grafos generales, el proyecto se enfocará en su aplicación para mantener información agregada sobre caminos o subárboles en un **árbol dinámico**. Las operaciones dinámicas permitidas podrían ser: cambiar el peso de una arista, añadir una nueva hoja conectada a un nodo existente, o eliminar una hoja existente. Se podría considerar incluso la operación de `Evert` (rerooting del árbol) si resulta factible.
* **Objetivos de implementación:**
    -  Comprender los conceptos básicos de la descomposición de un árbol en "clusters" que utilizan los Top Trees. Esto incluye entender las fronteras de los clusters, los diferentes tipos de clusters (por ejemplo, path-cluster, star-cluster, o una representación más genérica basada en dos nodos frontera y datos agregados que resumen el subárbol representado por el cluster). Elegir una representación de cluster específica y coherente.
    -  Implementar una representación de Top Tree para un árbol dinámico. Esto debe incluir las operaciones fundamentales que manipulan la jerarquía de clusters, como:
        * `Link` (para añadir una hoja o conectar dos componentes si se simplifica a eso).
        * `Cut` (para quitar una hoja o desconectar un componente).
        * `Expose(v)` o `ExposePath(u,v)` (o una primitiva similar) que reconfigure el Top Tree para que la información del camino desde la raíz (o desde `u`) hasta `v` sea fácilmente accesible o modificable en la raíz del Top Tree o en un cluster específico.
    -  Implementar operaciones para actualizar información en las aristas (por ejemplo, cambiar el peso de una arista) y para consultar al menos dos propiedades agregadas diferentes sobre caminos entre dos nodos dados (por ejemplo, la suma de los pesos de las aristas en el camino y la arista de peso máximo/mínimo en ese camino). Estas consultas y actualizaciones deben utilizar la operación `Expose` (o similar) para traer la información relevante a un lugar accesible.
    -  **(Opcional muy avanzado y desafiante)** Considerar la implementación de una operación de `Evert(v)`, que cambia la raíz del árbol representado a ser el nodo `v`, y actualiza el Top Tree consistentemente.
* **Desafíos potenciales:** Incluso en su versión simplificada para árboles, los conceptos detrás de los Top Trees son avanzados. La gestión de la jerarquía de clusters, la implementación correcta de las operaciones de mantenimiento del Top Tree como `Compress` y `Rake` (o sus equivalentes lógicos en la variante de Top Tree elegida como los "self-adjusting top trees" o los basados en Link-Cut Trees), y la actualización correcta y eficiente de los datos agregados durante estas operaciones de reestructuración son el núcleo de la dificultad y son muy propensos a errores.
* **Ideas para el video y repositorio:**
    * Video: Explicar la idea general de los Top trees y el problema simplificado que se aborda (mantenimiento de propiedades en árboles dinámicos). Describir la representación de clusters utilizada y cómo se combinan o modifican durante las operaciones clave (`Link`, `Cut`, `Expose`). Visualizar una operación `Expose` o una actualización de un peso de arista y cómo esta se propaga lógicamente a través de la estructura del Top Tree, afectando los datos agregados.
    * Repo: Implementación de la estructura Top tree para el problema simplificado. Incluir una documentación exhaustiva de la representación de clusters elegida, las invariantes mantenidas, y la lógica detrás de las operaciones de mantenimiento del Top Tree (como `Join`, `Split`, `PushUp`, `PushDown` de información agregada, etc., según la perspectiva de implementación). Proveer pruebas unitarias rigurosas para cada operación y para las consultas de propiedades de caminos.

**Proyecto 15: Construyendo BSTs con voracidad geométrica y análisis de su optimalidad (o sub-optimalidad)**

* **Tema(s) principal(es):** Árboles binarios de búsqueda (BSTs) construidos mediante algoritmos de voracidad geométrica, con un enfoque en la implementación de un algoritmo específico y un análisis riguroso de qué tan cerca está el árbol resultante de un BST óptimo para ciertas métricas de costo (por ejemplo, el tiempo de búsqueda promedio).
* **Descripción:** Esta idea se refiere a algoritmos que construyen árboles binarios de búsqueda para un conjunto de claves (a menudo interpretadas como puntos en 1D o 2D) utilizando algún criterio geométrico o de distribución de datos de forma codiciosa (greedy) en cada paso de la construcción. El objetivo puede ser generar un BST con ciertas propiedades deseables (como buen balance esperado, o bajo costo de acceso ponderado si se conocen las frecuencias de acceso) o investigar las cotas de rendimiento relacionadas con tales construcciones, comparándolas con la optimalidad.
* **Objetivos de implementación:**
    -  Investigar y seleccionar un algoritmo específico de construcción de BST que utilice un criterio "geométrico" o "distributivo" de forma codiciosa. Algunos ejemplos:
        * Para claves 1D: Elegir recursivamente la mediana de los elementos restantes como raíz del subárbol actual.
        * Para claves 1D con frecuencias de acceso conocidas ($p_i$ para clave $k_i$, $q_j$ para rangos entre claves): Un algoritmo greedy que intente minimizar el costo esperado.
        * Para puntos 2D (proyectados a 1D o usando un orden espacial): Podría ser una variante de construcción de k-d tree o quadtree, pero adaptada para que el resultado siga siendo un BST sobre una dimensión principal, donde la elección del elemento raíz en cada paso se basa en alguna heurística de "buena partición" del espacio geométrico.
        La elección del algoritmo greedy debe ser clara y justificada.
    -  Implementar este algoritmo para construir un BST a partir de un conjunto de claves (y frecuencias, si aplica).
    -  Analizar las propiedades del árbol resultante de forma exhaustiva:
        * Altura (peor caso y promedio).
        * Costo de búsqueda promedio (Average Search Time - AST), calculado como $\sum p_i (\text{depth}(k_i)+1)$ si las claves tienen pesos/frecuencias $p_i$, o simplemente el promedio de profundidades si todas las claves son igualmente probables.
        * Comparar el AST (o la métrica de costo elegida) del árbol construido por el algoritmo greedy con el AST de un BST óptimo. Para conjuntos de datos pequeños donde el BST óptimo sea calculable (por ejemplo, usando el algoritmo de Knuth-Yao de programación dinámica, que es `O(n^2)` o `O(n^3)` dependiendo de la versión), realizar esta comparación directa. Para conjuntos más grandes, comparar con cotas inferiores teóricas conocidas para el costo de BSTs, si existen para el problema.
    -  Investigar si el algoritmo greedy implementado produce árboles que son competitivos en algún sentido formal (por ejemplo, si se puede argumentar o encontrar en la literatura que su costo es a lo sumo un factor constante del costo del árbol óptimo para ciertas distribuciones de entrada o tipos de claves).
* **Desafíos potenciales:** Definir y justificar claramente el criterio "geométrico" o "distributivo" codicioso si no está bien especificado en la literatura. El análisis comparativo con BSTs óptimos es el mayor desafío; la implementación del algoritmo de Knuth-Yao para construir BSTs óptimos (si se usa para comparación directa) es un subproyecto significativo en sí mismo. Interpretar los resultados y argumentar sobre la "calidad" o "cercanía a la optimalidad" del árbol greedy requiere una comprensión sólida de las métricas de costo de los BSTs.
* **Ideas para el video y repositorio:**
    * Video: Explicar el algoritmo codicioso de construcción elegido y el criterio de voracidad. Mostrar paso a paso cómo se construye el árbol para un conjunto de ejemplo. Detallar la métrica de optimalidad que se está considerando (ej. AST, costo ponderado). Presentar los resultados del análisis comparativo, mostrando qué tan bien (o mal) se desempeña el árbol greedy en comparación con el óptimo o las cotas teóricas.
    * Repo: Implementación del algoritmo de construcción greedy. Scripts para generar diferentes tipos de datos de entrada (y frecuencias, si aplica). Opcionalmente, implementación del algoritmo de Knuth-Yao para BSTs óptimos (o un enlace a una implementación de referencia utilizada). Herramientas y scripts para realizar el análisis comparativo de costos y para visualizar los árboles resultantes. Incluir una discusión profunda de los resultados experimentales y de la "calidad" del algoritmo greedy en función de los hallazgos.
