{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994a54b6-1418-4a40-aa4b-ee91d64598e0",
   "metadata": {},
   "source": [
    "### K-d tree y K-NN\n",
    "\n",
    "Esta lista de ejercicios se basa en la implementación de k-dtree  dada [aquí](https://github.com/kapumota/CC0E5/blob/main/kd-tree.py)\n",
    "\n",
    "\n",
    "\n",
    "#### Ejercicio 1: Implementando k-vecinos más cercanos (k-NN)\n",
    "\n",
    "**Contexto:**\n",
    "El código proporcionado tiene un método `nearestNeighbour` que encuentra el único punto más cercano. El algoritmo k-NN requiere encontrar los *k* puntos más cercanos. Esto usualmente implica mantener una lista de los *k* mejores candidatos encontrados hasta ahora, ordenados por distancia.\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **Diseñar `kNearestNeighbours`:**\n",
    "    * En la clase `_Node`, diseña un nuevo método `kNearestNeighbours(self, target_point, k, current_k_nn=None, max_dist_in_k_nn=float('inf'))`.\n",
    "    * `current_k_nn` sería una lista de tuplas (distancia, punto), mantenida ordenada por distancia, y de tamaño máximo `k`.\n",
    "    * `max_dist_in_k_nn` sería la distancia al punto más lejano en `current_k_nn`.\n",
    "2.  **Implementar `kNearestNeighbours`:**\n",
    "    * Adapta la lógica de `nearestNeighbour`.\n",
    "    * Cuando se verifica un punto en un nodo, si su distancia a `target_point` es menor que `max_dist_in_k_nn` (o si `current_k_nn` tiene menos de `k` puntos), agrégalo a `current_k_nn`, reordena, y si `current_k_nn` ahora excede `k` ítems, elimina el más lejano. Actualiza `max_dist_in_k_nn`.\n",
    "    * La condición de poda para verificar la \"rama más lejana\" (`further_branch`) ahora usará `max_dist_in_k_nn`: si `abs(target_coord - node_coord) < max_dist_in_k_nn`.\n",
    "3.  **Exponer en la clase `Kdtree`:**\n",
    "    * Agrega un método público `kNearestNeighbours(self, point, k)` a la clase `Kdtree` que iniciBudincita el proceso y llame al método recursivo en la raíz.\n",
    "4.  **Pruebas:**\n",
    "    * Agrega casos de prueba para verificar tu implementación de `kNearestNeighbours` usando los datos de ejemplo `points_2d`. Por ejemplo, encuentra los 3 vecinos más cercanos a `Point([5, 5])`.\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* ¿Cómo afecta el mantenimiento de una lista de `k` candidatos a la complejidad de la búsqueda en comparación con 1-NN?\n",
    "* ¿Qué estructura de datos es adecuada para `current_k_nn` para gestionar eficientemente los `k` vecinos principales (por ejemplo, una cola de prioridad mínima de tamaño k, o una lista ordenada)? La sugerencia actual utiliza una lista ordenada.\n",
    "* Compara la complejidad temporal del k-NN por fuerza bruta ($O(N \\cdot D)$ para cálculos de distancia más $O(N \\log k)$ o $O(N \\cdot k)$ para la selección, donde N es el número de puntos, D es la dimensionalidad) con el k-NN del k-d tree (promedio $O(k \\log N)$, peor caso $O(kN)$).\n",
    "\n",
    "\n",
    "#### Ejercicio 2: Simulación de Kapumota y Budincita - comprendiendo la poda (Pruning) \n",
    "\n",
    "**Contexto:**\n",
    "El texto describe un escenario donde los datos se dividen entre Kapumota y Budincita. Si Kapumota encuentra 7 vecinos y su séptimo vecino está a una distancia `d_A7`, y Kapumota puede proporcionar rápidamente una cota inferior `d_B_lower` para *cualquier* punto en su conjunto de datos (o incluso si solo su primer vecino más cercano está a `d_B1`), entonces si `d_A7 < d_B_lower` (o `d_A7 < d_B1`), no es necesario realizar una búsqueda completa en el conjunto de datos de Kapumota. Los k-d trees hacen esto implícitamente al verificar la distancia al hiperrectángulo delimitador (bounding box) de una rama.\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **Análisis conceptual:**\n",
    "    * Considera el método `nearestNeighbour` en `_Node`. La línea `if abs(target_coord - node_coord) < current_nn_dist:` es crucial.\n",
    "    * Explica cómo `abs(target_coord - node_coord)` actúa como una cota inferior de la distancia a cualquier punto en la \"further_branch\". Si esta cota inferior ya es mayor que `current_nn_dist`, esa rama entera se poda.\n",
    "2.  **Simulación de escenario (código simple):**\n",
    "    * Imagina que `tree_A` y `tree_B` son dos instancias separadas de `Kdtree`.\n",
    "    * Quieres encontrar el 1-NN para un `query_point` en ambos árboles.\n",
    "    * Primero, encuentra `nn_A = tree_A.nearestNeighbour(query_point)` y sea su distancia `dist_A`.\n",
    "    * ¿Cómo podrías obtener una \"cota inferior\" para `tree_B` sin buscarlo completamente? (Pista: La clase `Cube` que representa toda la región de `tree_B` o la partición de su nodo raíz podría usarse. La distancia desde `query_point` hasta el *borde más cercano* del hiperrectángulo delimitador de `tree_B` sería una cota inferior.)\n",
    "    * Si esta cota inferior es mayor que `dist_A`, no necesitas buscar en `tree_B`.\n",
    "3.  **Bosquejo de código (opcional):**\n",
    "    * Bosqueja cómo podrías modificar la clase `Cube` proporcionada para calcular `min_distance_to_point(self, point)` (distancia más corta desde el punto a cualquier parte del cubo).\n",
    "    * ¿Cómo se relacionaría esta `min_distance_to_point` para el hiperrectángulo delimitador efectivo de la `further_branch` con `abs(target_coord - node_coord)`?\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* ¿Cómo la división recursiva de los k-d trees y los hiperplanos alineados con los ejes simplifican el cálculo de estas \"cotas inferiores\" o \"distancias a las particiones\"?\n",
    "* El texto afirma: \"Esta capacidad de ahorrar trabajo con una cota inferior sobre la distancia a un conjunto de puntos es lo que subyace a la estructura de datos del Árbol KD.\". Discute esta afirmación en el contexto del código de `nearestNeighbour`.\n",
    "\n",
    "#### Ejercicio 3: Construcción de k-d trees y búsqueda por región \n",
    "\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **Construcción manual:**\n",
    "    * Toma un pequeño subconjunto de `points_2d`, por ejemplo, `P1 = Point([2,3]), P2 = Point([5,4]), P3 = Point([4,7]), P4 = Point([8,1])`.\n",
    "    * Construye manualmente un k-d tree usando estos puntos. En cada paso:\n",
    "        * Determina la dimensión de división (profundidad % K).\n",
    "        * Encuentra el punto mediano para esa dimensión.\n",
    "        * Muestra las sub-particiones izquierda y derecha.\n",
    "    * Dibuja la estructura del árbol resultante y las particiones correspondientes en un espacio 2D.\n",
    "2.  **Trazado de búsqueda por región:**\n",
    "    * Define un `search_cube = Cube(Point([3,0]), Point([6,5]))`.\n",
    "    * Traza la ejecución de `tree_2d.pointsInRegion(search_cube)` (o usando tu árbol construido manualmente).\n",
    "    * Para cada nodo visitado:\n",
    "        * Identifica la `current_node_region` (el hiperrectángulo delimitador implícitamente definido por las divisiones hasta ahora).\n",
    "        * Verifica `target_region.intersects(current_node_region)`.\n",
    "        * Verifica `target_region.containsPoint(self._point)`.\n",
    "        * Muestra cómo el algoritmo decide recursar en los hijos izquierdo/derecho y cómo se actualiza su `current_node_region`.\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* ¿Cómo afecta la elección de la mediana al balance del árbol y al rendimiento de la búsqueda? La función `median` proporcionada ordena todos los puntos en cada paso; ¿cuáles son algoritmos de búsqueda de mediana más eficientes (como se insinúa en los comentarios del código)?\n",
    "* Explica la poda en `pointsInRegion`: ¿cuándo puede evitar buscar subárboles enteros?\n",
    "\n",
    "\n",
    "#### Ejercicio 4: Complejidad temporal y la maldición de la dimensionalidad\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **Análisis de complejidad (teórico):**\n",
    "    * **Construcción:** ¿Cuál es la complejidad temporal de construir el k-d tree con N puntos en D dimensiones usando la búsqueda de mediana proporcionada? (Pista: La función `median` ordena, $O(N \\log N)$ o $O(N)$ para esa dimensión, y esto se hace en cada nivel).\n",
    "    * **`contains` / `add` / `delete` (un solo punto):** ¿Promedio y peor caso?\n",
    "    * **`nearestNeighbour` / `kNearestNeighbours`:** ¿Promedio y peor caso?\n",
    "    * **`pointsInRegion`:** La complejidad depende del tamaño de la consulta y del tamaño de la salida. Discutir.\n",
    "2.  **Maldición de la dimensionalidad:**\n",
    "    * Investiga y explica qué significa la \"maldición de la dimensionalidad\" en el contexto de k-NN y estructuras de datos espaciales como los k-d trees.\n",
    "    * ¿Por qué los k-d trees (y estructuras similares) tienden a tener un rendimiento pobre (acercándose a la fuerza bruta) en dimensiones muy altas? (Pista: considera cuántos hiperrectángulos delimitadores de los hijos probablemente se superpondrán con la hiperesfera/hiperrectángulo de la consulta).\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* ¿Para qué rango de dimensiones son típicamente más efectivos los k-d trees?\n",
    "* ¿Cuáles son las alternativas a los k-d trees para k-NN en espacios de dimensiones muy altas (por ejemplo, hashing sensible a la localidad, algoritmos de vecinos más cercanos aproximados, o incluso ball trees en algunos casos)?\n",
    "\n",
    "\n",
    "#### Ejercicio 5: Ball trees y búsqueda A*\n",
    "\n",
    "\n",
    "**Contexto:**\n",
    "* **Ball trees:** Particionan los datos en hiperesferas anidadas en lugar de hiperrectángulos.\n",
    "* **Búsqueda A*:** Un algoritmo de búsqueda de caminos que utiliza una heurística ($h(n)$) para estimar el costo hasta el objetivo y el costo real hasta el momento ($g(n)$). Explora primero los caminos con menor $f(n) = g(n) + h(n)$.\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **Ball trees (conceptual):**\n",
    "    * Investiga brevemente \"construcción de Ball trees\" y \"búsqueda en Ball trees\".\n",
    "    * ¿En qué se diferencia la partición con hiperesferas de los hiperplanos alineados con los ejes de los k-d trees?\n",
    "    * ¿Cuándo podría ser ventajoso un Ball tree sobre un k-d tree? (Considera la distribución de los datos, tipos de consultas).\n",
    "    * ¿Cómo funcionaría la condición de poda en un Ball trees para la búsqueda del vecino más cercano? (Pista: distancia desde el punto de consulta hasta la superficie de una bola, y el radio de la bola).\n",
    "2.  **Analogía con la búsqueda A*:**\n",
    "    * En el contexto de `_Node.nearestNeighbour`:\n",
    "        * ¿Qué podría considerarse el \"estado\" o \"nodo\" en el sentido de la búsqueda A*? (Un nodo del k-d tree).\n",
    "        * ¿Qué es $g(n)$, el costo desde el inicio hasta el estado/nodo actual? (Esto es menos directo, pero considera el camino tomado en el árbol).\n",
    "        * ¿Qué actúa como $h(n)$, la estimación heurística del costo desde el estado/nodo actual hasta el objetivo (el verdadero vecino más cercano)? (Pista: La distancia desde el punto de consulta hasta el *hiperrectángulo delimitador* de la región de un nodo del k-d tree).\n",
    "        * ¿Cómo prioriza el algoritmo qué nodos del k-d tree (ramas) explorar, y cómo se relaciona esto con $f(n)$ de A*? (El algoritmo explora primero la rama \"más cercana\", y solo explora la rama \"más lejana\" si la distancia optimista a ella es menor que la mejor encontrada hasta ahora).\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* ¿Por qué la distancia al hiperrectángulo delimitador de un subárbol es una heurística *admisible* para A* (es decir, nunca sobrestima la verdadera distancia al mejor punto dentro de ese subárbol)?\n",
    "* ¿Garantiza la búsqueda en el k-d tree encontrar la solución óptima (el verdadero vecino más cercano)? ¿Por qué?\n",
    "\n",
    "\n",
    "#### Ejercicio 6: Conexiones con el aprendizaje automático y exploración adicional\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **K-NN como clasificador/regresor:**\n",
    "    * Explica cómo se puede usar k-NN para tareas de clasificación (voto mayoritario entre k vecinos) y tareas de regresión (promedio de los valores de k vecinos).\n",
    "    * ¿Cuáles son las principales ventajas de k-NN como algoritmo de aprendizaje automático? (por ejemplo, simplicidad, sin fase de entrenamiento explícita, a menudo llamado \"aprendiz perezoso\" o \"lazy learner\").\n",
    "    * ¿Cuáles son sus principales desventajas? (por ejemplo, costo computacional en el momento de la consulta, sensibilidad al escalado de características, maldición de la dimensionalidad).\n",
    "2.  **Impacto de `K` en k-NN:**\n",
    "    * ¿Cómo afecta la elección de `k` al sesgo y la varianza del modelo k-NN?\n",
    "    * ¿Qué sucede si `k` es demasiado pequeño? ¿Demasiado grande?\n",
    "3.  **Escalado de características:**\n",
    "    * ¿Por qué el escalado de características (por ejemplo, normalización o estandarización) suele ser crucial antes de aplicar k-NN (y, por lo tanto, al usar un k-d tree)? ¿Cómo afectarían las características no escaladas a los cálculos de distancia y al comportamiento de las divisiones del k-d tree?\n",
    "4.  **Mejorando el k-d tree:**\n",
    "    * El método `delete` proporcionado está marcado como \"simplificado\". Investiga y discute las complejidades de la eliminación robusta en k-d trees y cómo puede llevar a un desequilibrio del árbol. ¿Qué estrategias existen para manejar esto (por ejemplo, reconstrucción periódica, marcar nodos como eliminados)?\n",
    "    * La búsqueda de la mediana ordena todos los puntos en el subconjunto actual para una dimensión dada. ¿Cómo mejoraría la complejidad de la construcción el uso de un verdadero algoritmo de mediana de medianas en tiempo lineal?\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* ¿Cuándo elegirías un k-NN optimizado con k-d tree sobre otros algoritmos como SVM,  árboles de decisión o redes neuronales?\n",
    "* Considera aplicaciones del mundo real donde el k-NN acelerado por k-d trees podría ser particularmente útil.\n",
    "\n",
    "#### Ejercicio 7: Múltiples métricas de distancia en `Point`\n",
    "\n",
    "**Contexto:**\n",
    "La clase `Point` actual implementa la distancia Euclidiana. Sin embargo, en diferentes aplicaciones, otras métricas de distancia como la distancia de Manhattan (o \"city block\") pueden ser más apropiadas.\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **Implementar distancia de Manhattan:**\n",
    "    * Añade un nuevo método `distanceToManhattan(self, other_point)` a la clase `Point`.\n",
    "    * Esta distancia se calcula como la suma de las diferencias absolutas de sus coordenadas:\n",
    "        $d_M(p, q) = \\sum_{i=1}^{D} |p_i - q_i|$\n",
    "    * Asegúrate de que maneje las mismas validaciones que `distanceTo` (misma dimensionalidad).\n",
    "2.  **Modificar `Kdtree` para usar diferentes métricas:**\n",
    "    * Considera cómo podrías modificar la clase `Kdtree` (y `_Node`) para permitir que el usuario especifique qué métrica de distancia usar al construir el árbol o al realizar búsquedas. Esto podría implicar pasar una función de distancia como parámetro.\n",
    "    * *Nota:* La lógica de poda del k-d tree está intrínsecamente ligada a la distancia Euclidiana (o métricas $L_p$ donde las proyecciones sobre los ejes son relevantes para las cotas). Cambiar la métrica podría requerir repensar la validez de la poda actual si la nueva métrica no se comporta bien con las divisiones axiales. Para este ejercicio, centrarse en la clase `Point` es suficiente, pero la discusión sobre el impacto en el `Kdtree` es valiosa.\n",
    "3.  **Pruebas:**\n",
    "    * Crea instancias de `Point` y prueba tu nuevo método `distanceToManhattan`. Compara los resultados con `distanceTo` (Euclidiana) para los mismos puntos.\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* ¿En qué tipo de escenarios o con qué tipo de datos podría ser preferible la distancia de Manhattan sobre la Euclidiana?\n",
    "* Si se usara la distancia de Manhattan en el algoritmo k-NN, ¿cómo cambiarían las \"vecindades\"?\n",
    "* ¿Sigue siendo válida la estrategia de poda del k-d tree (que se basa en la distancia perpendicular a los hiperplanos de división) si la métrica principal de \"cercanía\" es la de Manhattan? ¿Por qué sí o por qué no?\n",
    "\n",
    "#### Ejercicio 8: Puntos en un nivel específico del árbol\n",
    "\n",
    "**Contexto:**\n",
    "A veces es útil analizar la distribución de los datos o la estructura del árbol por niveles. Por ejemplo, para entender cómo se han dividido los datos en las primeras etapas de la construcción del árbol.\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **Implementar `_points_at_depth_recursive` en `_Node`:**\n",
    "    * Añade un método auxiliar recursivo `_points_at_depth_recursive(self, target_depth, current_depth)` a la clase `_Node`.\n",
    "    * Si `current_depth == target_depth`, el método debe producir (yield) el punto del nodo actual (`self._point`).\n",
    "    * Si `current_depth < target_depth`, el método debe llamar recursivamente a sí mismo en los hijos izquierdo y derecho (si existen), incrementando `current_depth`.\n",
    "2.  **Implementar `points_at_depth` en `Kdtree`:**\n",
    "    * Añade un método público `points_at_depth(self, depth)` a la clase `Kdtree`.\n",
    "    * Este método debe manejar el caso de un árbol vacío o si la `depth` solicitada es negativa o mayor que la altura del árbol.\n",
    "    * Debe llamar al método recursivo en el nodo raíz, iniciando `current_depth` en 0.\n",
    "    * Debe devolver una lista o un iterador de los puntos encontrados.\n",
    "3.  **Pruebas:**\n",
    "    * Utiliza el `tree_2d` de ejemplo del código.\n",
    "    * Llama a `tree_2d.points_at_depth(0)`, `tree_2d.points_at_depth(1)`, `tree_2d.points_at_depth(2)`, etc., y verifica que los puntos devueltos sean los correctos según la estructura esperada del árbol.\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* ¿Cuál es la relación entre la profundidad máxima para la cual este método devuelve puntos y la altura del árbol (`tree.height`)?\n",
    "* ¿Cómo podrías modificar este método para devolver no solo los puntos, sino también los propios nodos o la región (hipercubo) que representa cada nodo en esa profundidad?\n",
    "* ¿Para qué tipo de análisis o visualización podría ser útil obtener los puntos por nivel?\n",
    "\n",
    "#### Ejercicio 9: Encontrar el vecino más lejano\n",
    "\n",
    "**Contexto:**\n",
    "Mientras que la búsqueda del vecino más cercano es común, encontrar el vecino más lejano también puede tener aplicaciones (por ejemplo, en detección de anomalías o para entender la dispersión de un conjunto de datos). Para un k-d tree, esto requiere una estrategia de poda diferente.\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  **Diseñar la lógica de búsqueda del vecino más lejano:**\n",
    "    * Se necesitará un método recursivo en `_Node`, similar a `nearestNeighbour`, pero que lleve un seguimiento del punto más lejano encontrado hasta ahora y su distancia.\n",
    "    * Llamémoslo `_farthest_neighbour_recursive(self, target_point, current_farthest_point_info, current_node_region)`.\n",
    "    * `current_farthest_point_info` podría ser una tupla `(distancia_max, punto_mas_lejano)`.\n",
    "2.  **Implementar `_farthest_neighbour_recursive` en `_Node`:**\n",
    "    * **Visitar nodo actual:** Calcula la distancia desde `target_point` al punto del nodo actual (`self._point`). Si es mayor que la `distancia_max` actual, actualiza `current_farthest_point_info`.\n",
    "    * **Estrategia de ramificación:** Decide el orden para visitar los hijos. Podrías, por ejemplo, priorizar la rama cuyo hiperrectángulo delimitador tenga un vértice que esté más lejos del `target_point`.\n",
    "    * **Poda (Pruning):** Esta es la parte clave. Para podar una rama (por ejemplo, `other_branch_node` con su región `other_branch_region`):\n",
    "        * Calcula la distancia desde `target_point` al **vértice más lejano** del `other_branch_region`. Esta es la máxima distancia posible que un punto dentro de esa región podría tener al `target_point`.\n",
    "        * Si esta \"máxima distancia posible en la rama\" es *menor* que la `distancia_max` actual (la distancia al vecino más lejano encontrado *hasta ahora*), entonces no hay necesidad de explorar esa rama.\n",
    "        * *Nota:* La clase `Cube` necesitará un método para calcular la distancia desde un punto externo a su vértice más lejano, o alternativamente, la máxima distancia al cuadrado para evitar `sqrt`.\n",
    "3.  **Implementar `farthestNeighbour` en `Kdtree`:**\n",
    "    * Añade un método público `farthestNeighbour(self, target_point)` a `Kdtree`.\n",
    "    * Inicializa `current_farthest_point_info` con el primer punto que encuentres y su distancia, o maneja el caso de árbol vacío.\n",
    "    * Llama al método recursivo en la raíz.\n",
    "4.  **Pruebas:**\n",
    "    * Con `tree_2d`, encuentra el vecino más lejano a `Point([0,0])` o a un punto en el centro del conjunto de datos. Verifica manualmente si el resultado es correcto.\n",
    "\n",
    "**Puntos de discusión:**\n",
    "* Compara la complejidad y la efectividad de la poda para la búsqueda del vecino más lejano versus la del vecino más cercano en un k-d tree.\n",
    "* ¿Cómo se implementaría el cálculo de la \"distancia al vértice más lejano de un hipercubo (`Cube`)\"?\n",
    "* ¿Existen configuraciones de puntos o puntos de consulta donde la poda para el vecino más lejano sea poco efectiva?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6106b1-4e2f-409d-9e16-5e534d27c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f0b92a-a363-49be-9f70-5333dba2395c",
   "metadata": {},
   "source": [
    "### Proyecto: Exploración avanzada de búsqueda de vecinos cercanos con K-d trees y alternativas\n",
    "\n",
    "#### Módulo 1: Optimización y análisis del K-d tree \n",
    "\n",
    "#### Sesión 1.1: Del vecino más cercano a k-vecinos (k-NN) y primeras mediciones\n",
    "\n",
    "* **Introducción:**\n",
    "    * Presentación del problema: \"Un cliente con un K-d tree funcional ahora requiere encontrar los *k* vecinos más cercanos, no solo uno.\"\n",
    "    * Discusión inicial sobre la necesidad de eficiencia.\n",
    "* **Actividad principal (codificación):**\n",
    "    1.  **Implementación de `kNearestNeighbours`:**\n",
    "        * Modificar el método `nearestNeighbour` existente para convertirlo en `kNearestNeighbours`.\n",
    "        * **Requisito de eficiencia:** Utilizar un **max-heap de tamaño fijo `k`** para mantener el ranking provisional de los `k` mejores candidatos encontrados hasta ahora, evitando ordenaciones completas en cada paso.\n",
    "    2.  **Instrumentación básica del código:**\n",
    "        * Añadir contadores para:\n",
    "            * Número de nodos visitados.\n",
    "            * Número de ramas podadas (debido a la distancia al hiperplano de corte).\n",
    "        * Utilizar `time.perf_counter()` para medir el tiempo de ejecución de las búsquedas.\n",
    "* **Experimentación y análisis inicial:**\n",
    "    1.  Generar conjuntos de datos (nubes de puntos aleatorios) con:\n",
    "        * Dimensionalidades: 2, 5, y 10.\n",
    "        * Tamaños (N): Creciente desde 1,000 hasta 100,000 puntos.\n",
    "    2.  Comparar el rendimiento de la búsqueda k-NN con el K-d tree vs. un algoritmo de fuerza bruta.\n",
    "    3.  Visualizar en una gráfica el tiempo de ejecución (K-d tree vs. Fuerza Bruta) en función de la dimensionalidad. Observar el \"cruce\" de las curvas.\n",
    "* **Discusión en clase:**\n",
    "    * La \"maldición de la dimensionalidad\": ¿Qué significa y cómo se manifiesta en los resultados observados?\n",
    "* **Tarea 1:**\n",
    "    1.  Finalizar la instrumentación del código para `kNearestNeighbours`.\n",
    "    2.  Generar y entregar dos gráficos (archivos PNG):\n",
    "        * Gráfico 1: Tiempo medio de búsqueda vs. tamaño del dataset (N) para una D fija.\n",
    "        * Gráfico 2: Nodos visitados vs. tamaño del dataset (N) para una D fija.\n",
    "        (Ambos gráficos deben mostrar K-d tree y fuerza bruta).\n",
    "\n",
    "\n",
    "#### Sesión 1.2: Profundizando en la teoría y optimizando con cajas delimitadoras (bounding boxes)\n",
    "\n",
    "* **Revisión y discusión:**\n",
    "    * Analizar las gráficas de la tarea. Pregunta guía: \"¿Por qué las curvas de rendimiento (especialmente nodos visitados) tienen esa forma particular?\"\n",
    "* **Componente teórico:**\n",
    "    * Cada estudiante investiga y prepara una breve explicación (máx. 3 ideas clave/diapositivas) sobre:\n",
    "        * Complejidad de construcción del K-d tree: $O(N \\log N)$ en promedio.\n",
    "        * Complejidad de búsqueda promedio: $O(\\log N)$ para 1-NN (o $O(k + \\log N)$ / $O(k \\log N)$ para k-NN).\n",
    "        * Peor caso de rendimiento: ¿Cuándo y por qué el rendimiento se degrada (ej. puntos colineales o estructura de datos degenerada)?\n",
    "* **Actividad principal (codificación y optimización):**\n",
    "    1.  **Integración de cajas delimitadoras (bounding boxes):**\n",
    "        * Modificar la estructura de cada nodo del K-d tree para almacenar explícitamente sus `min_bounds` y `max_bounds` (el hiperrectángulo mínimo que encierra todos los puntos en el subárbol rooted en ese nodo).\n",
    "        * Estos bounds se calculan durante la construcción del árbol.\n",
    "    2.  **Optimización de la poda:**\n",
    "        * Modificar la lógica de búsqueda (`kNearestNeighbours`) para utilizar estas cajas delimitadoras.\n",
    "        * Calcular la distancia desde el punto de consulta hasta la caja delimitadora de una rama.\n",
    "        * Si esta distancia (cota inferior) ya es mayor que la distancia al k-ésimo vecino más lejano encontrado hasta ahora, podar la rama completa sin necesidad de visitarla.\n",
    "* **Experimentación y tarea:**\n",
    "    1.  Ejecutar un script que repita los experimentos de la Sesión 1.1 (tiempo y nodos visitados), pero ahora comparando:\n",
    "        * K-d tree con poda original (solo hiperplano).\n",
    "        * K-d tree con poda mejorada usando bounding boxes.\n",
    "    2.  Generar y entregar dos nuevos gráficos (png) que muestren el impacto de la poda con bounding boxes, especialmente en el número de nodos visitados.\n",
    "\n",
    "#### Módulo 2: Escenarios distribuidos y estructuras de datos alternativas\n",
    "\n",
    "#### Sesión 2.1: K-d trees en un entorno distribuido\n",
    "\n",
    "* **Introducción del escenario:**\n",
    "    * \"Kapumota y Budincita gestionan cada una la mitad de un mega-dataset en servidores separados. Un cliente necesita realizar búsquedas k-NN sobre el dataset completo.\"\n",
    "    * Objetivo: Minimizar el cómputo caro y las transferencias de datos.\n",
    "* **Actividad principal (codificación y simulación):**\n",
    "    1.  **Simulación de sistema distribuido:**\n",
    "        * Utilizar el módulo `multiprocessing` de Python.\n",
    "        * Crear dos procesos \"servidor\" (Kapumota, Budincita), cada uno con una instancia de `Kdtree` sobre una porción del dataset.\n",
    "        * Crear un proceso \"cliente\".\n",
    "        * Utilizar `multiprocessing.Queue` para la comunicación (mensajes) entre cliente y servidores.\n",
    "    2.  **Implementación del protocolo de consulta eficiente:**\n",
    "        * **Paso 1 (Cliente):** Solicitar a *ambos* servidores una \"cota inferior\" de distancia para un punto de consulta dado. Esta cota puede ser la distancia al vecino más cercano en su respectivo dataset, o una estimación más rápida (ej. distancia a la bounding box de la raíz de su árbol).\n",
    "        * **Paso 2 (Cliente):** Elegir el servidor \"más prometedor\" (con la menor cota inferior). Solicitarle sus `k` vecinos más cercanos.\n",
    "        * **Paso 3 (Cliente):** Sea `d_k` la distancia al k-ésimo vecino retornado por el primer servidor. Si `d_k` es menor que la cota inferior del *segundo* servidor, la búsqueda termina. ¡Se ahorra la consulta completa al segundo servidor!\n",
    "        * Si no, se realiza la consulta completa al segundo servidor y se combinan los resultados.\n",
    "* **Experimentación y análisis:**\n",
    "    * Medir:\n",
    "        * Número promedio de llamadas de red \"caras\" (búsqueda k-NN completa) ahorradas.\n",
    "        * Speed-up conseguido en comparación con una consulta secuencial a ambos servidores o una consulta ingenua que siempre pide los k vecinos a ambos.\n",
    "* **Tarea 2:**\n",
    "    1.  Generalizar la simulación para `M` servidores.\n",
    "    2.  Implementar y comparar dos estrategias para la obtención de cotas inferiores de los `M` servidores:\n",
    "        * **Secuencial:** Consultar las cotas una por una.\n",
    "        * **Paralela:** Lanzar todas las solicitudes de cotas en paralelo (usando `multiprocessing` o `threading` para las esperas).\n",
    "    3.  Justificar la elección de la mejor estrategia y presentar resultados de speed-up para `M=2, 4, 8`.\n",
    "\n",
    "\n",
    "#### Sesión 2.2: Introducción a los Ball-trees \n",
    "\n",
    "* **Introducción a los Ball-trees:**\n",
    "    * Presentar el Ball-tree como una alternativa al K-d tree, que particiona el espacio usando hiperesferas en lugar de hiperplanos.\n",
    "    * Discutir las diferencias conceptuales en la construcción y la búsqueda.\n",
    "* **Actividad principal (codificación y comparación):**\n",
    "    1.  **Implementación de un Ball-tree (simplificado):**\n",
    "        * **Estructura del nodo:** Cada nodo representa una hiperesfera (centro y radio) que encapsula todos los puntos en su subárbol.\n",
    "        * **Construción (simplificada):** En cada nodo, elegir un punto pivote, y asignar puntos a dos nuevas sub-esferas hijas basándose en su distancia al pivote o alguna otra heurística de partición.\n",
    "        * **Cálculo de cota inferior para poda:** La distancia mínima desde un punto de consulta `q` a cualquier punto dentro de una bola (centro `c`, radio `r`) es $| \\text{distancia}(q,c) - r |$. Esto se usa para la poda de manera análoga a los K-d trees.\n",
    "    2.  **Reutilización de benchmarks:**\n",
    "        * Reutilizar el framework de benchmarking existente.\n",
    "        * Comparar el rendimiento (tiempo de búsqueda, nodos visitados) del Ball-tree implementado con el K-d tree (versión con bounding boxes).\n",
    "        * Prestar especial atención a dimensiones más altas (ej. D = 5, 10, 15, 20).\n",
    "* **Análisis de resultados:**\n",
    "    * Generar gráficos comparativos K-d tree vs. Ball-tree en función de la dimensionalidad.\n",
    "    * Observar si el Ball-tree muestra mejor resiliencia a la maldición de la dimensionalidad en el rango probado.\n",
    "* **Debate final:**\n",
    "    * \"¿Cuándo elegiríamos un K-d tree, un Ball-tree, o un VP-tree?\"\n",
    "    * \"¿En qué escenarios sería apropiado abandonar las estructuras de búsqueda exacta y optar por métodos aproximados como Locality-Sensitive Hashing (LSH)?\"\n",
    "* **Ejercicio:**\n",
    "    * Investigar e implementar una versión básica de LSH para k-NN aproximado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe21f1f-76f4-4223-b1fb-fcbe76d0db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fcf36a-b3ff-4be9-b96d-691e702327fb",
   "metadata": {},
   "source": [
    "### Trabajo final de consolidación \n",
    "\n",
    "**Objetivo:** Integrar los aprendizajes, explorar extensiones y reflexionar sobre el proyecto.\n",
    "\n",
    "**Tareas (a elegir o combinar, los equipos deben completar al menos dos de las tres primeras):**\n",
    "\n",
    "1.  **Búsqueda aproximada ($\\epsilon$-NN) en K-d tree:**\n",
    "    * Modificar la condición de poda en el K-d tree: en lugar de `podar si cota_inferior_rama > distancia_k_actual`, usar `podar si cota_inferior_rama > (1 +` $\\epsilon$ ` * distancia_k_actual`.\n",
    "    * Experimentar con valores de $\\epsilon$ (ej. 0.1, 0.3).\n",
    "    * Medir el trade-off entre speed-up y precisión (ej. porcentaje de veces que se encuentra el mismo conjunto de k-vecinos que la búsqueda exacta).\n",
    "2.  **Dinámica de rebalanceo del K-d tree:**\n",
    "    * Implementar operaciones de `add` y `delete` (la `delete` puede ser simplificada, marcando nodos como borrados).\n",
    "    * Realizar un experimento: insertar y luego borrar un gran número de puntos (ej. 1 millón de inserciones aleatorias, seguidas de 500,000 eliminaciones aleatorias).\n",
    "    * Medir la altura del árbol antes y después, y el rendimiento de búsqueda.\n",
    "    * Implementar una heurística de rebalanceo simple: si la altura del árbol supera `C * log N` (ej. `C = 1.5` o `2`), reconstruir el árbol completo. Mostrar el impacto de esta heurística.\n",
    "3.  **Mejoras adicionales al Ball-tree o K-d tree:**\n",
    "    * Investigar e implementar una estrategia de selección de pivote o eje de división más sofisticada para el K-d tree o Ball-tree.\n",
    "    * Implementar una construcción de Ball-tree más robusta.\n",
    "4.  **README  (Obligatorio):**\n",
    "    * Un documento conciso (formato Markdown) que incluya:\n",
    "        * Breve resumen de las implementaciones y experimentos realizados.\n",
    "        * Principales hallazgos y sorpresas.\n",
    "        * Discusión sobre qué heurísticas o estructuras funcionaron mejor y bajo qué condiciones.\n",
    "        * Argumentación sobre cuándo optarían por K-d tree, Ball-tree, o LSH en un problema real.\n",
    "        * Gráficos clave generados durante el proyecto.\n",
    "\n",
    "**Entrega final:**\n",
    "* Repositorio de código con todas las implementaciones.\n",
    "* El README reflexivo.\n",
    "* Script (ej. `Makefile` o `run_all_reports.sh`) que permita al docente generar automáticamente los principales gráficos y métricas del proyecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d4736-2418-4957-b658-a3a150b4b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
