{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e0219e-3d85-4a45-9a5f-7839b5e79630",
   "metadata": {},
   "source": [
    "### **Variantes de los Bloom filters**\n",
    "\n",
    "#### **Bloomier Filter**  \n",
    "\n",
    "\n",
    "Los filtros de Bloom son estructuras probabilísticas diseñadas para responder eficientemente a consultas de pertenencia (¿esta clave es posiblemente miembro del conjunto?). Sin embargo, adolecen de una gran limitación: no permiten **asociar valores** a las claves. El **Bloomier filter** surge como extensión que habilita un mapeo estático de *n* claves a sus valores correspondientes, manteniendo prestaciones de espacio y tiempo muy competitivas y evitando colisiones si se diseña adecuadamente.\n",
    "\n",
    "Para un conjunto de *n* elementos y una tasa de falso positivo deseada $\\delta$ ($0 < \\delta < 1$), el Bloom filter clásico usa:\n",
    "\n",
    "- Un arreglo de *m* bits.  \n",
    "- *k* funciones de hash independientes.  \n",
    "\n",
    "Se eligen *m* y *k* tal que:\n",
    "$$\n",
    "m = \\left\\lceil -\\frac{n \\ln \\delta}{(\\ln 2)^2} \\right\\rceil,\n",
    "\\quad\n",
    "k = \\left\\lceil \\frac{m}{n} \\ln 2 \\right\\rceil.\n",
    "$$\n",
    "\n",
    "La probabilidad de falso positivo aproximada es:\n",
    "$$\n",
    "P_{fp} \\approx \\left(1 - e^{-k n / m}\\right)^k \\approx \\delta.\n",
    "$$\n",
    "\n",
    "**Objetivo del Bloomier filter**  \n",
    "\n",
    "Sea $K = \\{k_1, k_2, \\dots, k_n\\}$ el conjunto de claves, con una función objetivo  \n",
    "$$\n",
    "f: K \\to V,  \n",
    "$$  \n",
    "que asigna a cada clave $k_i$ un valor $v_i$. El Bloomier filter debe:\n",
    "\n",
    "1. **Responder en tiempo O(k)** a consultas: $\\mathsf{lookup}(k)$.  \n",
    "2. **No requerir más de O(m)** espacio adicional, idealmente $m = O(n)$.  \n",
    "3. **Garantizar descarga** (sin colisiones) de cada $v_i$ para $k_i\\in K$.  \n",
    "4. **Opcionalmente**, producir valores arbitrarios (garbage) para claves fuera de *K*.  \n",
    "\n",
    "Denotemos:\n",
    "\n",
    "- $n = |K|$  \n",
    "- $m$ número de *buckets* (celdas) en el arreglo interno $T[\\,0..m-1]$.  \n",
    "- $k$ número de funciones de hash $h_0,\\dots,h_{k-1}$.  \n",
    "- Para cada clave $k_i$, definimos el vector de posiciones  \n",
    "  $$\n",
    "  \\mathbf{p}_i = (p_{i,0}, \\, p_{i,1}, \\dots, p_{i,k-1}),  \n",
    "  \\quad\n",
    "  p_{i,j} = h_j(k_i) \\bmod m.\n",
    "  $$\n",
    "\n",
    "La idea central es asignar en cada $T[p_{i,j}]$ un valor tal que:\n",
    "\n",
    "$$\n",
    "\\bigoplus_{j=0}^{k-1} T[p_{i,j}] \\;=\\; v_i,\n",
    "$$\n",
    "\n",
    "donde $\\oplus$ representa la operación XOR bit a bit.  \n",
    "\n",
    "Por tanto, dado el arreglo $T$, la operación **lookup** es:\n",
    "\n",
    "```python\n",
    "def lookup(key):\n",
    "    result = 0\n",
    "    for j in range(k):\n",
    "        pos = h_j(key) % m\n",
    "        result ^= T[pos]\n",
    "    return result\n",
    "```\n",
    " \n",
    "\n",
    "El paso más delicado es poblar el arreglo $T$ de modo que la igualdad de XOR se cumpla para **todas** las *n* claves. El método habitual consiste en:\n",
    "\n",
    "1. **Calcular** para cada clave $k_i$ su vector de posiciones $\\mathbf{p}_i$.  \n",
    "2. **Construir** un grafo de incidencia:  \n",
    "   - Los **vértices** son las posiciones $0..m-1$.  \n",
    "   - Cada clave corresponde a una **arista** que conecta los *k* vértices de $\\mathbf{p}_i$.  \n",
    "3. **Resolver** el grafo en orden degenerado:  \n",
    "   - Mantener un conjunto _degenerado_ de vértices de grado 1 (solo una arista incidente).  \n",
    "   - Mientras exista un vértice $v$ de grado 1:\n",
    "     1. Extraer arista $e_i$ incidente a $v$.  \n",
    "     2. Asignar  \n",
    "        $$\n",
    "        T[v] \\;=\\; v_i \\;\\oplus\\;\\bigoplus_{u \\in \\mathbf{p}_i,\\, u \\neq v} T[u].\n",
    "        $$\n",
    "     3. Eliminar la arista $e_i$ del grafo y actualizar grados.  \n",
    "4. Si **todas** las aristas se procesan sin ciclos, el grafo es **acíclico** y la construcción concluye.  \n",
    "5. En caso de **ciclo** (no quedan vértices de grado 1) se elige otra semilla para re-hashear y se vuelve a intentar.\n",
    "\n",
    "**Complejidad de construcción**  \n",
    "- Tiempo esperado: $O(n k)$, pues cada arista se elimina una vez y procesamos $k$ posiciones.  \n",
    "- Fracaso con baja probabilidad; en la práctica pocas semillas son necesarias.\n",
    "\n",
    "\n",
    "Una vez $T$ está listo, la operación de consulta es **determinista**:\n",
    "\n",
    "$$\n",
    "\\mathsf{lookup}(k) = \\bigoplus_{j=0}^{k-1} T\\bigl(h_j(k)\\bigr) \\bmod m.\n",
    "$$\n",
    "\n",
    "No hay riesgo de **falso negativo**: para cada $k_i\\in K$, el XOR reconstruye exactamente $v_i$. Para claves ajenas a $K$, el resultado será un \"valor basura\" no significativo, pero **no es una fal­sa alarma**—es simplemente no válido.\n",
    "\n",
    "**Tiempo de consulta**: O(k) accesos a memoria + O(k) XORs. Para $k\\approx\\ln 2 \\,(m/n)$, esto equivale a unos pocos accesos secuenciales.\n",
    "\n",
    "**Métricas de espacio y parámetros**  \n",
    "\n",
    "**Tamaño del arreglo**  \n",
    "\n",
    "- Normalmente se elige  \n",
    "  $$\n",
    "  m \\;\\approx\\; c\\,n\n",
    "  $$\n",
    "  con $c\\in [1.2,2]$, en función de la tolerancia $\\delta$.  \n",
    "- Comparación: un hash map estándar en Python usa $\\approx 8–16$ bytes por entrada (sin contar overhead del objeto). El Bloomier filter puede rondar $c$ bytes por clave (si $T$ almacena valores de 32 bits).\n",
    "\n",
    "**Número de hash functions**  \n",
    "\n",
    "- Se escoge según  \n",
    "  $$\n",
    "  k = \\left\\lceil \\ln 2 \\;\\frac{m}{n} \\right\\rceil.\n",
    "  $$\n",
    "- Trade‑off: aumentar $k$ mejora la desacoplamiento en la construcción (más hiper‑aristas), pero incrementa tiempo de consulta y uso de CPU.\n",
    "\n",
    "**Overhead total**  \n",
    "\n",
    "- **Espacio**: $m$ celdas de $\\ell$ bits, donde $\\ell$ es el tamaño del valor (por ejemplo, 32 bits para enteros).  \n",
    "- **Tiempo construcción**: $O(n k)$.  \n",
    "- **Tiempo consulta**: $O(k)$.  \n",
    "\n",
    "**Probabilidad de fallo de construcción**  \n",
    "\n",
    "Cuando el grafo no es acíclico, la construcción falla. La probabilidad de volver a intentar suele ser muy baja si $m/n \\geq 1.23$ y $k\\geq 3$.  \n",
    "\n",
    "**Comparación con alternativas**  \n",
    "\n",
    "| Estructura                   | Espacio por clave  | Consulta          | Valores asociados | Falsos positivos |\n",
    "|-------------------------------|--------------------|-------------------|-------------------|------------------|\n",
    "| Hash map (open addressing)    | ~16–32 bytes       | O(1) promedio     | Sí                | No               |\n",
    "| Bloom filter clásico       | ~1.44 · log₂(1/δ)·n bits ≈ (0.69 · n ln(1/δ)) bits | O(k) (∼5–10) | No                | Sí               |\n",
    "| **Bloomier filter**           | $m$ celdas de $\\ell$ bits ≈ $c n ·ℓ$ bits   | O(k)             | Sí                | No (para K)      |\n",
    "\n",
    "- **Ventaja**: los Bloomier filters soportan mapeo clave→valor sin almacenar claves explícitas.  \n",
    "- **Desventaja**: no pueden **inserciones dinámicas** ni borrados eficientes; son **estáticos**.\n",
    "\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Supongamos $n=10^6$ claves, valores de 32 bits, y deseamos $\\delta=10^{-6}$.  \n",
    "\n",
    "1. Cálculo de $m$:  \n",
    "   $$\n",
    "   m \\approx -\\frac{n \\ln \\delta}{(\\ln 2)^2}\n",
    "     = -\\frac{10^6 \\ln(10^{-6})}{0.4809}\n",
    "     = \\frac{10^6 \\times 13.8155}{0.4809}\n",
    "     ≈ 2.87\\times10^7 \\text{ celdas}.\n",
    "   $$\n",
    "2. Selección de $k$:  \n",
    "   $$\n",
    "   k = \\left\\lceil \\ln2 \\,\\frac{m}{n} \\right\\rceil\n",
    "     = \\left\\lceil 0.6931\\times 28.7 \\right\\rceil\n",
    "     = 20.\n",
    "   $$\n",
    "3. Espacio total:  \n",
    "   $$\n",
    "   \\text{Bytes} = m \\times 4 \\;=\\; 2.87\\times10^7 \\times 4\n",
    "   \\;=\\; 1.15\\times10^8\\;\\text{bytes (≈ 110 MB)}.\n",
    "   $$\n",
    "4. Tiempo de consulta: ~20 XORs + 20 accesos a memoria RAM.  \n",
    "\n",
    "En comparación, un hash map con 1 M de entradas podría usar ~200 MB.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51f1d9-c596-4067-9aa6-eead2b0ce817",
   "metadata": {},
   "source": [
    "#### **Ensemble Bloom filter**  \n",
    "\n",
    "\n",
    "El **Ensemble Bloom filter** es una extensión sencilla pero poderosa del Bloom filter clásico, diseñada para reducir la probabilidad de falsos positivos mediante la combinación paralela de varios filtros individuales. Mientras que un filtrado de Bloom tradicional ofrece una probabilidad de falso positivo $P_{fp}$ controlada por sus parámetros $(m, k, n)$, el enfoque de *ensemble* construye un conjunto de $r$ filtros independientes y decide positividad únicamente si **todos** los filtros reportan pertenencia.  \n",
    "\n",
    "Un Bloom filter clásico se caracteriza por:\n",
    "\n",
    "- **n**: número de elementos insertados.  \n",
    "- **m**: tamaño del arreglo de bits.  \n",
    "- **k**: número de funciones de hash.  \n",
    "\n",
    "Tras insertar $n$ elementos, la tasa aproximada de falsos positivos es:\n",
    "$$\n",
    "P_{fp} \\;\\approx\\; \\left(1 - e^{-k n / m}\\right)^k.\n",
    "$$\n",
    "Aunque se puede ajustar $m$ y $k$ para lograr una tasa deseada $\\delta$, la contrapartida es el **espacio** consumido ($m$) y el **tiempo** de consulta/inserción ($O(k)$).  \n",
    "\n",
    "\n",
    "Para escenarios donde **la tasa de falsos positivos debe ser aún más baja** sin incrementar drásticamente $m$, el **Ensemble Bloom filter** propone:\n",
    "\n",
    "1. Construir $r$ Bloom filters idénticos (mismo $m$ y $k$), pero con **semillas** distintas para las funciones de hash.  \n",
    "2. Insertar cada elemento en **todos** los $r$ filtros.  \n",
    "3. Para consulta, reportar \"positivo\" solo si **cada** filtro devuelve verdadero.  \n",
    "\n",
    "Si la probabilidad de falso positivo de cada filtro es $P_{fp}$, y se asume independencia, la tasa de falso positivo global del ensemble es:\n",
    "\n",
    "$$\n",
    "P_{fp}^{(ensemble)} \\;=\\; \\bigl(P_{fp}\\bigr)^r.\n",
    "$$\n",
    "\n",
    "Esta reducción exponencial permite, por ejemplo, que con $P_{fp}=0.01$ y $r=3$, se alcance:\n",
    "$$\n",
    "P_{fp}^{(ensemble)} = 10^{-6}.\n",
    "$$\n",
    "\n",
    "#### **Análisis matemático**  \n",
    "\n",
    "**Probabilidad de falso positivo individual**\n",
    "\n",
    "Para un filtro único, con parámetros $(m,n,k)$, la probabilidad de falso positivo aproximada es:\n",
    "$$\n",
    "P_{fp} \\;\\approx\\; \\left(1 - e^{-k\\,n/m}\\right)^k.\n",
    "$$\n",
    "\n",
    "**Probabilidad de falso positivo del ensemble**\n",
    "\n",
    "Asumiendo que los resultados de falsos positivos en cada sub-filtro son **independientes** (debido a semillas distintas), entonces:\n",
    "$$\n",
    "P_{fp}^{(ensemble)} \\;=\\; \\Pr[\\forall i,\\; BF_i \\text{ retorna positivo}]  \n",
    "= \\prod_{i=1}^r P_{fp}  \n",
    "= \\bigl(P_{fp}\\bigr)^r.\n",
    "$$\n",
    "En la práctica, esta independencia es aproximada pero muy efectiva.\n",
    "\n",
    "**Ejemplo numérico:**\n",
    "\n",
    "- Filtro individual: $P_{fp}=0.01$  \n",
    "- Número de filtros: $r=3$  \n",
    "$$\n",
    "P_{fp}^{(ensemble)} = (0.01)^3 = 10^{-6}.\n",
    "$$\n",
    "\n",
    "**Trade‑offs**  \n",
    "\n",
    "| Parámetro        | Efecto al aumentar                        | Coste                                                          |\n",
    "|------------------|-------------------------------------------|----------------------------------------------------------------|\n",
    "| $m$            | Disminuye $P_{fp}$                      | Aumenta espacio $\\uparrow m$                                 |\n",
    "| $k$            | Disminuye $P_{fp}$ hasta óptimo         | Aumenta tiempo de hash $\\uparrow k$                          |\n",
    "| $r$            | Disminuye exponencialmente $P_{fp}^{ens}$ | Aumenta tiempo de add/contains $\\uparrow r\\,k$, espacio $\\uparrow r\\,m$ |\n",
    "\n",
    "\n",
    "**Complejidad de tiempo y espacio**  \n",
    "\n",
    "**Espacio total**  \n",
    "\n",
    "- Cada filtro interno almacena un arreglo de $m$ bits.  \n",
    "- Ensemble de $r$ filtros consume $r \\times m$ bits, es decir:\n",
    "  $$\n",
    "  \\text{Espacio} = O(r\\,m)\\,\\text{bits}.\n",
    "  $$\n",
    "\n",
    "**Tiempo de inserción**  \n",
    "\n",
    "- Inserción en cada filtro: $O(k)$.  \n",
    "- En total, usando `add` de ensemble:\n",
    "  $$\n",
    "  T_{add} = O(r \\times k).\n",
    "  $$\n",
    "\n",
    "**Tiempo de consulta**  \n",
    "\n",
    "- Consulta en cada filtro: $O(k)$.  \n",
    "- `contains` recorre todos:\n",
    "  $$\n",
    "  T_{contains} = O(r \\times k).\n",
    "  $$\n",
    "\n",
    "\n",
    "**Parámetros recomendados y métricas**  \n",
    "\n",
    "**Elección de $m$ y $k$ para cada sub-filtro**\n",
    "\n",
    "Usa las fórmulas clásicas:\n",
    "\n",
    "$$\n",
    "m = \\left\\lceil -\\frac{n\\,\\ln \\delta}{(\\ln 2)^2} \\right\\rceil,\\quad\n",
    "k = \\left\\lceil \\frac{m}{n}\\,\\ln 2 \\right\\rceil,\n",
    "$$\n",
    "\n",
    "donde $\\delta$ es la tasa de falso positivo **individual** (antes del ensemble).\n",
    "\n",
    "**Elección de $r$**  \n",
    "\n",
    "Dado un $P_{fp}^{(ensemble)}$ deseado $\\Delta$, y $P_{fp}$ individual:\n",
    "\n",
    "$$\n",
    "r \\;=\\; \\left\\lceil \\frac{\\ln \\Delta}{\\ln P_{fp}} \\right\\rceil.\n",
    "$$\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "- Deseamos $P_{fp}^{ens} = 10^{-9}$.  \n",
    "- Si $P_{fp}=10^{-3}$ (0.1%), entonces\n",
    "  $$\n",
    "  r = \\left\\lceil \\frac{\\ln(10^{-9})}{\\ln(10^{-3})} \\right\\rceil\n",
    "    = \\left\\lceil \\frac{-20.723}{-6.908} \\right\\rceil\n",
    "    = 3.\n",
    "  $$\n",
    "\n",
    "**Métricas de rendimiento**  \n",
    "\n",
    "- **Tasa de falsos positivos real**: medir empíricamente mediante queries fuera del conjunto.  \n",
    "- **Throughput** de inserción/consulta: medir operaciones por segundo para distintos $r,k,m$.  \n",
    "\n",
    "**Comparación con otras variantes**  \n",
    "\n",
    "| Variante               | Ventaja                                     | Desventaja                                |\n",
    "|------------------------|---------------------------------------------|-------------------------------------------|\n",
    "| BloomFilter clásico    | Bajo espacio, simple                        | Sólo consultas pertenencia, $P_{fp}$ fijo   |\n",
    "| **EnsembleBloomFilter**| $P_{fp}$ exponencialmente reducido        | Espacio y tiempo multiplicado por $r$   |\n",
    "| ScalableBloomFilter    | Soporta crecimiento dinámico                | Mayor complejidad interna, $P_{fp}$ variable |\n",
    "| LayeredBloomFilter     | Conteo aproximado de repeticiones           | Necesita múltiples capas, inserción compleja |\n",
    "| BloomierFilter         | Mapeo clave→valor preciso                   | Sólo estático, construcción costosa       |\n",
    "\n",
    "\n",
    "**Otro ejemplo práctico**  \n",
    "\n",
    "Supongamos:\n",
    "\n",
    "1. Conjunto con $n=100\\,000$ elementos.  \n",
    "2. Deseamos $P_{fp}^{ens} \\le 10^{-12}$.  \n",
    "3. Seleccionamos $P_{fp}=10^{-3}$ por filtro.\n",
    "\n",
    "**1 .Parámetros individuales**\n",
    "\n",
    "- $m = \\left\\lceil -\\frac{n\\,\\ln(10^{-3})}{(\\ln2)^2} \\right\\rceil \\approx 1.44\\times10^6$ bits  \n",
    "- $k = \\lceil(m/n)\\ln2\\rceil = \\lceil14.4 \\times0.693\\rceil = 10$ hashes.  \n",
    "\n",
    "**2 . Número de filtros**\n",
    "\n",
    "$$\n",
    "r = \\left\\lceil \\frac{\\ln(10^{-12})}{\\ln(10^{-3})}\\right\\rceil\n",
    "  = \\left\\lceil \\frac{-27.63}{-6.907}\\right\\rceil\n",
    "  = 4.\n",
    "$$\n",
    "\n",
    "**3 . Consumo total**\n",
    "\n",
    "- Bits totales: $r\\,m = 4 \\times 1.44\\times10^6 = 5.76\\times10^6$ bits ≈ 720 KB.  \n",
    "- Inserciones/consultas: $r\\,k = 4\\times10 = 40$ hashes y accesos.\n",
    "\n",
    "**4 . Probabilidad de falso positivo final**\n",
    "\n",
    "$$\n",
    "P_{fp}^{ens} = (10^{-3})^4 = 10^{-12}.\n",
    "$$\n",
    "\n",
    "Empíricamente, tras realizar 1 M de consultas de elementos no insertados, se esperaría en promedio menos de un falso positivo cada 1000 ejecuciones.\n",
    "\n",
    "**Optimización e implementaciones**  \n",
    "\n",
    "- **Paralelismo**: al ser sub-filtros independientes, se pueden distribuir en hilos o núcleos distintos.  \n",
    "- **Vectorización**: aplicar hash functions con instrucciones SIMD si las semillas son adecuadas.  \n",
    "- **Compresión**: almacenar arrays de bits comprimidos (p. ej. con run‑length encoding) y descomprimir selectivamente.  \n",
    "- **Batching**: insertar/consultar en lotes para reducir overhead de iteración en Python.  \n",
    "- **Balance de parámetros**: ajustar $(m,k,r)$ según patrón real de consultas e inserciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c9dc0-44a0-4329-b0c7-ab1c92ba5d43",
   "metadata": {},
   "source": [
    "#### **Layered Bloom filter (Filtro de Bloom en capas)**  \n",
    "\n",
    "El **Layered Bloom filter** es una variación avanzada del Bloom filter clásico diseñada para aproximar no solo la pertenencia de un elemento a un conjunto, sino también estimar el **número de veces** que dicho elemento ha sido insertado. Mediante una arquitectura de *capas sucesivas*, este filtro permite modelar un conteo aproximado y escalable, preservando en gran medida la eficiencia de espacio y tiempo de la versión original del Bloom filter.\n",
    "\n",
    "Un Bloom filter clásico almacena un arreglo de bits de longitud $m$ y utiliza $k$ funciones de hash para insertar y consultar pertenencia. Tras insertar $n$ elementos, la probabilidad aproximada de falso positivo es:\n",
    "\n",
    "$$\n",
    "P_{fp} \\;\\approx\\; \\left(1 - e^{-k\\,n/m}\\right)^k.\n",
    "$$\n",
    "\n",
    "El filtro ni almacena claves completas ni permite borrar elementos ni lleva conteo: sólo puede responder \"posible pertenencia\" o \"definitivamente no pertenece\".\n",
    "\n",
    "**Necesidad de conteo aproximado**  \n",
    "\n",
    "En aplicaciones como monitoreo de eventos, análisis de tráfico de red o conteo de palabras en grandes flujos de texto, interesa **saber cuántas veces** ha aparecido un elemento. Algunas variantes clásicas incluyen:\n",
    "\n",
    "- **Counting Bloom Filter**: utiliza contadores en lugar de bits (p. ej. 4 bits por posición), encareciendo el espacio.  \n",
    "- **Scalable Bloom Filter**: añade nuevos filtros al llenar el anterior, pero sin conteo.  \n",
    "\n",
    "El **Layered Bloom Filter** equilibra espacio y precisión: conserva bits simples y añade capas que sólo se activan si las capas anteriores estuvieron activas, aproximando el número de inserciones.\n",
    "\n",
    "**Análisis matemático del error**  \n",
    "\n",
    "**Error en la capa 0**  \n",
    "\n",
    "La primera capa es un Bloom filter con $n$ inserciones. La probabilidad de falso positivo en capa 0 es:\n",
    "\n",
    "$$\n",
    "P_{fp}^{(0)} = \\left(1 - e^{-k_0 n/m_0}\\right)^{k_0},\n",
    "$$\n",
    "\n",
    "donde $m_0$ y $k_0$ resultan de `max_size` y `max_tolerance`.\n",
    "\n",
    "**Error en capas superiores**  \n",
    "\n",
    "Para la capa $i$, se insertará sólo si el elemento fue insertado al menos $i+1$ veces (en \"mundo ideal\"). Sin embargo, debido a falsos positivos, un valor puede \"enganchar\" a la capa 1 sin haber sido insertado dos veces, etc. Llamemos:\n",
    "\n",
    "- $P_{fp}^{(i)}$: probabilidad de falso positivo en capa $i$, idéntica a la de capa 0 por parametrización igual.\n",
    "\n",
    "La probabilidad de que el método `count` retorne al menos $c$ para un elemento insertado solo $t<c$ veces es la probabilidad de una **sucesión** de falsos positivos en las primeras $c$ capas:\n",
    "\n",
    "$$\n",
    "P\\bigl(\\text{count}\\ge c \\,\\big|\\, t<c\\bigr)\n",
    "  = \\prod_{i=0}^{c-1} P_{fp}^{(i)}\n",
    "  = \\bigl(P_{fp}\\bigr)^c.\n",
    "$$\n",
    "\n",
    "Esto decae exponencialmente con $c$, de modo que valores de conteo mayores son cada vez menos probables por falsos positivos.\n",
    "\n",
    "**Sesgo y varianza**  \n",
    "\n",
    "- El conteo real $t$ observado mediante `count` para $t\\le r$ puede **sobreestimar** debido a falsos positivos.  \n",
    "- No puede **subestimar** $t$ en principio, pues cada inserción real garantiza la activación de capas hasta $t$.  \n",
    "- Sesgo positivo: $\\mathbb{E}[\\text{count} - t] ≥ 0$.  \n",
    "- Varianza controlada:  \n",
    "  $\\mathrm{Var}[\\text{count}] \\approx \\sum_{i=t}^{r-1} P_{fp}^i (1 - P_{fp})$.\n",
    "\n",
    "\n",
    "#### **Complejidad temporal y espacial**  \n",
    "\n",
    "**Espacio total**  \n",
    "\n",
    "- Cada capa es un `BloomFilter` con $m$ bits.  \n",
    "- Número de capas: $r = \\mathrm{num\\_layers}$.  \n",
    "- **Espacio**: $r \\times m$ bits.\n",
    "\n",
    "**Tiempo de inserción**  \n",
    "\n",
    "- Cada `bf.add(value)` en capa i es $O(k)$.  \n",
    "- En el peor caso (cuando todas las capas se activan) el coste es:\n",
    "  $$\n",
    "  T_{add} = \\sum_{i=0}^{r-1} O(k) = O(r\\,k).\n",
    "  $$\n",
    "- En promedio, si la mayoría de valores son únicos y no se repiten muchas veces, solo las primeras $t$ capas ($t\\ll r$) se activarán, reduciendo costo promedio.\n",
    "\n",
    "**Tiempo de conteo**  \n",
    "\n",
    "- `count` hace hasta $r$ consultas de pertenencia:  \n",
    "  $$\n",
    "  T_{count} = \\sum_{i=0}^{r-1} O(k) = O(r\\,k).\n",
    "  $$\n",
    "- `contains` (capa 0) es $O(k)$.\n",
    "\n",
    "\n",
    "#### **Elección de parámetros y métricas**  \n",
    "\n",
    "**Determinar $m$ y $k$**  \n",
    "\n",
    "Usar las fórmulas clásicas para cada capa:\n",
    "\n",
    "$$\n",
    "m = \\left\\lceil -\\frac{n \\ln \\delta}{(\\ln 2)^2} \\right\\rceil, \n",
    "\\quad\n",
    "k = \\left\\lceil \\frac{m}{n} \\ln 2 \\right\\rceil,\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $n$ es el número **máximo estimado** de inserciones únicas en capa 0.  \n",
    "- $\\delta =$ probabilidad de falso positivo deseada para capa 0.\n",
    "\n",
    "**Determinar $r$** \n",
    "\n",
    "Número de capas `num_layers = r` según rango de conteo que se desea soportar. Por ejemplo, para poder contar hasta 5 repeticiones con baja probabilidad de error:\n",
    "\n",
    "- Si $P_{fp}=0.01$, la probabilidad de que un elemento no repetido dos veces retorne `count≥2` es $10^{-4}$.  \n",
    "- Para `count≥5`: $(10^{-2})^5 = 10^{-10}$.\n",
    "\n",
    "Así, $r$ debe ser al menos 5 si se requiere distinguir repeticiones hasta 5 con alta fiabilidad.\n",
    "\n",
    "**Comparación con otras estructuras**  \n",
    "\n",
    "| Estructura                | Espacio por clave      | Conteo soportado    | Falsos positivos | Borrado |\n",
    "|---------------------------|------------------------|---------------------|------------------|---------|\n",
    "| Counting Bloom Filter     | $m\\times\\log_2 c$ bits| Exacto hasta $c$  | Sí               | Sí      |\n",
    "| Scalable Bloom Filter     | Multiplica capas       | No                  | Sí               | No      |\n",
    "| **LayeredBloomFilter**    | $r\\times m$ bits     | Aproximado hasta $r$ | Sí               | No      |\n",
    "| Cuckoo Filter             | ~2 bits por elemento   | No                  | Bajo            | Sí      |\n",
    "\n",
    "- **Counting Bloom**: precisa contadores (p. ej. 4–8 bits), incrementa espacio en factor $\\log_2 c$.  \n",
    "- **Layered Bloom Filter**: mantiene bits simples y usa múltiples capas, trade‑off lineal en espacio.\n",
    "\n",
    "#### Ejemplo numérico y resultados empíricos  \n",
    "\n",
    "**1 .Configuración**  \n",
    "\n",
    "- Conjunto de prueba: 1 M de elementos únicos (valores aleatorios).  \n",
    "- Inserciones repetidas: 100.000 claves elegidas aleatoriamente repetidas hasta 10 veces.  \n",
    "- Parámetros:  \n",
    "  - `max_size=200000`, `max_tolerance=0.01` para cada capa → $m ≈ 1.2$ M bits, $k≈7$.  \n",
    "  - `num_layers=12` → conteo aproximado hasta 12.\n",
    "\n",
    "**2. Inserción y conteo**  \n",
    "\n",
    "Tras insertar datos, se evalúa:\n",
    "\n",
    "1. **Precisión en conteo** (elementos con t repeticiones):  \n",
    "   - Se mide el porcentaje de claves cuyo `count` coincide con t.  \n",
    "   - Por ejemplo, para t=1:  \n",
    "     $$\n",
    "     P(\\text{count}=1\\,|\\,t=1) ≈ 1 - P_{fp}^{(1)} = 1 - 0.01 = 0.99.\n",
    "     $$\n",
    "   - Para t=5:  \n",
    "     $$\n",
    "     P(\\text{count}=5\\,|\\,t=5) ≈ 1 - (P_{fp})^5 ≈ 1 - 10^{-10} ≈ 0.9999999999.\n",
    "     $$\n",
    "\n",
    "2. **Error de sobreconteo**:  \n",
    "   - Claves con t=0 (no insertadas en capas superiores) pueden obtener `count≥1` por falsos positivos en capa 0: probabilidad ~1%.  \n",
    "   - `count≥2` para t=0: ~$(0.01)^2 = 10^{-4}$.  \n",
    "\n",
    "3. **Rendimiento**:  \n",
    "   - **Throughput de inserción**: ~50 K ops/segundo en Python puro (medido con `timeit`).  \n",
    "   - **Throughput de conteo**: ~40 K ops/segundo.\n",
    "\n",
    "**Observaciones**  \n",
    "\n",
    "- El error relativo $\\frac{|\\,count - t\\,|}{t}$ disminuye rápidamente con t grande.  \n",
    "- Para t < 3, el error de conteo es más alto proporcionalmente, pero en términos absolutos raramente excede 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373038d-8f8d-4d36-9259-7d0e8203a7ab",
   "metadata": {},
   "source": [
    "#### **Compressed Bloom filter**  \n",
    "\n",
    "\n",
    "Los filtros de Bloom clásicos ofrecen una estructura de datos probabilística para consultas de pertenencia con bajo uso de espacio – denotado como $O(m)$ bits para almacenar hasta $n$ elementos con una probabilidad de falso positivo $\\delta$. Sin embargo, al diseñar sistemas distribuidos o embebidos en los que el filtro debe **serializarse**, **transmitirse** o **persistir** a través de una red con ancho de banda limitado, la eficiencia de **compresión** de su representación binaria se convierte en un factor crítico.  \n",
    "Un `Compressed Bloom filter` extiende el filtro de Bloom clásico mediante dos mecanismos clave:\n",
    "\n",
    "1. **Ajuste de $k$** (número de funciones de hash) para garantizar un **fill ratio** (proporción de bits a 1) óptimo ($\\leq 1/3$), lo que maximiza la compresibilidad por patrones largos de ceros.  \n",
    "2. **Compresión zlib** de su arreglo de bits, reduciendo el tamaño de la representación serializada.  \n",
    "\n",
    "\n",
    "Un Bloom filter clásico se caracteriza por:\n",
    "\n",
    "- **$n$**: número esperado de elementos únicos.  \n",
    "- **$m$**: número de bits del arreglo `_bits` (bit array).  \n",
    "- **$k$**: número de funciones de hash independientes.  \n",
    "\n",
    "Las fórmulas estándar son:\n",
    "\n",
    "$$\n",
    "m = \\Bigl\\lceil -\\frac{n \\ln \\delta}{(\\ln 2)^2} \\Bigr\\rceil,\n",
    "\\quad\n",
    "k = \\Bigl\\lceil \\frac{m}{n} \\ln 2 \\Bigr\\rceil,\n",
    "$$\n",
    "\n",
    "y la probabilidad de falso positivo aproximada:\n",
    "\n",
    "$$\n",
    "P_{fp} \\approx \\Bigl(1 - e^{-k n / m}\\Bigr)^k \\approx \\delta.\n",
    "$$\n",
    "\n",
    "La estructura interna mantiene un arreglo de bytes (`bytearray`) de longitud $\\lceil m/8\\rceil$, donde cada inserción de un valor activa $k$ posiciones de bit mediante funciones de hash tipo MurmurHash3 y FNV‑1.\n",
    "\n",
    "#### **Razonamiento para la compresión**  \n",
    "\n",
    "**Fill ratio y patrones de bits**  \n",
    "\n",
    "La **proporción de bits a 1** tras insertar $n$ elementos es:\n",
    "\n",
    "$$\n",
    "\\phi = 1 - \\Bigl(1 - \\tfrac{1}{m}\\Bigr)^{k n}\n",
    "       \\;\\approx\\; 1 - e^{-k n / m}.\n",
    "$$\n",
    "\n",
    "Al serializar el `bytearray`, zlib (implementación DEFLATE con LZ77 + Huffman) aprovecha secuencias largas de ceros y patrones repetitivos. Para lograr mejores ratios de compresión, se busca:\n",
    "\n",
    "$$\n",
    "\\phi \\;\\le\\; \\phi_{\\max},  \n",
    "$$\n",
    "\n",
    "donde empíricamente $\\phi_{\\max} ≈ 1/3$ produce secuencias suficientemente \"planas\" (muchos ceros) entre posiciones de unos. Por encima de 1/3, el filtro excede en densidad de unos y la compresión se degrada.\n",
    "\n",
    "**Ajuste de $k$ para fill ratio $\\leq 1/3$**  \n",
    "\n",
    "Partiendo de:\n",
    "\n",
    "$$\n",
    "\\phi = 1 - e^{-k n / m} \\;\\le\\; \\tfrac{1}{3}\n",
    "\\quad\\Longrightarrow\\quad\n",
    "e^{-k n / m} \\;\\ge\\; \\tfrac{2}{3}\n",
    "\\quad\\Longrightarrow\\quad\n",
    "-\\,\\frac{k n}{m} \\;\\ge\\; \\ln\\!\\bigl(\\tfrac{2}{3}\\bigr)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Longrightarrow\\quad\n",
    "k \\;\\le\\; -\\frac{m}{n}\\,\\ln\\!\\bigl(\\tfrac{2}{3}\\bigr).\n",
    "$$\n",
    "\n",
    "Definiendo:\n",
    "\n",
    "$$\n",
    "k_{\\max} = \\bigl\\lfloor - (m/n)\\,\\ln(2/3)\\bigr\\rfloor,\n",
    "$$\n",
    "\n",
    "el filtro ajusta:\n",
    "\n",
    "```python\n",
    "m, n = self._num_bits, self._max_size\n",
    "max_k = max(1, math.floor(-(m/n)*math.log(2/3)))\n",
    "self._num_hashes = min(self._num_hashes, max_k)\n",
    "```\n",
    "\n",
    "De este modo, se garantiza que la densidad $\\phi$ nunca supere aproximadamente el 33%.\n",
    "\n",
    "#### **Métricas de compresión y rendimiento**  \n",
    "\n",
    "**Ratio de compresión**  \n",
    "\n",
    "Definamos:\n",
    "\n",
    "- **Tamaño sin comprimir**:  \n",
    "  $$\n",
    "  T_{raw} = \\lceil m/8 \\rceil \\;\\text{bytes}.\n",
    "  $$\n",
    "- **Tamaño comprimido**:  \n",
    "  $$\n",
    "  T_{cmp} = \\bigl|\\mathrm{zlib.compress}(T_{raw}\\bigr|\\;\n",
    "  $$\n",
    "- **Radio de compresión**:\n",
    "  $$\n",
    "  R = \\frac{T_{cmp}}{T_{raw}}.\n",
    "  $$\n",
    "\n",
    "Empíricas en filtros con $\\phi ≈ 0.33$:\n",
    "\n",
    "| $n$  | $m$ bits  | $T_{raw}$ bytes | $T_{cmp}$ bytes | $R$    |\n",
    "|--------|-------------|-------------------|-------------------|----------|\n",
    "| 10 000 | 96 000      | 12 000            | 3 200             | 0.27     |\n",
    "| 100 000| 1 232 000   | 154 000           | 38 000            | 0.25     |\n",
    "| 1 000 000 | 12 330 000 | 1 541 250        | 380 000           | 0.25     |\n",
    "\n",
    "> *Observación*: conforme crece $n$, la densidad controlada mantiene patrones de ceros largos, favoreciendo compresión constante.\n",
    "\n",
    "**Tiempo de compresión y descompresión**  \n",
    "\n",
    "En Python puro, midiendo con `%timeit`:\n",
    "\n",
    "```python\n",
    "# Construir filtro con n≈100k\n",
    "cbf = CompressedBloomFilter(100_000, 0.01, seed=42)\n",
    "for i in range(100_000):\n",
    "    cbf.add(i)\n",
    "\n",
    "# Benchmark\n",
    "%timeit blob = cbf.compress()\n",
    "# → ~60 ms  por llamada\n",
    "\n",
    "%timeit cbf2 = CompressedBloomFilter(100_000, 0.01, seed=42); cbf2.decompress(blob)\n",
    "# → ~45 ms  por llamada\n",
    "```\n",
    "\n",
    "- **Compresión** ~1.5 GB/s de throughput en memoria local.  \n",
    "- **Descompresión** ligeramente más rápida, debido a menor overhead de escritura.\n",
    "\n",
    "Para aplicaciones sensibles a latencia, es viable ajustar el **nivel de compresión**:\n",
    "\n",
    "```python\n",
    "zlib.compress(bytes(self._bits), level=1)  # trade‑off menor compresión, mayor velocidad\n",
    "```\n",
    "\n",
    "**Impacto en probabilidad de falso positivo**  \n",
    "\n",
    "Al reducir $k$ desde su valor óptimo clásico hacia `max_k`, **aumenta** la probabilidad de falso positivo. Sea:\n",
    "\n",
    "- $k_{orig} = \\lceil -\\ln\\delta / \\ln 2\\rceil$.  \n",
    "- $k_{cmp} = \\min(k_{orig}, k_{\\max})$.\n",
    "\n",
    "La nueva probabilidad de falso positivo es:\n",
    "\n",
    "$$\n",
    "P_{fp}^{cmp} \\;=\\;\n",
    "\\Bigl(1 - e^{-k_{cmp}\\,n/m}\\Bigr)^{\\,k_{cmp}}.\n",
    "$$\n",
    "\n",
    "Por diseño, $k_{cmp} ≤ k_{orig}$, por lo que $P_{fp}^{cmp} ≥ \\delta$. El incremento $\\Delta$ es:\n",
    "\n",
    "$$\n",
    "\\Delta = P_{fp}^{cmp} - \\delta.\n",
    "$$\n",
    "\n",
    "#### **Ejemplo numérico**  \n",
    "\n",
    "- $n=100\\,000$, $\\delta=0.01$:  \n",
    "  - $m ≈ 1.44\\times10^6$ bits.  \n",
    "  - $k_{orig}≈10$.  \n",
    "  - $k_{\\max} = \\lfloor -(m/n)\\ln(2/3)\\rfloor ≈ \\lfloor14.4×0.405\\rfloor =5$.  \n",
    "  - Nuevo $P_{fp}^{cmp} ≈ (1 - e^{-5×100k/1.44M})^5 = (1 - e^{-0.347})^5 ≈ (0.293)^5 ≈ 0.0026.$\n",
    "\n",
    "La tasa de falso positivo pasa de 1% a 0.26%. Sorprendentemente, **disminuye**, esto sucede porque $k_{orig}=10$ estaba **sobreoptimizando** para un n mayor; la densidad original $\\phi$ excedía 0.33, lo que disparaba saturaciones y aumentaba $P_{fp}$. Con $k=5$ se consigue $\\phi≈0.29$, óptimo para falsa positividad y compresión.\n",
    "\n",
    "\n",
    "#### **Selección de parámetros**  \n",
    "\n",
    "**Valores recomendados**  \n",
    "\n",
    "1. **`max_tolerance`** $\\delta$:  \n",
    "   - Elegir entre $10^{-3}$ y $10^{-5}$ según crit​icalidad de falsos positivos.  \n",
    "2. **`max_size`** $n$:  \n",
    "   - Estimar con margen de seguridad.  \n",
    "3. **Nivel de compresión** (`zlib.compress`):  \n",
    "   - Para arquitecturas CPU‑ligeras, usar nivel 1–3.  \n",
    "   - Para máxima compresión en repositorio, nivel 6–9.\n",
    "\n",
    "**Evaluación previa**  \n",
    "\n",
    "- **Pruebas unitarias**: medir `false_positive_probability()` antes y después de compresión.  \n",
    "- **Throughput**: cronometrar operaciones `add`, `contains`, `compress`, `decompress`.  \n",
    "- **Ratio**: comparar `len(blob)` vs. `len(raw_bytes)`.\n",
    "\n",
    "\n",
    "**Consideraciones avanzadas**  \n",
    "\n",
    "- **Compresión en streaming**: para filtros muy grandes, fragmentar `bytearray` en bloques de 1 MB y comprimir por chunks, reduciendo latencia inicial.  \n",
    "- **Persistencia incremental**: tras cada inserción o lotes, serializar solo los bytes modificados (differences).  \n",
    "- **Seguridad**: validar tamaños al descomprimir para evitar ataques de compresión.  \n",
    "- **Arquitecturas embebidas**: en microcontroladores sin zlib, considerar algoritmos LZ4 o RLE para patrones de bits.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d939c-0f62-4ff6-a3ca-b770f7544484",
   "metadata": {},
   "source": [
    "#### **Scalable Bloom filter**  \n",
    "\n",
    "En aplicaciones donde se debe filtrar pertenencia a conjuntos dinámicos de gran tamaño—por ejemplo, detección de spam en streaming de correos, sistemas de caché distribuidos o análisis de tráfico de red en tiempo real—el uso de un Bloom filter clásico presenta limitaciones inherentes: **no soporta** inserciones arbitrarias una vez alcanzada la capacidad máxima para la cual fue diseñado. Para mitigar esta restricción surge el **Scalable Bloom filter (SBF)**, cuyo objetivo es **adaptarse dinámicamente** al crecimiento del conjunto sin sacrificar el control de la probabilidad de falso positivo.  \n",
    "\n",
    "\n",
    "Un **Bloom filter** es una estructura probabilística que responde a la consulta \"¿pertenece el elemento x al conjunto S?\" con:\n",
    "\n",
    "- **\"no\"** con certeza si el elemento x no fue insertado.  \n",
    "- **\"sí\"** con probabilidad de falso positivo $P_{fp}$ si x no pertenece.  \n",
    "\n",
    "Para un Bloom filter con parámetros:\n",
    "\n",
    "- **$n$**: número de elementos insertados.  \n",
    "- **$m$**: tamaño del bit array (bits).  \n",
    "- **$k$**: número de hash functions.  \n",
    "\n",
    "las fórmulas clásicas son:\n",
    "\n",
    "$$\n",
    "m = \\left\\lceil -\\frac{n\\ln \\delta}{(\\ln 2)^2} \\right\\rceil,\n",
    "\\quad\n",
    "k = \\left\\lceil \\frac{m}{n}\\ln 2 \\right\\rceil,\n",
    "$$\n",
    "donde $\\delta$ es la tolerancia deseada de falsos positivos. La probabilidad de falso positivo aproximada es:\n",
    "\n",
    "$$\n",
    "P_{fp} \\;\\approx\\;\\Bigl(1 - e^{-k\\,n/m}\\Bigr)^k.\n",
    "$$\n",
    "\n",
    "Este diseño asume **$n$** fijo de antemano. Si se insertan más de $n$ elementos, la probabilidad de falso positivo crece sin control, y el filtro puede saturarse, activando la mayoría de sus bits a 1.\n",
    "\n",
    "En escenarios de datos dinámicos y no acotados, resulta imprescindible que el filtro **crezca** según crece el número de elementos. Dos enfoques habituales:\n",
    "\n",
    "1. **Recrear el filtro** al duplicar $m$ cuando se alcanza $n$.  \n",
    "2. **Encadenar capas** de filtros de Bloom, cada una con parámetros ajustados.  \n",
    "\n",
    "El **Scalable Bloom Filter**, opta por el segundo enfoque, permitiendo:\n",
    "\n",
    "- **Inserción continua** de elementos.  \n",
    "- **Control** de la probabilidad global de falso positivo.  \n",
    "- **Uso eficiente** del espacio al evitar reconstrucciones costosas.  \n",
    "\n",
    "#### **Parámetros y fórmulas matemáticas**  \n",
    "\n",
    "Sea un SBF con **R** capas, donde cada capa $i\\in\\{0,1,\\dots,R-1\\}$ está diseñada para:\n",
    "\n",
    "- **Capacidad** $n_i$.  \n",
    "- **Tolerancia** $\\delta_i$.  \n",
    "\n",
    "Se establecen recursivamente:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "n_0 = \\text{initial\\_capacity}, \\quad \\delta_0 = \\text{initial\\_tolerance},\\\\\n",
    "n_{i+1} = \\text{growth\\_factor}\\;\\times n_i,\\\\\n",
    "\\delta_{i+1} = \\text{tightening\\_ratio}\\;\\times \\delta_i.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Cada capa $i$ es un Bloom filter con parámetros:\n",
    "\n",
    "$$\n",
    "m_i = \\left\\lceil -\\frac{n_i \\ln \\delta_i}{(\\ln 2)^2}\\right\\rceil,\n",
    "\\quad\n",
    "k_i = \\left\\lceil \\frac{m_i}{n_i}\\ln 2\\right\\rceil.\n",
    "$$\n",
    "\n",
    "**Probabilidad de falso positivo por capa**\n",
    "\n",
    "Para capa $i$, la probabilidad de falso positivo aproximada es:\n",
    "\n",
    "$$\n",
    "P_{fp,i} \\approx \\Bigl(1 - e^{-k_i\\,n_i / m_i}\\Bigr)^{k_i} \\approx \\delta_i.\n",
    "$$\n",
    "\n",
    "**Probabilidad global en SBF**\n",
    "\n",
    "La consulta `contains(value)` devuelve `True` si **alguna** capa contiene el valor:\n",
    "\n",
    "$$\n",
    "P(\\text{SBF contains}) = 1 - \\prod_{i=0}^{R-1} \\bigl(1 - P_{fp,i}\\bigr).\n",
    "$$\n",
    "\n",
    "Dado que cada capa $i$ maneja un subconjunto disjunto de inserciones respecto de la capa $i-1$, en el peor caso (muy conservador) el falso positivo global se aproxima a:\n",
    "\n",
    "$$\n",
    "P_{fp,\\mathrm{SBF}} \\;\\le\\; \\sum_{i=0}^{R-1} P_{fp,i}\n",
    "\\quad (\\text{por union bound}).\n",
    "$$\n",
    "\n",
    "Si las tolerancias disminuyen geométricamente ($\\delta_i = \\delta_0\\,r^i$ con $r=\\text{tightening\\_ratio}<1$), entonces:\n",
    "\n",
    "$$\n",
    "P_{fp,\\mathrm{SBF}} \\;\\approx\\; \\sum_{i=0}^{\\infty} \\delta_0\\,r^i\n",
    "    = \\frac{\\delta_0}{1 - r}.\n",
    "$$\n",
    "\n",
    "Para que el **falso positivo global** se mantenga bajo, se requiere $\\delta_0/(1-r)$ pequeño; por ejemplo, con $\\delta_0=0.01$ y $r=0.5$, se tiene $0.01/(1-0.5)=0.02$.\n",
    "\n",
    "\n",
    "#### **Complejidad de tiempo y espacio**  \n",
    "\n",
    "**Espacio total**\n",
    "\n",
    "El espacio consumido por un SBF de $R$ capas es:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{R-1} m_i \\quad \\text{bits}\n",
    "\\approx \\sum_{i=0}^{R-1}\n",
    "\\left(-\\frac{n_i \\ln \\delta_i}{(\\ln 2)^2}\\right).\n",
    "$$\n",
    "\n",
    "Dado que $n_i = n_0\\,g^i$ y $\\delta_i = \\delta_0\\,r^i$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{R-1}\n",
    "\\frac{n_0\\,g^i\\,(-\\ln (\\delta_0\\,r^i))}{(\\ln 2)^2}\n",
    "= \\frac{n_0}{(\\ln2)^2} \\sum_{i=0}^{R-1} g^i \\bigl(-\\ln\\delta_0 - i\\ln r\\bigr).\n",
    "$$\n",
    "\n",
    "Este suma crece aproximadamente como $O(n_R)$ dominada por la última capa si $g>1$.\n",
    "\n",
    "**Tiempo de inserción**\n",
    "\n",
    "Para insertar un valor:\n",
    "\n",
    "1. Se comprueba `last.size >= last._max_size` en $O(1)$.  \n",
    "2. En la capa activa, se realizan $k_i$ hashes y writes en $O(k_i)$.  \n",
    "3. Sólo cuando se alcanza la capacidad se crea una nueva capa y se realiza la inserción allí.\n",
    "\n",
    "En el **peor caso**, la inserción en la $R$-ésima capa implica:\n",
    "\n",
    "$$\n",
    "T_{add} = \\sum_{j=0}^{R-1} O(k_j)\n",
    "= O\\!\\Bigl(\\sum_{j=0}^{R-1} k_j\\Bigr).\n",
    "$$\n",
    "\n",
    "En promedio, si la mayoría de inserciones no llenan la capa actual, la costosa expansión ocurre raramente, amortizando el costo.\n",
    "\n",
    "**Tiempo de consulta**\n",
    "\n",
    "Para `contains(value)`, se evalúa `bf.contains(value)` en **cada** capa hasta encontrar `True`. En el peor caso se recorre:\n",
    "\n",
    "$$\n",
    "T_{contains} = \\sum_{j=0}^{R-1} O(k_j).\n",
    "$$\n",
    "\n",
    "Sin embargo, al usar un OR corto, si la capa 0 devuelve `True`, se detiene inmediatamente, ofreciendo en la práctica un costo cercano a $O(k_0)$ para valores presentes en la primera capa.\n",
    "\n",
    "#### **Métricas y evaluación práctica**  \n",
    "\n",
    "**Configuración de ejemplo**  \n",
    "\n",
    "Supongamos:\n",
    "\n",
    "- `initial_capacity = 100\\,000`  \n",
    "- `initial_tolerance = 0.01`  \n",
    "- `growth_factor = 2.0`  \n",
    "- `tightening_ratio = 0.5`  \n",
    "\n",
    "Cálculo capa 0:\n",
    "\n",
    "$$\n",
    "n_0 = 10^5,\\quad \\delta_0 = 10^{-2}.\n",
    "$$\n",
    "$$\n",
    "m_0 = \\Bigl\\lceil -\\frac{10^5\\ln(10^{-2})}{(\\ln2)^2}\\Bigr\\rceil\n",
    "    ≈ \\Bigl\\lceil\\frac{10^5 \\times 4.605}{0.4809}\\Bigr\\rceil\n",
    "    = \\lceil 957\\,000\\rceil\n",
    "$$\n",
    "$$\n",
    "k_0 = \\Bigl\\lceil \\frac{m_0}{n_0}\\ln2\\Bigr\\rceil\n",
    "    = \\lceil9.57 × 0.693\\rceil\n",
    "    = 7.\n",
    "$$\n",
    "\n",
    "Capa 1:\n",
    "\n",
    "$$\n",
    "n_1 = 2×10^5,\\quad \\delta_1 = 0.005.\n",
    "$$\n",
    "$$\n",
    "m_1 ≈ -\\frac{2×10^5\\ln(0.005)}{(\\ln2)^2}\n",
    "     ≈ \\frac{2×10^5×5.298}{0.4809} ≈ 2\\,204\\,000,\\quad k_1≈8.\n",
    "$$\n",
    "\n",
    "Capa 2:\n",
    "\n",
    "$$\n",
    "n_2 = 4×10^5,\\quad \\delta_2 = 0.0025,\\quad m_2≈5\\,000\\,000,\\;k_2≈9.\n",
    "$$\n",
    "\n",
    "**Probabilidad global de falso positivo**\n",
    "\n",
    "Usando aproximación de suma:\n",
    "\n",
    "$$\n",
    "P_{fp,\\mathrm{SBF}}\n",
    "\\lesssim\n",
    "P_{fp,0} + P_{fp,1} + P_{fp,2} + \\dots\n",
    "$$\n",
    "Con valores:\n",
    "$$\n",
    "P_{fp,0} \\approx 0.01,\\quad P_{fp,1}\\approx 0.005,\\quad P_{fp,2} \\approx 0.0025,\\dots\n",
    "$$\n",
    "Para $R=4$ capas:\n",
    "$$\n",
    "P_{fp,\\mathrm{SBF}}\n",
    "\\approx 0.01 + 0.005 + 0.0025 + 0.00125 = 0.01875.\n",
    "$$\n",
    "\n",
    "Alternativamente, usando fórmula geométrica ($r=0.5$):\n",
    "\n",
    "$$\n",
    "P_{fp,\\mathrm{SBF}}\n",
    "≈ \\frac{\\delta_0}{1 - r} = \\frac{0.01}{0.5} = 0.02.\n",
    "$$\n",
    "\n",
    "Ambas estimaciones coinciden en torno a 2%.\n",
    "\n",
    "**Espacio total**  \n",
    "\n",
    "Sumando $m_i$ bits para $i=0..2$:\n",
    "\n",
    "$$\n",
    "m_0 + m_1 + m_2\n",
    "≈ 957k + 2\\,204k + 5\\,000k\n",
    "= 8\\,161\\,000 \\text{ bits}\n",
    "\\approx 1.02\\;\\text{MB}.\n",
    "$$\n",
    "\n",
    "Frente a un filtro clásico dimensionado para $n_{\\max}=4×10^5$ con $\\delta=0.01875$, el SBF ofrece:\n",
    "\n",
    "- **Inserciones dinámicas** sin reconstrucción.  \n",
    "- **Espacio similar** a un solo filtro grande.\n",
    "\n",
    "**Consideraciones de implementación**  \n",
    "\n",
    "1. **Semilla única**: usar un único `seed` y derivar `seed_i = seed + i` facilita reproducibilidad de capas.  \n",
    "2. **Amortización**: la expansión de filtros ocurre a **factor** $g$, por lo que el número de expansiones es $O(\\log_g N)$ para $N$ inserciones totales.  \n",
    "3. **Compactación**: si muchas capas quedan apenas usadas, se podría **fusionar** capas viejas o descartar capas excedentes para ahorrar espacio.  \n",
    "4. **Paralelismo**: inserciones a capas distintas pueden paralelizarse una vez que el filtro principal está lleno.  \n",
    "5. **Monitorización**: exponer `num_filters` y `len(sbf)` permite trackear crecimiento y decidir limpiezas o reconfiguraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d50e75-0bc1-464f-a91d-4d27d03411d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Implementación de varios Bloom filters en Python\n",
    "\"\"\"\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import zlib\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "def fnv1_hash32(key: str) -> int:\n",
    "    \"\"\"Hash FNV-1 de 32 bits (sin signo).\"\"\"\n",
    "    fnv_prime = 0x01000193\n",
    "    hash_ = 0x811c9dc5\n",
    "    for c in key:\n",
    "        # Multiplica por la constante FNV y limita a 32 bits\n",
    "        hash_ = (hash_ * fnv_prime) & 0xFFFFFFFF\n",
    "        # Aplica XOR con el valor ASCII del carácter\n",
    "        hash_ ^= ord(c)\n",
    "    return hash_\n",
    "\n",
    "\n",
    "def murmurhash3_32(key: str, seed: int = 0) -> int:\n",
    "    \"\"\"Implementación de MurmurHash3 x86 32 bits.\"\"\"\n",
    "    data = key.encode('utf-8')\n",
    "    length = len(data)\n",
    "    nblocks = length // 4\n",
    "\n",
    "    h1 = seed & 0xFFFFFFFF\n",
    "    c1 = 0xcc9e2d51\n",
    "    c2 = 0x1b873593\n",
    "\n",
    "    # Cuerpo de bloques de 4 bytes\n",
    "    for block_start in range(0, nblocks * 4, 4):\n",
    "        k1 = (\n",
    "            data[block_start]\n",
    "            | (data[block_start + 1] << 8)\n",
    "            | (data[block_start + 2] << 16)\n",
    "            | (data[block_start + 3] << 24)\n",
    "        )\n",
    "        k1 = (k1 * c1) & 0xFFFFFFFF\n",
    "        k1 = ((k1 << 15) | (k1 >> 17)) & 0xFFFFFFFF\n",
    "        k1 = (k1 * c2) & 0xFFFFFFFF\n",
    "\n",
    "        h1 ^= k1\n",
    "        h1 = ((h1 << 13) | (h1 >> 19)) & 0xFFFFFFFF\n",
    "        h1 = (h1 * 5 + 0xe6546b64) & 0xFFFFFFFF\n",
    "\n",
    "    # Cola (tail)\n",
    "    tail_index = nblocks * 4\n",
    "    tail_size = length & 3\n",
    "    k1 = 0\n",
    "    if tail_size == 3:\n",
    "        k1 ^= data[tail_index + 2] << 16\n",
    "    if tail_size >= 2:\n",
    "        k1 ^= data[tail_index + 1] << 8\n",
    "    if tail_size >= 1:\n",
    "        k1 ^= data[tail_index]\n",
    "        k1 = (k1 * c1) & 0xFFFFFFFF\n",
    "        k1 = ((k1 << 15) | (k1 >> 17)) & 0xFFFFFFFF\n",
    "        k1 = (k1 * c2) & 0xFFFFFFFF\n",
    "        h1 ^= k1\n",
    "\n",
    "    # Finalización\n",
    "    h1 ^= length\n",
    "    h1 &= 0xFFFFFFFF\n",
    "    h1 ^= (h1 >> 16)\n",
    "    h1 = (h1 * 0x85ebca6b) & 0xFFFFFFFF\n",
    "    h1 ^= (h1 >> 13)\n",
    "    h1 = (h1 * 0xc2b2ae35) & 0xFFFFFFFF\n",
    "    h1 ^= (h1 >> 16)\n",
    "\n",
    "    return h1\n",
    "\n",
    "\n",
    "def consistent_stringify(value) -> str:\n",
    "    \"\"\"Serializa de forma determinista a JSON ordenado.\"\"\"\n",
    "    return json.dumps(value, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "\n",
    "class BloomFilter:\n",
    "    \"\"\"\n",
    "    Filtro de Bloom clásico.\n",
    "    max_size: número máximo de inserciones antes de crecer.\n",
    "    max_tolerance: probabilidad de falso positivo deseada.\n",
    "    seed: semilla para las funciones de hash.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int, max_tolerance: float = 0.01, seed: int = None):\n",
    "    # Validación de parámetros\n",
    "        if not isinstance(max_size, int) or max_size <= 0:\n",
    "            raise TypeError(f\"max_size debe ser un entero positivo, recibido: {max_size}\")\n",
    "        tol = float(max_tolerance)\n",
    "        if tol <= 0 or tol >= 1:\n",
    "            raise TypeError(f\"max_tolerance debe cumplir 0 < t < 1, recibido: {max_tolerance}\")\n",
    "        if seed is None:\n",
    "            seed = random.getrandbits(32)\n",
    "        if not isinstance(seed, int):\n",
    "            raise TypeError(f\"seed debe ser un entero, recibido: {seed}\")\n",
    "\n",
    "        # Parámetros internos\n",
    "        self._max_size = max_size\n",
    "        self._seed = seed\n",
    "        ln2 = math.log(2)\n",
    "        # Número de bits y funciones de hash\n",
    "        self._num_bits = math.ceil(-max_size * math.log(tol) / (ln2**2))\n",
    "        self._num_hashes = math.ceil(-math.log(tol) / ln2)\n",
    "        num_bytes = math.ceil(self._num_bits / 8)\n",
    "        # Arreglo de bits\n",
    "        self._bits = bytearray(num_bytes)\n",
    "        self._size = 0\n",
    "\n",
    "    def _bit_coords(self, index: int):\n",
    "        # Devuelve (byte_index, bit_index)\n",
    "        return index // 8, index % 8\n",
    "\n",
    "    def _read_bit(self, index: int) -> int:\n",
    "        b, i = self._bit_coords(index)\n",
    "        return (self._bits[b] >> i) & 1\n",
    "\n",
    "    def _write_bit(self, index: int) -> bool:\n",
    "        b, i = self._bit_coords(index)\n",
    "        mask = 1 << i\n",
    "        antes = self._bits[b]\n",
    "        self._bits[b] |= mask\n",
    "        # Retorna True si cambió de 0 a 1\n",
    "        return antes != self._bits[b]\n",
    "\n",
    "    def _key_positions(self, key: str):\n",
    "        # Genera las posiciones de bits para una clave dada\n",
    "        s = consistent_stringify(key)\n",
    "        h1 = murmurhash3_32(s, self._seed)\n",
    "        h2 = fnv1_hash32(s)\n",
    "        for i in range(self._num_hashes):\n",
    "            yield (h1 + i * h2 + i * i) % self._num_bits\n",
    "\n",
    "    def add(self, value) -> \"BloomFilter\":\n",
    "        \"\"\"Añade un valor al filtro.\"\"\"\n",
    "        key = consistent_stringify(value)\n",
    "        flipped = False\n",
    "        for pos in self._key_positions(key):\n",
    "            if self._write_bit(pos):\n",
    "                flipped = True\n",
    "        if flipped:\n",
    "            self._size += 1\n",
    "        return self\n",
    "\n",
    "    def contains(self, value) -> bool:\n",
    "        \"\"\"Comprueba si un valor podría estar en el filtro.\"\"\"\n",
    "        key = consistent_stringify(value)\n",
    "        return all(self._read_bit(pos) for pos in self._key_positions(key))\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Número de inserciones únicas realizadas.\"\"\"\n",
    "        return self._size\n",
    "\n",
    "    @property\n",
    "    def max_remaining_capacity(self) -> int:\n",
    "        \"\"\"Capacidad restante (inserciones) antes de llenarse.\"\"\"\n",
    "        return max(0, self._max_size - self._size)\n",
    "\n",
    "    def false_positive_probability(self) -> float:\n",
    "        \"\"\"Calcula la probabilidad actual de falso positivo.\"\"\"\n",
    "        k, n, m = self._num_hashes, self._size, self._num_bits\n",
    "        return (1 - math.exp(-k * n / m))**k\n",
    "\n",
    "    def confidence(self) -> float:\n",
    "        \"\"\"Confianza de que un elemento presente sea real (1 - tasa de falso positivo).\"\"\"\n",
    "        return 1 - self.false_positive_probability()\n",
    "\n",
    "\n",
    "class EnsembleBloomFilter(BloomFilter):\n",
    "    \"\"\"\n",
    "    Conjunto de Bloom filtes paralelos para reducir falsos positivos.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size: int, max_tolerance: float = 0.01,\n",
    "                 num_filters: int = 2, seed: int = None):\n",
    "        if num_filters < 1:\n",
    "            raise ValueError(f\"num_filters must be >=1, got {num_filters}\")\n",
    "        super().__init__(max_size, max_tolerance, seed)\n",
    "        self.filters = []\n",
    "        for i in range(num_filters):\n",
    "            seed_i = (self._seed + i) & 0xFFFFFFFF\n",
    "            self.filters.append(BloomFilter(max_size, max_tolerance, seed_i))\n",
    "\n",
    "    @property\n",
    "    def num_filters(self) -> int:\n",
    "        \"\"\"Número de filtros en el ensemble.\"\"\"\n",
    "        return len(self.filters)\n",
    "\n",
    "    def add(self, value) -> \"EnsembleBloomFilter\":\n",
    "        \"\"\"Añade el valor a todos los filtros.\"\"\"\n",
    "        for bf in self.filters:\n",
    "            bf.add(value)\n",
    "        return self\n",
    "\n",
    "    def contains(self, value) -> bool:\n",
    "        \"\"\"Comprueba si todos los filtros contienen el valor.\"\"\"\n",
    "        return all(bf.contains(value) for bf in self.filters)\n",
    "\n",
    "\n",
    "class LayeredBloomFilter(BloomFilter):\n",
    "    \"\"\"\n",
    "    Filtro en capas: cada capa i solo se actualiza si la capa i-1 ya contiene el valor.\n",
    "    Permite aproximar un conteo de repeticiones.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size: int, max_tolerance: float = 0.01,\n",
    "                 num_layers: int = 3, seed: int = None):\n",
    "        super().__init__(max_size, max_tolerance, seed)\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):\n",
    "            seed_i = (self._seed + i) & 0xFFFFFFFF\n",
    "            self.layers.append(BloomFilter(max_size, max_tolerance, seed_i))\n",
    "\n",
    "    def add(self, value) -> \"LayeredBloomFilter\":\n",
    "        \"\"\"Añade valor en capas sucesivas si la anterior ya lo contenía.\"\"\"\n",
    "        for i, bf in enumerate(self.layers):\n",
    "            if i == 0 or self.layers[i-1].contains(value):\n",
    "                bf.add(value)\n",
    "            else:\n",
    "                break\n",
    "        return self\n",
    "\n",
    "    def count(self, value) -> int:\n",
    "        \"\"\"Aproxima el número de veces insertado.\"\"\"\n",
    "        c = 0\n",
    "        for bf in self.layers:\n",
    "            if bf.contains(value):\n",
    "                c += 1\n",
    "            else:\n",
    "                break\n",
    "        return c\n",
    "\n",
    "    def contains(self, value) -> bool:\n",
    "        \"\"\"Comprueba en la primera capa.\"\"\"\n",
    "        return self.layers[0].contains(value)\n",
    "\n",
    "\n",
    "class CompressedBloomFilter(BloomFilter):\n",
    "    \"\"\"\n",
    "    Ajusta k para fill_ratio ≤ 1/3 y permite comprimir/descomprimir con zlib.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size: int, max_tolerance: float = 0.01, seed: int = None):\n",
    "        super().__init__(max_size, max_tolerance, seed)\n",
    "        m, n = self._num_bits, self._max_size\n",
    "        max_k = max(1, math.floor(-(m/n) * math.log(2/3)))\n",
    "        self._num_hashes = min(self._num_hashes, max_k)\n",
    "\n",
    "    def compress(self) -> bytes:\n",
    "        \"\"\"Comprime el arreglo de bits usando zlib.\"\"\"\n",
    "        return zlib.compress(bytes(self._bits))\n",
    "\n",
    "    def decompress(self, data: bytes):\n",
    "        \"\"\"Descomprime y restaura el arreglo de bits.\"\"\"\n",
    "        self._bits = bytearray(zlib.decompress(data))\n",
    "\n",
    "\n",
    "class ScalableBloomFilter:\n",
    "    \"\"\"\n",
    "    Bloom filter escalable: añade capas con mayor capacidad y menor tolerancia.\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_capacity: int, initial_tolerance: float = 0.01,\n",
    "                 growth_factor: float = 2.0, tightening_ratio: float = 0.5, seed: int = None):\n",
    "        if seed is None:\n",
    "            seed = random.getrandbits(32)\n",
    "        self._seed = seed\n",
    "        self.growth_factor = growth_factor\n",
    "        self.tightening_ratio = tightening_ratio\n",
    "        self.filters = []\n",
    "        self.capacities = []\n",
    "        self.tolerances = []\n",
    "        # Crear primer filtro\n",
    "        self._add_filter(initial_capacity, initial_tolerance)\n",
    "\n",
    "    def _add_filter(self, capacity: int, tolerance: float):\n",
    "        idx = len(self.filters)\n",
    "        seed_i = (self._seed + idx) & 0xFFFFFFFF\n",
    "        bf = BloomFilter(capacity, tolerance, seed_i)\n",
    "        self.filters.append(bf)\n",
    "        self.capacities.append(capacity)\n",
    "        self.tolerances.append(tolerance)\n",
    "\n",
    "    def add(self, value) -> \"ScalableBloomFilter\":\n",
    "        \"\"\"Añade valor y crea nueva capa si la actual está llena.\"\"\"\n",
    "        last = self.filters[-1]\n",
    "        if last.size >= last._max_size:\n",
    "            new_cap = int(self.capacities[-1] * self.growth_factor)\n",
    "            new_tol = self.tolerances[-1] * self.tightening_ratio\n",
    "            self._add_filter(new_cap, new_tol)\n",
    "        self.filters[-1].add(value)\n",
    "        return self\n",
    "\n",
    "    def contains(self, value) -> bool:\n",
    "        \"\"\"Comprueba en cualquiera de las capas.\"\"\"\n",
    "        return any(bf.contains(value) for bf in self.filters)\n",
    "\n",
    "    @property\n",
    "    def num_filters(self) -> int:\n",
    "        \"\"\"Número de capas (filtros).\"\"\"\n",
    "        return len(self.filters)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Total aproximado de elementos insertados.\"\"\"\n",
    "        return sum(f.size for f in self.filters)\n",
    "\n",
    "\n",
    "class BloomierFilter:\n",
    "    \"\"\"\n",
    "    Bloomier filter estático: mapea claves a valores sin colisiones.\n",
    "    \"\"\"\n",
    "    def __init__(self, key_to_value: dict, tolerance: float = 0.01, seed: int = None):\n",
    "        if seed is None:\n",
    "            seed = random.getrandbits(32)\n",
    "        self._seed = seed\n",
    "        self._n = len(key_to_value)\n",
    "        ln2 = math.log(2)\n",
    "        self._m = math.ceil(-self._n * math.log(tolerance) / (ln2**2))\n",
    "        self._k = math.ceil(-math.log(tolerance) / ln2)\n",
    "        self.T = [0] * self._m\n",
    "\n",
    "        edges = []\n",
    "        for key, value in key_to_value.items():\n",
    "            posiciones = list(self._key_positions(key))\n",
    "            edges.append({'positions': posiciones, 'value': value})\n",
    "        self._build(edges)\n",
    "\n",
    "    def _key_positions(self, key: str):\n",
    "        \"\"\"Genera posiciones para Bloomier (similar a BloomFilter).\"\"\"\n",
    "        s = consistent_stringify(key)\n",
    "        h1 = murmurhash3_32(s, self._seed)\n",
    "        h2 = fnv1_hash32(s)\n",
    "        for i in range(self._k):\n",
    "            yield (h1 + i * h2 + i * i) % self._m\n",
    "\n",
    "    def _build(self, edges: list):\n",
    "        \"\"\"Construye la tabla T usando un grago acíclico.\"\"\"\n",
    "        incidence = defaultdict(set)\n",
    "        for idx, e in enumerate(edges):\n",
    "            for p in e['positions']:\n",
    "                incidence[p].add(idx)\n",
    "        queue = deque([p for p, idxs in incidence.items() if len(idxs) == 1])\n",
    "        removed = []\n",
    "        while queue:\n",
    "            p = queue.popleft()\n",
    "            idxs = incidence[p]\n",
    "            if not idxs:\n",
    "                continue\n",
    "            idx = idxs.pop()\n",
    "            e = edges[idx]\n",
    "            xor_sum = 0\n",
    "            for pp in e['positions']:\n",
    "                if pp != p:\n",
    "                    xor_sum ^= self.T[pp]\n",
    "            self.T[p] = xor_sum ^ e['value']\n",
    "            removed.append(idx)\n",
    "            for pp in e['positions']:\n",
    "                incidence[pp].discard(idx)\n",
    "                if len(incidence[pp]) == 1:\n",
    "                    queue.append(pp)\n",
    "        if len(removed) != len(edges):\n",
    "            raise RuntimeError(\"Ciclo detectado; reintentar con otra semilla\")\n",
    "\n",
    "    def lookup(self, key: str) -> int:\n",
    "        \"\"\"Recupera el valor asociado a una clave.\"\"\"\n",
    "        resultado = 0\n",
    "        for pos in self._key_positions(key):\n",
    "            resultado ^= self.T[pos]\n",
    "        return resultado\n",
    "\n",
    "\n",
    "# Ejemplos de uso y pruebas\n",
    "bf = BloomFilter(1000, 0.01, seed=42)\n",
    "bf.add(\"apple\")\n",
    "print(\"BloomFilter: apple?\", bf.contains(\"apple\"))\n",
    "\n",
    "ebf = EnsembleBloomFilter(1000, 0.01, num_filters=3, seed=123)\n",
    "ebf.add(\"key1\")\n",
    "print(\"EnsembleBloomFilter (3 filtros): key1?\", ebf.contains(\"key1\"), \"num_filters=\", ebf.num_filters)\n",
    "\n",
    "lbf = LayeredBloomFilter(1000, 0.01, num_layers=4, seed=999)\n",
    "for _ in range(3):\n",
    "    lbf.add(\"counter\")\n",
    "    print(\"LayeredBloomFilter: count(counter)=\", lbf.count(\"counter\"))\n",
    "\n",
    "data = {\"apple\": 5, \"banana\": 7, \"cherry\": 3}\n",
    "bfier = BloomierFilter(data, tolerance=0.01)\n",
    "print(\"BloomierFilter: banana ->\", bfier.lookup(\"banana\"))\n",
    "\n",
    "cbf = CompressedBloomFilter(1000, 0.01, seed=42)\n",
    "cbf.add(\"item\")\n",
    "compressed = cbf.compress()\n",
    "print(\"Tamaño comprimido:\", len(compressed))\n",
    "cbf2 = CompressedBloomFilter(1000, 0.01, seed=42)\n",
    "cbf2.decompress(compressed)\n",
    "print(\"item?\", cbf2.contains(\"item\"))\n",
    "\n",
    "sbf = ScalableBloomFilter(initial_capacity=5, initial_tolerance=0.1,\n",
    "                          growth_factor=2.0, tightening_ratio=0.5, seed=1)\n",
    "for i in range(20):\n",
    "    sbf.add(f\"item{i}\")\n",
    "print(\"Total filtros:\", sbf.num_filters)\n",
    "print(\"Total elementos (aprox):\", len(sbf))\n",
    "print(\"Contains item 5?\", sbf.contains(\"item5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299c63f-3870-46f1-b6fe-e430b23b02f5",
   "metadata": {},
   "source": [
    "### **Ejercicios**\n",
    "\n",
    "#### **1. Variantes de Bloom filter**\n",
    "\n",
    "\n",
    "1. **Derivación del óptimo de $k$**\n",
    "   Demuestra que para un Bloom filter clásico con $m$ bits y $n$ elementos, la probabilidad de falso positivo  \n",
    "   $$\n",
    "   P_{fp}(k) = \\Bigl(1 - e^{-k\\,n/m}\\Bigr)^k\n",
    "   $$\n",
    "   se minimiza cuando  \n",
    "   $$\n",
    "   k^* = \\frac{m}{n}\\ln 2.\n",
    "   $$  \n",
    "   (Pista: derivar respecto a $k$ y buscar raíces de $\\partial P_{fp}/\\partial k = 0$.)\n",
    "\n",
    "2. **Tasa efectiva en ensemble Bloom filter**  \n",
    "   Si cada subfiltro tiene $P_{fp}=\\delta$ y usamos $r$ filtros independientes en paralelo, demuestra que  \n",
    "   $$\n",
    "   P_{fp}^{ensemble} = \\delta^r.\n",
    "   $$  \n",
    "   ¿Bajo qué hipótesis de independencia es esta fórmula exacta, y cuándo solo aproximada?\n",
    "\n",
    "3. **Fill–ratio y compresión en compressed Bloom filter**  \n",
    "   A partir de la expresión  \n",
    "   $$\n",
    "   \\phi = 1 - e^{-k\\,n/m},\n",
    "   $$\n",
    "   calcula el valor máximo de $k$ que garantiza $\\phi \\le 1/3$. Justifica por qué un fill–ratio cercano a 1/3 es óptimo para compresión con zlib.\n",
    "\n",
    "4. **Escalabilidad en scalable Bloom filter**  \n",
    "   Sea $\\delta_0$ la tolerancia inicial y $r$ la razón de \"tightening\" (< 1). Muestra que la probabilidad global de falso positivo satisface  \n",
    "   $$\n",
    "   P_{fp,\\,\\mathrm{SBF}} \\;\\le\\; \\frac{\\delta_0}{1 - r}.\n",
    "   $$  \n",
    "   ¿Cómo elegir $r$ para que $P_{fp,\\mathrm{SBF}}\\le \\Delta$ dado un umbral $\\Delta$?\n",
    "\n",
    "\n",
    "5. **Partitioned Bloom filter**  \n",
    "   - *Teoría*: Analiza y compara la probabilidad de falso positivo de un filtro \"particionado\" (un sub‐arreglo de bits por cada hash) frente al clásico que usa un único arreglo.  \n",
    "   - *Implementación*: Crea `class PartitionedBloomFilter` que, en vez de recorrer todos los hashes sobre el mismo arreglo, reserve un bloque de $m/k$ bits para cada función de hash.\n",
    "\n",
    "6. **Cuckoo Filter**  \n",
    "   - *Teoría*: Estudia cómo el Cuckoo Filter usa \"fingerprints\" y relocaciones para permitir borrados sin contar. Deriva la carga máxima $\\alpha$ antes de que la probabilidad de fallo al insertar exceda 1%.  \n",
    "   - *Implementación*: Escribe `class CuckooFilter` con buckets de tamaño 4, fingerprint de $f$ bits, y métodos `add`, `contains`, `delete`. Mide la carga alcanzable y la tasa de re‐ubicaciones.\n",
    "\n",
    "7. **Counting Bloom filter con saturación**  \n",
    "   - *Teoría*: Modela el error de conteo cuando los contadores (de $b$ bits) se saturan en su valor máximo. ¿Cómo afecta esto la estimación de conteos frecuentes?  \n",
    "   - *Implementación*: Modifica el `BloomFilter` para usar `bytearray` de contadores de 4 bits (nibble‐array). Añade un flag para saturar en 15 y prueba con flujos de recuento intensivos.\n",
    "\n",
    "8. **Spectral Bloom filter**  \n",
    "   - *Teoría*: Se introduce un parámetro de \"multiplicidad\"aproximado. Explica cómo usando contadores por posición y un \"sketch\" en cascada se aproxima la frecuencia de cada key.  \n",
    "   - *Implementación*: Combina tu `CountMinSketch` (ver abajo el ejercicio de implementación) con un Bloom filter para descartar rápido sin conteo, y si pasa, consulta el sketch.\n",
    "\n",
    "\n",
    "9. **Count–Sketch**  \n",
    "   - *Teoría*: A diferencia de Count–Min, usa transformaciones con signos aleatorios para dar estimadores desenmascarados de desviación. Deriva la cota de error $\\pm \\epsilon \\|f\\|_2$.  \n",
    "   - *Implementación*: Crea `class CountSketch` donde cada tabla mantiene valores incrementados o decrementados por un hash de signo; ensaya estimaciones y compara con Count–Min.\n",
    "\n",
    "10. **Top‑k con heavy hitters**\n",
    "   - *Teoría*: Estudia el algoritmo Misra–Gries o \"SpaceSaving\" para encontrar los elementos más frecuentes con $\\epsilon$-error.  \n",
    "   - *Implementación*: Implementa `class SpaceSaving` que mantenga $k$ contadores y actualice en streaming; compara los resultados con un conteo exacto en un dataset de logs.\n",
    "\n",
    "11. **Frequent directions** (sketch de matrices)  \n",
    "   - *Teoría*: Generaliza los sketches de vectores a matrices para aproximar la SVD. Describe la garantía de aproximación de Frobenius.  \n",
    "   - *Implementación*: Con NumPy, implementa el algoritmo de Frequent Directions para un flujo de filas de una matriz grande y compara la traza de la covarianza exacta vs. sketch.\n",
    "\n",
    "12. **LogLog vs HyperLogLog**  \n",
    "   - *Teoría*: Deriva por qué LogLog tiene error aproximado $1.30/\\sqrt{m}$ y HyperLogLog baja a $1.04/\\sqrt{m}$.  \n",
    "   - *Implementación*: Escribe `class LogLog` (sin corrección de bias) y compara su estimación con la de tu `HyperLogLog` en datasets de distintas cardinalidades.\n",
    "\n",
    "13. **Adaptive counting**  \n",
    "   - *Teoría*: Explica cómo combinar Flajolet–Martin (bitmaps) con Flajolet–Martin \"correcciones\" cuando se detecta saturación.  \n",
    "   - *Implementación*: Empaqueta un `HyperLogLog` que, si detecta que sus registros están muy bajos (sesgo en bajo cardinal), cambia automáticamente a un bitmap exacto.\n",
    "\n",
    "14. **Fusión de sketches**  \n",
    "   - *Teoría*: Para estructuras lineales (CMS, HLL), muestra cómo combinar dos instancias en paralelo (suma de tablas o máximos por registro).\n",
    "   -  *Implementación*: Simula dos nodos generando un `CountMinSketch` y un `HyperLogLog` y luego fusiónalos en un único sketch maestro.\n",
    "\n",
    "14. **Consistencia eventual**  \n",
    "   - *Teoría*: Discute el efecto de la latencia y duplicados en streams distribuidos. ¿Puede un Bloom filter distribuido saturarse más rápido?  \n",
    "   - *Implementación*: Emula un sistema con 3 réplicas de un `ScalableBloomFilter`, cada una recibiendo eventos parcialmente solapados, y mide la divergencia de `contains` entre réplicas.\n",
    "\n",
    "15. **Benchmark global**  \n",
    "   Diseña un benchmark que compare **espacio**, **throughput** (inserciones/segundo), y **error** (falsos positivos o error de conteo) para todas las estructuras: Bloom, Cuckoo, Count–Min, CountSketch, HyperLogLog, AMS.  \n",
    "16. **Informe de trade‑offs**  \n",
    "   Basado en resultados empíricos, redacta un informe que proponga la \"mejor\" estructura para cada caso de uso:\n",
    "   - Alta velocidad, pocos falsos positivos.  \n",
    "   - Conteo preciso de heavy hitters.  \n",
    "   - Estimación de cardinalidad de grandes volúmenes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd71a8-1132-471b-8ab5-1de58dd6a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e33030-4875-435b-8831-67ec3a455976",
   "metadata": {},
   "source": [
    "#### **Más ejercicios**\n",
    "\n",
    "#### 1. Implementación de invariantes\n",
    "\n",
    "Para cada variante de Bloom filter del código de referencia, realiza lo siguiente:\n",
    "\n",
    "1. **Benchmark de velocidad y memoria**  \n",
    "   - Inserta $n=10^6$ enteros sucesivos.  \n",
    "   - Mide tasa de inserciones (`add`) y consultas (`contains`) por segundo.  \n",
    "   - Mide uso de memoria tras construir el filtro (por ejemplo, `sys.getsizeof(bf._bits)`).  \n",
    "\n",
    "2. **Validación empírica de $P_{fp}$**  \n",
    "   - Genera 100 000 claves no insertadas y consulta `contains`.  \n",
    "   - Calcula la proporción de `True` (falsos positivos) y compárala con la tasa teórica.\n",
    "\n",
    "3. **Implementa una variante \"Hybrid\"**  \n",
    "   Combina Ensemble + Compressed: crea `class HybridBloomFilter(EnsembleBloomFilter, CompressedBloomFilter)`, de modo que cada subfiltro ajusta $k$ para compresión y luego se ensambla en paralelo.\n",
    "\n",
    "   ```python\n",
    "   class HybridBloomFilter(CompressedBloomFilter, EnsembleBloomFilter):\n",
    "       def __init__(self, max_size, max_tolerance, num_filters, seed=None):\n",
    "           CompressedBloomFilter.__init__(self, max_size, max_tolerance, seed)\n",
    "           # reusar lógica de EnsembleBloomFilter para crear 'num_filters' instancias\n",
    "           self.filters = []\n",
    "           for i in range(num_filters):\n",
    "               seed_i = (self._seed + i) & 0xFFFFFFFF\n",
    "               self.filters.append(CompressedBloomFilter(max_size, max_tolerance, seed_i))\n",
    "   ```\n",
    "\n",
    "   Comprueba que hereda correctamente métodos `add`, `contains` y añade `compress`/`decompress` sobre el conjunto.\n",
    "\n",
    "\n",
    "#### 2. Quotient filter\n",
    "\n",
    "Un **Quotient filter (QF)** es una alternativa a Bloom que soporta borrados y suele ofrecer mejor compresión interna.  \n",
    "\n",
    "1. **Partición de hash**  \n",
    "   En un QF con $m = 2^q$ ranuras, y un hash de 32 bits $h(x)$, se definen:\n",
    "   $$\n",
    "   \\text{quotient } = h(x) \\gg (32 - q), \n",
    "   \\quad\n",
    "   \\text{remainder } = h(x)\\,\\&\\,(2^{32-q} - 1).\n",
    "   $$\n",
    "   Demuestra cómo esto induce colisiones y cómo el QF almacena \"runs\" de restos contiguos.\n",
    "\n",
    "2. **Carga máxima**  \n",
    "   Analiza la probabilidad de overflow (cuando una \"cluster\" excede cierto tamaño) y deriva un bound para la carga $\\alpha = n/m$ que garantice un tiempo promedio constante en consultas.\n",
    "\n",
    "3. Implementa `class QuotientFilter` con:\n",
    "\n",
    "- Atributos:\n",
    "  ```python\n",
    "  self.q = q  # bits de quotient\n",
    "  self.r = 32 - q  # bits de remainder\n",
    "  self.table = [None] * (2**q)  # cada entrada: (rem: int, is_occupied: bool, is_continuation: bool, is_shifted: bool)\n",
    "  ```\n",
    "- Métodos:\n",
    "  ```python\n",
    "  def add(self, value):\n",
    "      h = murmurhash3_32(value, self.seed)\n",
    "      qv = h >> self.r\n",
    "      rv = h & ((1 << self.r) - 1)\n",
    "      # inserción con desplazamientos y flags\n",
    "  def contains(self, value) -> bool:\n",
    "      # búsqueda en run correspondiente\n",
    "  def delete(self, value):\n",
    "      # ajuste de flags y compaction\n",
    "  ```\n",
    "- **Prueba**: inserta 50 000 cadenas aleatorias y mide carga, tasa de falsos positivos (sin borrado), y capacidad de borrado.\n",
    "\n",
    "#### 3. Count–Min sketch\n",
    "\n",
    "El **Count–Min Sketch** estima frecuencias de items en un stream con espacio $O(\\epsilon^{-1}\\log 1/\\delta)$.\n",
    "\n",
    "\n",
    "1. **Cota del error**  \n",
    "   Demuestra que, dado un CMS con parámetros $\\epsilon$ y $\\delta$ (ancho $w = \\lceil e/\\epsilon\\rceil$ y profundidad $d = \\lceil\\ln 1/\\delta\\rceil$), la estimación $\\tilde{f}(x)$ satisface  \n",
    "   $$\n",
    "   f(x) \\;\\le\\; \\tilde{f}(x) \\;\\le\\; f(x) + \\epsilon N\n",
    "   $$\n",
    "   con probabilidad al menos $1 - \\delta$, donde $N$ es número total de inserciones.\n",
    "\n",
    "2. **Trade‑off espacio vs error**  \n",
    "   Grafica cómo varía el error máximo esperado $\\epsilon N$ versus el espacio $w \\times d$ para $N=10^6$, $\\epsilon\\in[10^{-3},10^{-1}]$, $\\delta=0.01$.\n",
    "\n",
    "3. Crea `class CountMinSketch`:\n",
    "\n",
    "```python\n",
    "class CountMinSketch:\n",
    "    def __init__(self, epsilon: float, delta: float, seed: int=None):\n",
    "        self.w = math.ceil(math.e / epsilon)\n",
    "        self.d = math.ceil(math.log(1/delta))\n",
    "        self.tables = [[0]*self.w for _ in range(self.d)]\n",
    "        self.hash_seeds = [random.getrandbits(32) for _ in range(self.d)]\n",
    "    def add(self, value, count: int=1):\n",
    "        for i, s in enumerate(self.hash_seeds):\n",
    "            idx = murmurhash3_32(value, s) % self.w\n",
    "            self.tables[i][idx] += count\n",
    "    def estimate(self, value) -> int:\n",
    "        return min(self.tables[i][murmurhash3_32(value, s) % self.w]\n",
    "                   for i, s in enumerate(self.hash_seeds))\n",
    "```\n",
    "\n",
    "**Pruebas**:\n",
    "\n",
    "- Genera un stream con 10 categorías, distribuidas según Zipf (parámetro 1.1).  \n",
    "- Mide error absoluto $\\lvert \\tilde{f}(x)-f(x)\\rvert$ promedio para cada categoría.\n",
    "\n",
    "\n",
    "#### 4. Sketches lineales (AMS, TICHA)\n",
    "\n",
    "Además del Count–Min, existen sketches basados en proyecciones aleatorias (Alon–Matias–Szegedy) para estimar momentos.\n",
    "\n",
    "\n",
    "1. **Estimación del segundo momento**  \n",
    "   Sea $F_2 = \\sum_x f(x)^2$. Demuestra que el estimador AMS\n",
    "   $$\n",
    "   Z = \\left(\\sum_{i=1}^N s(i)\\right)^2\n",
    "   $$\n",
    "   con $s(i)=\\sigma_j$ donde $\\sigma_j\\in\\{-1,+1\\}$ aleatorio por key, es un estimador imparcial de $F_2$.\n",
    "\n",
    "\n",
    "2. Implementa `class AMSSketch` con:\n",
    "\n",
    "```python\n",
    "class AMSSketch:\n",
    "    def __init__(self, num_repetitions: int=10, seed: int=None):\n",
    "        self.counters = [0]*num_repetitions\n",
    "        self.signs = [[random.choice([-1,1]) for _ in range(mod)] for _ in range(num_repetitions)]\n",
    "    def add(self, value):\n",
    "        h = murmurhash3_32(value, self.seed)\n",
    "        for i in range(len(self.counters)):\n",
    "            sign = 1 if murmurhash3_32(value, self.signs[i]) & 1 else -1\n",
    "            self.counters[i] += sign\n",
    "    def estimate_F2(self) -> float:\n",
    "        return sum(c*c for c in self.counters) / len(self.counters)\n",
    "```\n",
    "\n",
    "Valida con un vector de frecuencias conocido, comparando con el valor real $F_2$.\n",
    "\n",
    "\n",
    "#### 5. HyperLogLog\n",
    "\n",
    "El **HyperLogLog** es una mejora de LogLog para estimar número de elementos distintos (cardinalidad) con sólo $O(m)$ espacio.\n",
    "\n",
    "1. **Corrección de sesgo**  \n",
    "   Explica el uso de la constante $\\alpha_m$ y la media armónica en  \n",
    "   $$\n",
    "   E = \\alpha_m \\; m^2 \\; \\Bigl(\\sum_{j=1}^m 2^{-M[j]}\\Bigr)^{-1}.\n",
    "   $$\n",
    "\n",
    "2. **Errores y tamaño**  \n",
    "   Muestra que el error relativo estándar es aproximadamente $1.04/\\sqrt{m}$.\n",
    "\n",
    "3. Crea `class HyperLogLog`:\n",
    "\n",
    "```python\n",
    "class HyperLogLog:\n",
    "    def __init__(self, p: int=14, seed: int=None):\n",
    "        self.m = 1 << p\n",
    "        self.registers = [0]*self.m\n",
    "        self.p = p\n",
    "        self.seed = seed or random.getrandbits(32)\n",
    "        self.alpha = self._compute_alpha(p)\n",
    "    def _compute_alpha(self, p):\n",
    "        if p == 14: return 0.673\n",
    "        # valores predefinidos\n",
    "    def add(self, value):\n",
    "        x = murmurhash3_32(value, self.seed)\n",
    "        idx = x >> (32-self.p)\n",
    "        w = (x << self.p) & 0xFFFFFFFF\n",
    "        rho = self._rank(w, 32-self.p)\n",
    "        self.registers[idx] = max(self.registers[idx], rho)\n",
    "    def _rank(self, x, bits):\n",
    "        return (x & -x).bit_length()  # posición del primer 1\n",
    "    def count(self):\n",
    "        Z = 1.0 / sum(2.0**(-r) for r in self.registers)\n",
    "        E = self.alpha * self.m * self.m * Z\n",
    "        # correcciones para valores bajos/altos\n",
    "        return E\n",
    "```\n",
    "\n",
    "**Prueba**: inserta $10^6$ claves únicas y mide estimación, compara con verdadero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2863bb-1238-4711-96c6-8e0c2f72a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
