{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTe_gH32tpQH"
   },
   "source": [
    "### **Clustering**\n",
    "\n",
    "El clustering es la tarea de dividir el conjunto de datos en grupos, llamados *clusters*. El objetivo es agrupar los datos de tal manera que:\n",
    "\n",
    "* Los puntos dentro de un mismo clúster sean lo más similares posible (alta cohesión interna).\n",
    "* Los puntos de diferentes clústeres sean lo más distintos posible (alta separación entre clústeres).\n",
    "\n",
    "De forma similar a los algoritmos de clasificación, los algoritmos de clustering asignan (o “predicen”) un número o etiqueta a cada punto de datos, indicando a qué grupo pertenece.\n",
    "\n",
    "#### **Tipos de algoritmos de clustering**\n",
    "\n",
    "1. **Particionales**\n",
    "\n",
    "   * **k-Means**:\n",
    "\n",
    "     * Inicializa k centroides aleatorios.\n",
    "     * Asigna cada punto al centroide más cercano (por ejemplo, por distancia Euclídea).\n",
    "     * Recalcula los centroides como la media de los puntos asignados.\n",
    "     * Repite hasta converger o alcanzar un número máximo de iteraciones.\n",
    "   * **k-Medoids (PAM)**:\n",
    "\n",
    "     * Similar a k-Means, pero cada clúster está representado por un punto real (medoide) en lugar de la media, lo que es más robusto ante valores atípicos.\n",
    "\n",
    "2. **Jerárquicos**\n",
    "\n",
    "   * **Aglomerativo (bottom-up)**:\n",
    "\n",
    "     * Comienza con cada punto como un clúster separado.\n",
    "     * En cada paso, fusiona los dos clústeres más similares, según un criterio de enlace (simple, completo, promedio).\n",
    "     * Genera un dendrograma que se puede \"cortar\" para obtener el número deseado de clústeres.\n",
    "   * **Divisivo (top-down)**:\n",
    "\n",
    "     * Parte de un solo clúster que contiene todos los puntos.\n",
    "     * En cada paso, divide un clúster en dos, típicamente usando k-Means con k=2.\n",
    "\n",
    "3. **Basados en densidad**\n",
    "\n",
    "   * **DBSCAN**:\n",
    "\n",
    "     * Agrupa puntos densamente conectados (vecinos dentro de un radio ε).\n",
    "     * Los puntos con al menos *minPts* vecinos son *puntos centrales*; otros pueden ser *puntos frontera* o *ruido*.\n",
    "   * **OPTICS**:\n",
    "\n",
    "     * Similar a DBSCAN, pero capaz de detectar clústeres de densidades variables sin fijar un radio global.\n",
    "\n",
    "4. **Basados en modelos**\n",
    "\n",
    "   * **EM (Expectation–Maximization) para mezclas Gaussianas**:\n",
    "\n",
    "     * Asume que los datos provienen de la combinación de varias distribuciones gaussianas.\n",
    "     * Alterna entre asignar probabilidades de pertenencia (*E-step*) y ajustar parámetros de las Gaussianas (*M-step*).\n",
    "\n",
    "#### **Métricas de distancia y similitud**\n",
    "\n",
    "* **Distancia Euclídea**: Ideal para datos continuos en espacio ℝⁿ.\n",
    "* **Distancia de Manhattan**: Más robusta ante dimensiones con escalas diferentes.\n",
    "* **Distancia de Mahalanobis**: Tiene en cuenta la covarianza y escala de las variables.\n",
    "* **Coeficiente de correlación de Pearson**: Para medir similitud de forma (series temporales, vectores normalizados).\n",
    "* **Distancias específicas** (Jaccard, Hamming) para datos binarios o categóricos.\n",
    "\n",
    "#### **Evaluación de resultados**\n",
    "\n",
    "* **Índice de Silhouette**:\n",
    "\n",
    "  $$\n",
    "  s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n",
    "  $$\n",
    "\n",
    "  donde *a(i)* es la distancia media del punto *i* a los demás de su clúster, y *b(i)* es la distancia media al clúster más cercano. Valores cercanos a 1 indican buena asignación.\n",
    "\n",
    "* **Índice de Dunn**:\n",
    "  Relaciona la separación mínima entre clústeres con el diámetro máximo dentro de clústeres; valores altos son mejores.\n",
    "\n",
    "* **Coeficiente de correlación de Rand** (o *Adjusted Rand Index*):\n",
    "  Compara la coincidencia entre dos particiones (por ejemplo, clustering vs. etiquetas verdaderas).\n",
    "\n",
    "* **Validación interna vs. externa**:\n",
    "\n",
    "  * *Interna*: sólo usa los propios datos y distancias (Silhouette, Dunn).\n",
    "  * *Externa*: compara contra una clasificación “verdadera” conocida (Rand, F-measure).\n",
    "\n",
    "#### **Estrategias de escalado y paralelización**\n",
    "\n",
    "* **Map-Reduce para k-Means**\n",
    "\n",
    "  * Mapa: cada nodo asigna puntos a centroides y calcula sumas parciales.\n",
    "  * Reduce: agrega sumas y cuentas para recalcular centroides globales.\n",
    "\n",
    "* **Mini-batch k-Means**\n",
    "\n",
    "  * Procesa pequeños lotes de datos para actualizar centroides, reduciendo memoria y acelerando convergencia en datasets muy grandes.\n",
    "\n",
    "* **Aceleración con GPUs**\n",
    "\n",
    "  * Implementaciones que aprovechan paralelismo masivo para calcular distancias en grandes volúmenes de datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UrwJP5btpQK"
   },
   "source": [
    "### **K-means clustering**\n",
    "\n",
    "El algoritmo de **K-means** es uno de los métodos de clustering más sencillos y utilizados. Su objetivo es encontrar **centroides** que representen regiones de los datos. El procedimiento alterna entre dos pasos:\n",
    "\n",
    "1. Asignar cada punto de datos al centroide más cercano.  \n",
    "2. Recalcular cada centroide como la media de los puntos que se le han asignado.\n",
    "\n",
    "El algoritmo termina cuando las asignaciones de puntos a clústeres no cambian de una iteración a la siguiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v43tRpPmtpQO"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciones auxiliares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LywqCBtMtpQi"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import  colorConverter, ListedColormap\n",
    "cm3 = ListedColormap(['#0000aa', '#ff2020', '#50ff50'])\n",
    "cm2 = ListedColormap(['#0000aa', '#ff2020'])\n",
    "\n",
    "def dibuja_dispersion_discreta(x1, x2, y=None, marcadores=None, s=10, ax=None,\n",
    "                     etiquetas=None, relleno=.2, alfa=1, c=None, anchobordemarcador=None):\n",
    "    \"\"\"Adaptacion de la funcion matplotlib.pyplot.scatter a dibujar clases o  clusters\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if y is None:\n",
    "        y = np.zeros(len(x1))\n",
    "\n",
    "    unico_y = np.unique(y)\n",
    "\n",
    "    if marcadores is None:\n",
    "        marcadores= ['o', '^', 'v', 'D', 's', '*', 'p', 'h', 'H', '8', '<', '>'] * 10\n",
    "\n",
    "    if len(marcadores) == 1:\n",
    "        marcadores = marcadores * len(unico_y)\n",
    "\n",
    "    if etiquetas is None:\n",
    "        etiquetas = unico_y\n",
    "\n",
    "    # lineas en el sentido de matplotlib\n",
    "    lineas = []\n",
    "\n",
    "    actual_cycler = mpl.rcParams['axes.prop_cycle']\n",
    "\n",
    "    for i, (yy, cycle) in enumerate(zip(unico_y, actual_cycler())):\n",
    "        mascara = y == yy\n",
    "        # if c is ninguno, use color cycle\n",
    "        if c is None:\n",
    "            color = cycle['color']\n",
    "        elif len(c) > 1:\n",
    "            color = c[i]\n",
    "        else:\n",
    "            color = c\n",
    "        # bordes claros para marcadores oscuros\n",
    "        if np.mean(colorConverter.to_rgb(color)) < .4:\n",
    "            colorbordemarcador = \"grey\"\n",
    "        else:\n",
    "            colorbordemarcador = \"black\"\n",
    "\n",
    "        lineas.append(ax.plot(x1[mascara], x2[mascara], marcadores[i], markersize=s,\n",
    "                             label=etiquetas[i], alpha=alfa, c=color,\n",
    "                             markeredgewidth=anchobordemarcador,\n",
    "                             markeredgecolor=colorbordemarcador)[0])\n",
    "\n",
    "    if relleno != 0:\n",
    "        pad1 = x1.std() * relleno\n",
    "        pad2 = x2.std() * relleno\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "        ax.set_xlim(min(x1.min() - pad1, xlim[0]), max(x1.max() + pad1, xlim[1]))\n",
    "        ax.set_ylim(min(x2.min() - pad2, ylim[0]), max(x2.max() + pad2, ylim[1]))\n",
    "\n",
    "    return lineas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PR3REmXftpQx"
   },
   "outputs": [],
   "source": [
    "def dibuja_clasificacion_2d(clasificador, X, relleno=False, ax=None, eps=None, alfa=1, cm=cm3):\n",
    "    # multiclase\n",
    "    if eps is None:\n",
    "        eps = X.std() / 2.\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
    "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n",
    "    xx = np.linspace(x_min, x_max, 1000)\n",
    "    yy = np.linspace(y_min, y_max, 1000)\n",
    "\n",
    "    X1, X2 = np.meshgrid(xx, yy)\n",
    "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "    valores_decision = clasificador.predict(X_grid)\n",
    "    ax.imshow(valores_decision.reshape(X1.shape), extent=(x_min, x_max,\n",
    "                        y_min, y_max), aspect='auto', origin='lower', alpha=alfa, cmap=cm)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsnwFWeGtpQ-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "def dibuja_algoritmos_kmeans():\n",
    "    X, y = make_blobs(random_state=1)\n",
    "    with mpl.rc_context(rc={'axes.prop_cycle': cycler('color', ['#0000aa',\n",
    "                                                                '#ff2020',\n",
    "                                                                '#50ff50'])}):\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(10, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "        axes = axes.ravel()\n",
    "        axes[0].set_title(\"Entrada de datos\")\n",
    "        dibuja_dispersion_discreta(X[:, 0], X[:, 1], ax=axes[0], marcadores=['o'], c='w')\n",
    "        axes[1].set_title(\"Inicializacion\")\n",
    "        inicio= X[:3, :]\n",
    "        dibuja_dispersion_discreta(X[:, 0], X[:, 1], ax=axes[1], marcadores=['o'], c='w')\n",
    "        dibuja_dispersion_discreta(inicio[:, 0], inicio[:, 1], [0, 1, 2], ax=axes[1], marcadores=['^'], anchobordemarcador=2)\n",
    "        axes[2].set_title(\"Asignamos puntos (1)\")\n",
    "        km = KMeans(n_clusters=3, init=inicio, max_iter=1, n_init=1).fit(X)\n",
    "        centros = km.cluster_centers_\n",
    "        # necesitamos calcular las etiquetas. \n",
    "        etiquetas = np.argmin(pairwise_distances(inicio, X), axis=0)\n",
    "        dibuja_dispersion_discreta(X[:, 0], X[:, 1], etiquetas, marcadores=['o'],\n",
    "                         ax=axes[2])\n",
    "        dibuja_dispersion_discreta(inicio[:, 0], inicio[:, 1], [0, 1, 2], ax=axes[2], marcadores=['^'], anchobordemarcador=2)\n",
    "\n",
    "        axes[3].set_title(\"Recalculamos los centros (1)\")\n",
    "        dibuja_dispersion_discreta(X[:, 0], X[:, 1], etiquetas, marcadores=['o'],\n",
    "                         ax=axes[3])\n",
    "        dibuja_dispersion_discreta(centros[:, 0], centros[:, 1], [0, 1, 2], ax=axes[3], marcadores=['^'], anchobordemarcador=2)\n",
    "\n",
    "        axes[4].set_title(\"Reasignar puntos (2)\")\n",
    "        km = KMeans(n_clusters=3, init=inicio, max_iter=1, n_init=1).fit(X)\n",
    "        etiquetas = km.labels_\n",
    "        dibuja_dispersion_discreta(X[:, 0], X[:, 1], etiquetas, marcadores=['o'],\n",
    "                         ax=axes[4])\n",
    "        dibuja_dispersion_discreta(centros[:, 0], centros[:, 1], [0, 1, 2], ax=axes[4], marcadores=['^'], anchobordemarcador=2)\n",
    "\n",
    "        km = KMeans(n_clusters=3, init=inicio, max_iter=2, n_init=1).fit(X)\n",
    "        axes[5].set_title(\"Recalculamos el centro (2)\")\n",
    "        centros = km.cluster_centers_\n",
    "        dibuja_dispersion_discreta(X[:, 0], X[:, 1], etiquetas, marcadores=['o'],\n",
    "                         ax=axes[5])\n",
    "        dibuja_dispersion_discreta(centros[:, 0], centros[:, 1], [0, 1, 2], ax=axes[5], marcadores=['^'], anchobordemarcador=2)\n",
    "\n",
    "        axes[6].set_title(\"Reasignamos puntos (3)\")\n",
    "        etiquetas = km.labels_\n",
    "        dibuja_dispersion_discreta(X[:, 0], X[:, 1], etiquetas, marcadores=['o'],\n",
    "                         ax=axes[6])\n",
    "        marcadores= dibuja_dispersion_discreta(centros[:, 0], centros[:, 1], [0, 1, 2], ax=axes[6], marcadores=['^'], anchobordemarcador=2)\n",
    "\n",
    "        axes[7].set_title(\"Recalculamos centros (3)\")\n",
    "        km = KMeans(n_clusters=3, init=inicio, max_iter=3, n_init=1).fit(X)\n",
    "        centros = km.cluster_centers_\n",
    "        dibuja_dispersion_discreta(X[:, 0], X[:, 1], etiquetas, marcadores=['o'],\n",
    "                         ax=axes[7])\n",
    "        dibuja_dispersion_discreta(centros[:, 0], centros[:, 1], [0, 1, 2], ax=axes[7], marcadores=['^'], anchobordemarcador=2)\n",
    "        axes[8].set_axis_off()\n",
    "        axes[8].legend(marcadores, [\"Cluster 0\", \"Cluster 1\", \"Cluster 2\"], loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7cwhCmEtpRK",
    "outputId": "07c088ac-8d97-4243-dfaf-582634ef9c5c"
   },
   "outputs": [],
   "source": [
    "dibuja_algoritmos_kmeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8mS-u3btpRY"
   },
   "source": [
    "Los **centroides** de los clústeres se representan con triángulos, mientras que los **puntos de datos** aparecen como círculos. Los colores indican la pertenencia de cada punto a un clúster.\n",
    "\n",
    "En este caso hemos especificado que deseamos **tres clústeres**, por lo que el algoritmo se inicializa seleccionando aleatoriamente tres puntos de datos como centroides. A continuación comienza el proceso iterativo:\n",
    "\n",
    "1. **Asignación**: cada punto de datos se asigna al centroide más cercano.  \n",
    "2. **Actualización**: cada centroide se recalcula como la media de los puntos asignados.  \n",
    "\n",
    "Estos dos pasos se repiten. Tras la tercera iteración, las asignaciones de puntos a centroides no cambiaron, por lo que el algoritmo concluye.\n",
    "\n",
    "Dado un nuevo conjunto de puntos, K-means los asignará al centroide más cercano. El siguiente ejemplo muestra las regiones (fronteras de Voronoi) correspondientes a los centroides aprendidos en el gráfico anterior:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R06A_9FUtpRb"
   },
   "outputs": [],
   "source": [
    "def dibuja_frontera_kmeans():\n",
    "    X, y = make_blobs(random_state=1)\n",
    "    inicio = X[:3, :]\n",
    "    km = KMeans(n_clusters=3, init=inicio, max_iter=2, n_init=1).fit(X)\n",
    "    dibuja_dispersion_discreta(X[:, 0], X[:, 1], km.labels_, marcadores=['o'])\n",
    "    dibuja_dispersion_discreta(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], [0, 1, 2], marcadores=['^'], anchobordemarcador=2)\n",
    "    dibuja_clasificacion_2d(km, X, cm=cm3, alfa=.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2G3kZSctpRo",
    "outputId": "95764f7b-f6d9-4bdf-9dfb-878306ee96bc"
   },
   "outputs": [],
   "source": [
    "dibuja_frontera_kmeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCaiLkdetpRz"
   },
   "source": [
    "La aplicación de K-means con scikit-learn es muy sencilla. En este ejemplo usamos los mismos datos artificiales de los gráficos anteriores. Primero, importamos e instanciamos la clase `KMeans`, indicando el número de clústeres que queremos (parámetro `n_clusters`). Luego llamamos al método `fit` con el conjunto de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3ZZIqmhtpR0",
    "outputId": "03cb3062-6889-4ebb-97d7-1f9a3c1f5215"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# generamos datos superficiales con dos dimensiones\n",
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "# construimos el modelo de clustering\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KlN_E0ptpR_",
    "outputId": "fa50cdd5-2801-4d9a-d242-5ebc14308170"
   },
   "outputs": [],
   "source": [
    "print(\"Miembros de los clustering:\\n{}\".format(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3ZU663etpSI"
   },
   "source": [
    "Como pedimos tres clusters, los clusters están numerados del `0` al `2`. También se puede asignar etiquetas de clúster a nuevos puntos, utilizando el método `predict`. Cada nuevo punto se asigna al centro de clúster más cercano al predecir, pero el modelo existente no se cambia. \n",
    "\n",
    "La ejecución de `predict `en el conjunto de entrenamiento resulta  el mismo resultado que `labels_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-QHOlJ4tpSK",
    "outputId": "f8021ced-22fe-4cd9-9873-f758a2f0bb77"
   },
   "outputs": [],
   "source": [
    "print(kmeans.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97khjfxntpSV"
   },
   "source": [
    "El clustering es similar a la clasificación, ya que cada elemento recibe una etiqueta. Sin embargo, no existe una **verdad absoluta** y, por lo tanto, las etiquetas en sí mismas no tienen un significado a priori. Es posible que el grupo 3 encontrado por el algoritmo contenga solo las caras de una misma persona; sin embargo, solo podrás confirmarlo después de revisar las imágenes, ya que el número 3 es completamente arbitrario. La única información que el algoritmo aporta es que todas las caras etiquetadas como \"3\" son similares.\n",
    "\n",
    "En el clustering que acabamos de calcular sobre el conjunto de datos bidimensional, no deberíamos asignar ningún significado al hecho de que un grupo se etiquete como **0** y otro como **1**. Si ejecutas el algoritmo de nuevo, podrías obtener una numeración distinta debido a la aleatoriedad en la inicialización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcZc5n57tpSY",
    "outputId": "98e2a104-26d7-4eb1-8863-d434afd8bf22"
   },
   "outputs": [],
   "source": [
    "# Las asignaciones de clúster y los centros de clúster encontrados por k-means con tres clusters\n",
    "\n",
    "dibuja_dispersion_discreta(X[:, 0], X[:, 1], kmeans.labels_, marcadores='o')\n",
    "dibuja_dispersion_discreta( kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2], marcadores='^', anchobordemarcador=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvqXU-bNtpSl"
   },
   "source": [
    "También podemos usar más o menos centros de clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKY4wNH4tpSo",
    "outputId": "bc3c6750-a70f-411d-c70f-ac0023d86b38"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# usando dos centros de clusters:\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "asignamientos = kmeans.labels_\n",
    "dibuja_dispersion_discreta(X[:, 0], X[:, 1], asignamientos, ax=axes[0])\n",
    "\n",
    "# usando cinco centros de clustering:\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "asignamientos = kmeans.labels_\n",
    "dibuja_dispersion_discreta(X[:, 0], X[:, 1], asignamientos, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TD-UAr04tpSx"
   },
   "source": [
    "### **Casos en que falla k-means**\n",
    "\n",
    "Aunque se conozca el número \"correcto\" de clusters en un conjunto de datos, k-means puede no ser capaz de detectarlos. Cada cluster se define únicamente por su centro, por lo que adopta formas convexas y, por tanto, solo captura distribuciones relativamente simples. \n",
    "\n",
    "Además, asume que todos los clusters tienen un \"diámetro\" similar: la frontera entre ellos siempre queda exactamente a mitad de camino entre sus centros. Esto, en ocasiones, conduce a resultados contraintuitivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-M79ccntpSz",
    "outputId": "a7ba7eb8-aaa3-4766-84b8-23a5f06c9b7a"
   },
   "outputs": [],
   "source": [
    "X_c, y_c = make_blobs(n_samples=200, cluster_std=[1.0, 2.5, 0.5], random_state=170)\n",
    "y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_c)\n",
    "dibuja_dispersion_discreta(X_c[:, 0], X_c[:, 1], y_pred)\n",
    "plt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc='best')\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvj1dZzytpTJ"
   },
   "source": [
    "Podrías haber esperado que la región densa en la parte inferior izquierda fuera el primer cluster, la región densa en la parte superior derecha el segundo, y la menos densa en el centro el tercero. En cambio, tanto el **cluster 0** como el **cluster 1** incluyen puntos muy alejados del resto, \"extendiéndose\" hacia el centro.\n",
    "\n",
    "Además, k-means asume que todas las direcciones son igualmente relevantes para cada cluster. La siguiente gráfica muestra un conjunto de datos bidimensional con tres grupos claramente separados, pero estirados en diagonal. Dado que k-means solo considera la distancia al centro de su cluster más cercano, no puede manejar este tipo de distribuciones:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCc0lVWCtpTL",
    "outputId": "b35d4e22-4270-4211-c3ea-a2154ed30791"
   },
   "outputs": [],
   "source": [
    "# Genera algunos datos de clúster aleatorios\n",
    "X, y = make_blobs(random_state=170, n_samples=600)\n",
    "rng = np.random.RandomState(74)\n",
    "\n",
    "# transforma los datos a extender\n",
    "transformacion = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformacion)\n",
    "\n",
    "# agrupa los datos en tres clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "#  dibuja las asignaciones de clúster y los centros de clúster\n",
    "dibuja_dispersion_discreta(X[:, 0], X[:, 1], kmeans.labels_, marcadores='o')\n",
    "dibuja_dispersion_discreta( kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2], \n",
    "                           marcadores='^', anchobordemarcador=2)\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFYu5rp0tpTV"
   },
   "source": [
    "k-means  funciona mal si los clusters tienen formas más complejas, como los datos `two_moons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iXU5YcKtpTW",
    "outputId": "c14ab7f0-7e6b-4273-f907-212e0f7715bd"
   },
   "outputs": [],
   "source": [
    "# generamos datos artificiales  two_moons con menor ruido\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# clustering en los datos en dos  clusteres\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "#  dibujamos las asignaciones de clúster y los centros de clúster\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap= cm2, s=60, edgecolor='k')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "            marker='^', c=[cm2(0), cm2(1)], s=100, linewidth=2,\n",
    "            edgecolor='k')\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ioczki7QtpTe"
   },
   "source": [
    "### **Cuantización vectorial o k-means como método de descomposición**\n",
    "\n",
    "Aunque k-means es un algoritmo de clustering, existen interesantes paralelismos con métodos de descomposición como PCA y NMF. Como se sabe, PCA busca las direcciones de máxima varianza en los datos, mientras que NMF intenta hallar componentes aditivos que suelen corresponder a \"extremos\" o \"partes\" de la información. Ambos enfoques representan cada punto de datos como una combinación lineal de varios componentes. \n",
    "\n",
    "En cambio, k-means representa cada punto mediante el centroide de su clúster, es decir, usando un único componente. Esta visión de k-means como un método de descomposición, en la que cada punto se cuantiza a un solo vector representativo, recibe el nombre de **cuantización vectorial**.\n",
    "\n",
    "A continuación, compararemos PCA, NMF y k-means en paralelo, mostrando tanto los componentes extraídos (o centroides) como las reconstrucciones de las caras del conjunto de prueba usando 100 componentes. En el caso de k-means, la reconstrucción de cada punto corresponde al centroide más cercano encontrado durante el entrenamiento:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w12AUVdptpTf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "%matplotlib inline\n",
    "\n",
    "personas = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "imagen_shape = personas.images[0].shape\n",
    "\n",
    "# Crea una máscara booleana seleccionando hasta 50 imágenes por persona\n",
    "mascara = np.zeros(personas.target.shape, dtype=bool)   # <-- usa bool en lugar de np.bool\n",
    "for objetivo in np.unique(personas.target):\n",
    "    indices = np.where(personas.target == objetivo)[0][:50]\n",
    "    mascara[indices] = True\n",
    "\n",
    "X_personas = personas.data[mascara]\n",
    "y_personas = personas.target[mascara]\n",
    "\n",
    "# Escala valores de píxeles a [0,1] para estabilidad numérica\n",
    "X_personas = X_personas / 255.0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbAAAVMRtpTz"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, PCA\n",
    "\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X_personas,\n",
    "                                        y_personas, stratify=y_personas, random_state=0)\n",
    "nmf = NMF(n_components=100, random_state=0)\n",
    "nmf.fit(X_entrenamiento)\n",
    "\n",
    "pca = PCA(n_components=100, random_state=0)\n",
    "pca.fit(X_entrenamiento)\n",
    "kmeans = KMeans(n_clusters=100, random_state=0)\n",
    "kmeans.fit(X_entrenamiento)\n",
    "\n",
    "X_reconstruida_pca = pca.inverse_transform(pca.transform(X_prueba))\n",
    "X_reconstruida_kmeans = kmeans.cluster_centers_[kmeans.predict(X_prueba)]\n",
    "X_reconstruida_nmf = np.dot(nmf.transform(X_prueba), nmf.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBk79ZOatpT6",
    "outputId": "3f6ccc7d-750e-460f-b4b0-af9f0ada5877"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(8, 8),\n",
    "subplot_kw={'xticks': (), 'yticks': ()})\n",
    "fig.suptitle(\"Componentes extraidos\")\n",
    "\n",
    "for ax, comp_kmeans, comp_pca, comp_nmf in zip( axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\n",
    "    ax[0].imshow(comp_kmeans.reshape(imagen_shape))\n",
    "    ax[1].imshow(comp_pca.reshape(imagen_shape), cmap='viridis')\n",
    "    ax[2].imshow(comp_nmf.reshape(imagen_shape))\n",
    "    \n",
    "axes[0, 0].set_ylabel(\"kmeans\")\n",
    "axes[1, 0].set_ylabel(\"pca\")\n",
    "axes[2, 0].set_ylabel(\"nmf\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, subplot_kw={'xticks': (), 'yticks': ()},figsize=(8, 8))\n",
    "fig.suptitle(\"Reconstrucciones\")\n",
    "for ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(axes.T, X_prueba, X_reconstruida_kmeans,\n",
    "                                                  X_reconstruida_pca,X_reconstruida_nmf):\n",
    "    ax[0].imshow(orig.reshape(imagen_shape))\n",
    "    ax[1].imshow(rec_kmeans.reshape(imagen_shape))\n",
    "    ax[2].imshow(rec_pca.reshape(imagen_shape))\n",
    "    ax[3].imshow(rec_nmf.reshape(imagen_shape))\n",
    "\n",
    "axes[0, 0].set_ylabel(\"original\")\n",
    "axes[1, 0].set_ylabel(\"kmeans\")\n",
    "axes[2, 0].set_ylabel(\"pca\")\n",
    "axes[3, 0].set_ylabel(\"nmf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufPomdvztpUC"
   },
   "source": [
    "Un aspecto interesante de la cuantización vectorial con k-means es que podemos emplear muchos más clústeres que las dimensiones de entrada para codificar nuestros datos. Volvamos al conjunto de datos `two_moons`. \n",
    "\n",
    "Con PCA o NMF apenas podemos hacer algo útil, pues estos datos residen en un espacio bidimensional; reducirlos a una dimensión con cualquiera de estos métodos destruiría por completo su estructura. En cambio, con k-means podemos obtener una representación mucho más rica empleando un número mayor de centroides:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFZQb9XptpUD",
    "outputId": "f604e48e-34f8-4275-f03f-1f40205f9935"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap='Paired')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=60,\n",
    "marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired')\n",
    "\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")\n",
    "print(\"Miembros de los clustering:\\n{}\".format(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPk47OFvtpUL"
   },
   "source": [
    "Usamos 10 centros de clúster, lo que significa que a cada punto se le asigna un número entre `0` y `9`. Podemos ver esto como datos representados con 10 componentes (es decir, disponemos de 10 nuevas características), en las que todas las dimensiones están en cero, salvo aquella que indica el centro del clúster al que está asignado el punto. Gracias a esta representación en 10 dimensiones, ahora sería posible separar las dos \"medias lunas\" con un modelo lineal, algo que no hubiese sido factible con las dos características originales.\n",
    "\n",
    "También podemos obtener una representación aún más expresiva usando, como características, las distancias a cada uno de los centros de clúster. Esto se logra fácilmente con el método `transform` de `KMeans`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCpGIefhtpUM",
    "outputId": "758533d9-09bf-4542-a6a9-e266fb8c86c3"
   },
   "outputs": [],
   "source": [
    "distancia_caracteristica = kmeans.transform(X)\n",
    "print(\"Dimension de las distancia caracteristica : {}\".format(distancia_caracteristica.shape))\n",
    "print(\"Distancia caracteristica:\\n{}\".format(distancia_caracteristica))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be1mBM9jtpUU"
   },
   "source": [
    "**Limitaciones de k-means**\n",
    "\n",
    "El algoritmo **k-means** es uno de los más populares para el **agrupamiento**, no solo porque resulta relativamente fácil de entender e implementar, sino también porque ofrece un rendimiento rápido. Además, **scikit-learn** incluye la clase `MiniBatchKMeans`, una variante aún más escalable capaz de procesar conjuntos de datos de gran tamaño.\n",
    "\n",
    "Sin embargo, k-means presenta algunas limitaciones:\n",
    "\n",
    "1. **Inicialización aleatoria**: el resultado depende de la semilla. Por defecto, scikit-learn ejecuta el algoritmo 10 veces con 10 inicializaciones distintas y devuelve el mejor resultado.\n",
    "2. **Suposiciones sobre la forma de los clústeres**: k-means asume clústeres aproximadamente esféricos y de tamaño similar.\n",
    "3. **Número de clústeres predefinido**: es necesario especificar de antemano el valor de *k*, lo cual puede no ser evidente en aplicaciones del mundo real.\n",
    "\n",
    "Para mitigar estos inconvenientes, a menudo se emplean estrategias como:\n",
    "\n",
    "* Inicialización mejorada (p. ej., **k-means++**).\n",
    "* Validación de clústeres (métricas de silueta, índice de Calinski–Harabasz, etc.).\n",
    "* Métodos alternativos basados en densidad o en grafos, cuando la forma de los grupos es irregular.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
