{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceee61fa-dfb0-4404-88bb-fc57338f0018",
   "metadata": {},
   "source": [
    "### Escenario\n",
    "\n",
    "- Tienes entradas para un algoritmo que es una caja negra y se tiene su tiempo de ejecución, es posible saber su complejidad algoritmica. \n",
    "\n",
    "- Tienes entradas para un algoritmo que es una caja negra y se tiene la complejidad temporal y espacial, es posible saber de tu tiempo de ejecución. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923ac98-87a3-4af2-84e7-735f4981b8fb",
   "metadata": {},
   "source": [
    "### **Complejidad algorítmica basado en tiempos de ejecución en algoritmos caja negra**\n",
    "\n",
    "Cuando se cuenta únicamente con entradas y mediciones experimentales del tiempo de ejecución de un algoritmo tratado como \"caja negra\", la tarea consiste en inferir la complejidad algorítmica subyacente sin conocer la implementación interna. \n",
    "\n",
    "\n",
    "El análisis asintótico es una herramienta esencial para describir el crecimiento del tiempo de ejecución $T(n)$ en función del tamaño de la entrada $n$. La notación $O$, $\\Omega$ y $\\Theta$ permiten abstraer las constantes y términos de orden inferior, enfocándose en el comportamiento para $n \\to \\infty$.  \n",
    "Cuando solo se dispone de los tiempos medidos, es posible ajustar una función que se aproxime a $ T(n) $. Sin embargo, la precisión del análisis dependerá de la calidad de los datos, la uniformidad de los experimentos y la variabilidad inherente a la ejecución en entornos reales.\n",
    "\n",
    "\n",
    "Una estrategia empírica común consiste en recolectar datos de ejecución para diversos tamaños de entrada y luego ajustar un modelo de la forma:\n",
    "\n",
    "$$\n",
    "T(n) \\approx c \\cdot n^k \\quad \\text{ó} \\quad T(n) \\approx c \\cdot f(n)\n",
    "$$\n",
    "\n",
    "donde $ c $ y $ k $ son constantes desconocidas.  \n",
    "Para diferenciar entre comportamientos polinómicos, exponenciales o logarítmicos, se emplea la transformación logarítmica. Si se tiene un modelo polinómico, se puede escribir:\n",
    "\n",
    "$$\n",
    "\\log(T(n)) \\approx \\log(c) + k \\cdot \\log(n)\n",
    "$$\n",
    "\n",
    "Este método transforma la relación en una recta en el plano log-log, donde la pendiente indica el exponente $ k $ del modelo polinómico. Técnicas de regresión lineal permiten estimar de forma robusta estos parámetros, siempre y cuando la muestra de datos cubra un rango suficientemente amplio de tamaños de entrada.\n",
    "\n",
    "\n",
    "Para descartar modelos alternativos, es habitual plantear hipótesis sobre diferentes tipos de crecimiento:\n",
    "- **Crecimiento lineal:** $ T(n) \\approx c \\cdot n $\n",
    "- **Crecimiento polinómico:** $ T(n) \\approx c \\cdot n^k $ con $ k > 1 $\n",
    "- **Crecimiento logarítmico o lineal-logarítmico:** $ T(n) \\approx c \\cdot n \\log n $\n",
    "- **Crecimiento exponencial:** $ T(n) \\approx c \\cdot a^n $\n",
    "\n",
    "El uso de análisis de la varianza (ANOVA) y criterios de información (como AIC o BIC) ayuda a seleccionar el modelo que mejor se ajusta a la distribución de los tiempos medidos, permitiendo inferir la complejidad teórica más adecuada.\n",
    "\n",
    "\n",
    "Un aspecto crítico en la inferencia de complejidad es la presencia de constantes ocultas y efectos no asintóticos. Por ejemplo, el comportamiento de $ T(n) $ en entradas pequeñas puede estar dominado por costos de inicialización, optimizaciones del compilador o latencias de memoria que no son relevantes para el crecimiento asintótico, pero que distorsionan la regresión.  \n",
    "Es fundamental, por tanto, seleccionar un rango de datos en el que el comportamiento asintótico sea claramente observable, minimizando la interferencia de estos factores externos.\n",
    "\n",
    "\n",
    "En muchos algoritmos, especialmente aquellos basados en la técnica divide y vencerás, el tiempo de ejecución se expresa mediante recurrencias. Una forma clásica es:\n",
    "\n",
    "$$\n",
    "T(n) = a \\cdot T\\left(\\frac{n}{b}\\right) + f(n)\n",
    "$$\n",
    "\n",
    "donde $ a $ es el número de subproblemas, $ b $ es el factor de reducción y $ f(n) $ representa el coste de la división y combinación.  \n",
    "El teorema maestro permite resolver este tipo de recurrencias y determinar la complejidad asintótica. Si, por ejemplo, los datos experimentales muestran que al duplicar $ n $ el tiempo se incrementa en una proporción predecible, se puede inferir la existencia de una recurrencia que se ajuste a este patrón.\n",
    "\n",
    "\n",
    "Aunque la mayoría de los análisis se enfocan en el peor caso, el análisis amortizado resulta útil cuando se tienen operaciones de costo variable. Por ejemplo, en estructuras de datos dinámicas, ciertas operaciones pueden tener un alto costo puntual (por ejemplo, una reestructuración o \"resize\"), pero el costo medio es considerablemente menor.  \n",
    "Si se dispone de una secuencia de tiempos medidos para una serie de operaciones, se pueden aplicar métodos como el análisis del potencial para determinar el costo amortizado de cada operación. Esto ofrece una visión más realista del comportamiento promedio del algoritmo.\n",
    "\n",
    "Es importante notar que inferir la complejidad solo a partir de tiempos medidos enfrenta ciertos desafíos:\n",
    "- **Variabilidad en la ejecución:** Factores externos (como la carga del sistema o la arquitectura del hardware) pueden introducir ruido en los datos.\n",
    "- **Rango de tamaño de entrada:** El comportamiento asintótico se manifiesta claramente solo cuando $ n $ es suficientemente grande.  \n",
    "- **Suposiciones del modelo:** La elección del modelo (polinómico, exponencial, etc.) puede sesgar los resultados si no se realiza una validación estadística rigurosa.\n",
    "\n",
    "**Ejemplos teóricos**\n",
    "\n",
    "\n",
    "Imaginemos un algoritmo que, al aumentar $ n $, presenta duplicación de tiempos de ejecución de manera consistente. Si se modela con la relación:\n",
    "\n",
    "$$\n",
    "T(n) = 2 \\cdot T\\left(\\frac{n}{2}\\right) + O(n)\n",
    "$$\n",
    "\n",
    "la aplicación del teorema Maestro sugiere que la complejidad es $ O(n \\log n) $. El análisis empírico mediante la transformación logarítmica confirmaría esta hipótesis si la gráfica log-log resultante es lineal con pendiente cercana a 1 (más un factor logarítmico adicional).\n",
    "\n",
    "\n",
    "Otro escenario es cuando el algoritmo muestra un comportamiento híbrido, como $ T(n) \\approx c \\cdot n \\log n + d \\cdot n $. En estos casos, el ajuste del modelo puede requerir técnicas de regresión no lineal que permitan discriminar la contribución de cada término. La correcta identificación de los coeficientes $ c $ y $ d $ es esencial para caracterizar la complejidad.\n",
    "\n",
    "\n",
    "El empleo de pruebas estadísticas y validación cruzada permite evaluar la robustez del modelo obtenido. Al dividir el conjunto de datos en muestras de entrenamiento y validación, es posible verificar que el modelo seleccionado no incurra en sobreajuste y que, efectivamente, capture la tendencia asintótica del algoritmo.\n",
    "\n",
    "La inferencia empírica se enriquece al incorporar conceptos de la teoría de la complejidad computacional. La complejidad de Kolmogorov y la noción de complejidad intrínseca de un problema ofrecen marcos teóricos para argumentar sobre la \"dificultad\" de una tarea computacional. Además, el uso de series de potencias y transformadas (como la transformada de Laplace) en el análisis de recurrencias proporciona herramientas matemáticas para obtener estimaciones más precisas de $ T(n) $.\n",
    "\n",
    "\n",
    "En la práctica, un algoritmo caja negra puede presentar variaciones en función del tipo de entrada. Mediante técnicas de análisis de sensibilidad y \"smoothed complexity\", se pueden detectar las diferencias entre el peor caso, el caso promedio y escenarios particulares en los que la complejidad observada varíe significativamente. Esta diferenciación es vital para comprender completamente el comportamiento algorítmico a partir de tiempos medidos en experimentos.\n",
    "\n",
    "\n",
    "La capacidad para inferir la complejidad algorítmica a partir de mediciones de tiempo tiene aplicaciones directas en la optimización de software y en la ingeniería de sistemas. Sin embargo, se debe tener presente que la inferencia experimental puede verse afectada por:\n",
    "- **Constantes ocultas y costos de inicialización:** Que distorsionan la observación en rangos de entrada pequeños.\n",
    "- **Factores de caché y arquitectura:** Que modifican el comportamiento real respecto al modelo teórico.\n",
    "\n",
    "La combinación de análisis teórico (a través del teorema Maestro, análisis amortizado y técnicas de recurrencia) con métodos estadísticos robustos constituye el enfoque más completo para acercarse a una estimación realista de la complejidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b401e-ecbc-4a4e-9952-051c6a70fb61",
   "metadata": {},
   "source": [
    "### **Complejidad temporal y espacial para la estimación del tiempo de ejecución en algoritmos caja negra**\n",
    "\n",
    "En el segundo escenario se dispone de información tanto de la complejidad temporal como de la complejidad espacial de un algoritmo caja negra. El objetivo es explorar cómo esta doble perspectiva permite una estimación más detallada y precisa del tiempo de ejecución, considerando además la interacción entre recursos temporales y espaciales. \n",
    "\n",
    "En muchos algoritmos, el uso de espacio influye de forma directa en el tiempo de ejecución. Por ejemplo, técnicas de memoización, estructuras auxiliares y estrategias de \"caché\" pueden reducir el tiempo computacional a costa de un mayor consumo de memoria. Esta relación se formaliza a través de modelos que establecen trade-offs entre espacio y tiempo, permitiendo prever que una mejora en uno de los parámetros puede implicar un incremento en el otro.\n",
    "\n",
    "El análisis conjunto de ambas magnitudes ofrece una imagen más completa del rendimiento del algoritmo. Se pueden plantear modelos en los que el tiempo de ejecución $ T(n) $ depende explícitamente de la cantidad de memoria $S(n)$ utilizada, lo que resulta especialmente relevante en sistemas con recursos limitados o en arquitecturas paralelas.\n",
    "\n",
    "Cuando se dispone de ambos conjuntos de datos (temporal y espacial), se pueden emplear técnicas de regresión multivariable para ajustar modelos que integren ambas dimensiones. Por ejemplo, se puede plantear un modelo de la forma:\n",
    "\n",
    "$$\n",
    "T(n) \\approx c \\cdot n^k \\cdot [S(n)]^p\n",
    "$$\n",
    "\n",
    "donde $k$ y $p$ son exponentes que describen la sensibilidad del tiempo de ejecución a cambios en el tamaño de la entrada y en el uso de memoria, respectivamente. La incorporación de la variable $S(n)$ permite aislar efectos derivados de operaciones intensivas en memoria, tales como accesos a disco o reestructuraciones en estructuras dinámicas.\n",
    "\n",
    "\n",
    "Existen escenarios en los que se ha demostrado teóricamente que optimizar el uso de memoria puede llevar a una reducción del tiempo de cómputo y viceversa. La técnica del \"space-time trade-off\" es particularmente relevante en algoritmos de búsqueda y ordenación, donde el empleo de índices o estructuras auxiliares (como árboles balanceados o tablas hash) puede acelerar la búsqueda a costa de utilizar mayor espacio.\n",
    "\n",
    "Un ejemplo teórico es el uso de estructuras de datos comprimidas o índices avanzados en el procesamiento de bases de datos, donde la complejidad temporal se optimiza mediante una mayor inversión en memoria. La modelación de estos sistemas requiere considerar tanto la complejidad teórica (por ejemplo, $O(n \\log n)$ o $O(n)$) como las constantes derivadas de la gestión del espacio, lo que se traduce en predicciones más realistas del tiempo de ejecución en entornos prácticos.\n",
    "\n",
    "\n",
    "En arquitecturas modernas, el análisis de la complejidad debe extenderse a modelos paralelos. Aquí, además del trabajo total (work) y la longitud de la cadena crítica (span), el uso de espacio puede afectar la eficiencia de la comunicación y la sincronización entre núcleos o nodos.  \n",
    "Medir la complejidad en términos de \"work\" y \"span\" y relacionarlo con el uso de memoria permite utilizar leyes como la de Amdahl o Gustafson para predecir el rendimiento en sistemas de cómputo paralelo. Estas predicciones se ajustan a modelos en los que tanto la complejidad temporal como la espacial interactúan para determinar el tiempo de ejecución final en entornos distribuidos.\n",
    "\n",
    "Al igual que en el análisis exclusivo de tiempos, el análisis amortizado resulta clave cuando el uso de memoria varía a lo largo de la ejecución. En estructuras de datos que requieren reestructuraciones o asignaciones dinámicas, se puede demostrar que, a pesar de que algunas operaciones sean costosas en tiempo y memoria, el costo promedio se mantiene dentro de límites aceptables.  \n",
    "La metodología del potencial permite evaluar la eficiencia global del algoritmo en función del uso combinado de ambos recursos, lo que se traduce en modelos más precisos para predecir la evolución temporal conforme se incrementa el tamaño de la entrada.\n",
    "junto\n",
    "\n",
    "Para integrar la complejidad temporal y espacial en un único marco analítico, se pueden aplicar técnicas matemáticas avanzadas. Las transformadas (por ejemplo, la de Mellin o Laplace) ofrecen un método para analizar la recurrencia de algoritmos en contextos donde la variable espacial influye de manera directa en el costo temporal.  \n",
    "Estos métodos permiten derivar expresiones analíticas que cuantifican, de manera conjunta, la dependencia de $T(n)$ y $S(n)$ respecto al tamaño de la entrada, lo que facilita la predicción del tiempo de ejecución a partir de mediciones experimentales en ambas dimensiones.\n",
    "\n",
    "**Ejemplos teóricos**\n",
    "\n",
    "Consideremos un algoritmo que, mediante el uso de estructuras auxiliares, reduce el tiempo de ejecución de una operación compleja. Supongamos que el algoritmo presenta la siguiente relación empírica:\n",
    "\n",
    "$$\n",
    "T(n) \\approx c_1 \\cdot n \\log n \\quad \\text{y} \\quad S(n) \\approx c_2 \\cdot n\n",
    "$$\n",
    "\n",
    "Al incorporar la información espacial en el modelo, es posible inferir que la utilización de memoria lineal permite mantener el crecimiento temporal en $ O(n \\log n) $. El análisis de regresión multivariable ajusta los coeficientes $ c_1 $ y $ c_2 $ para predecir con mayor exactitud el tiempo de ejecución, teniendo en cuenta los efectos de la arquitectura de memoria y los posibles cuellos de botella asociados a la transferencia de datos.\n",
    "\n",
    "En algoritmos recursivos complejos, el uso de memoria puede incrementar debido a la creación de marcos de pila o estructuras de datos temporales. Medir y analizar simultáneamente $T(n)$ y $S(n)$ permite distinguir si los incrementos en el tiempo se deben a la profundidad de la recursión o a la gestión ineficiente de la memoria. En tales casos, la transformación logarítmica aplicada a ambas variables y la posterior regresión permiten estimar la contribución de cada factor, ofreciendo una imagen detallada de la interacción espacio-tiempo.\n",
    "\n",
    "La predicción del tiempo de ejecución a partir de la información combinada de complejidad temporal y espacial se ve sujeta a múltiples desafíos:\n",
    "\n",
    "- **Interacción no lineal:** En algunos algoritmos, el efecto del uso de memoria en el tiempo de ejecución no es lineal.  \n",
    "- **Efectos de caché y jerarquía de memoria:** La distribución de la memoria en niveles (caché L1, L2, RAM, etc.) puede modificar significativamente el comportamiento real.\n",
    "- **Variabilidad en arquitecturas heterogéneas:** Sistemas con diferentes arquitecturas pueden presentar comportamientos distintos, lo que obliga a recalibrar los modelos para cada entorno.\n",
    "\n",
    "La integración de ambos conjuntos de datos requiere, por tanto, un análisis cuidadoso y el uso de técnicas estadísticas robustas que permitan separar los efectos de cada variable y ajustar modelos predictivos precisos.\n",
    "\n",
    "En ciertos algoritmos, la complejidad puede depender de parámetros adicionales además del tamaño $n$ (por ejemplo, características específicas de la estructura de datos utilizada). La complejidad parametrizada se expresa comúnmente en forma de $f(k) \\cdot n^{O(1)} $, donde $k$ es un parámetro intrínseco al problema.  \n",
    "La combinación de complejidad temporal y espacial en estos contextos permite construir modelos que predicen el rendimiento en función de variaciones tanto en $n$ como en $k$, lo que resulta especialmente útil en algoritmos de grafos o en problemas de optimización donde el parámetro $k$ puede tener un impacto significativo.\n",
    "\n",
    "La clave para una predicción robusta del tiempo de ejecución reside en la integración de modelos teóricos con datos experimentales. Utilizar técnicas de validación cruzada, análisis de residuos y métodos de regresión multivariable permite contrastar la teoría con la práctica.  \n",
    "Cuando se observan discrepancias entre las predicciones teóricas y los datos experimentales, se debe investigar si los factores externos (como la arquitectura del hardware o el manejo de memoria) están introduciendo sesgos, y ajustar los modelos en consecuencia.\n",
    "\n",
    "El uso de algoritmos de aprendizaje automático y métodos de análisis estadístico avanzado (por ejemplo, regresiones no lineales y redes neuronales) ha ganado terreno en la estimación de parámetros en sistemas complejos. Estos métodos permiten procesar grandes volúmenes de datos experimentales y extraer relaciones ocultas entre el uso de memoria y el tiempo de ejecución, ofreciendo así una herramienta.\n",
    "\n",
    "En aplicaciones de alto rendimiento, la optimización de algoritmos se fundamenta en entender el balance entre tiempo y espacio. La capacidad para predecir el tiempo de ejecución a partir de la complejidad conjunta es crucial en contextos donde la latencia y el throughput son parámetros críticos, tales como en sistemas de procesamiento en tiempo real o en bases de datos distribuidas.  \n",
    "El análisis detallado de algoritmos que hacen uso intensivo de recursos, como aquellos que implementan técnicas de memoización o estructuras de datos persistentes, permite diseñar sistemas que optimicen el trade-off espacio-tiempo, ajustando dinámicamente la asignación de recursos para maximizar el rendimiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392bb4bb-a313-4cff-9561-dcd287db95c0",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "\n",
    "1 . Se te proporciona el siguiente conjunto de datos experimentales para un algoritmo caja negra:\n",
    "\n",
    "| Tamaño de entrada $ n $ | Tiempo de ejecución $ T(n) $ (ms) |\n",
    "|---------------------------|-------------------------------------|\n",
    "| 100                       | 1.2                                 |\n",
    "| 500                       | 6.8                                 |\n",
    "| 1,000                     | 14.3                                |\n",
    "| 5,000                     | 85.0                                |\n",
    "| 10,000                    | 180.5                               |\n",
    "\n",
    "- Realiza una transformación logarítmica de ambos conjuntos de datos.\n",
    "-  Ajusta una regresión lineal sobre la gráfica log-log para obtener la relación  \n",
    "   $\\log(T(n)) = \\log(c) + k \\cdot \\log(n)$.\n",
    "- Determina el valor del exponente $ k $ y discute qué indica en cuanto a la complejidad (por ejemplo, $ O(n^k) $).\n",
    "\n",
    "*Sugerencia:* Utiliza métodos de mínimos cuadrados para obtener la pendiente de la recta ajustada.\n",
    "\n",
    "  \n",
    "2 .Supón que se sospecha que un algoritmo caja negra sigue una recurrencia de la forma:\n",
    "\n",
    "$$\n",
    "T(n) = 2 \\cdot T\\left(\\frac{n}{2}\\right) + n\n",
    "$$\n",
    "\n",
    "- Aplica el teorema maestro para determinar la complejidad asintótica de $ T(n) $.\n",
    "- Diseña un conjunto de experimentos (real o simulado) para validar la hipótesis empírica que indique un crecimiento $ O(n \\log n) $.\n",
    "- Discute las posibles discrepancias entre el análisis teórico y los datos empíricos, considerando factores como constantes ocultas y efectos de hardware.\n",
    "\n",
    "*Sugerencia:* Identifica los parámetros $ a = 2 $, $ b = 2 $ y $ f(n)=n $, y compara $ f(n) $ con $ n^{\\log_b a} $.\n",
    "\n",
    " \n",
    "3 . Imagina que se implementa una estructura de datos dinámica (por ejemplo, un arreglo dinámico) en la que cada inserción tiene un costo $ O(1) $ en la mayoría de los casos, pero ocasionalmente se requiere redimensionar el arreglo, lo cual tiene un costo de $ O(n) $.\n",
    "\n",
    "1. Modela la secuencia de inserciones y define el costo total en función del número de operaciones $ m $.\n",
    "2. Utiliza el método del potencial para demostrar que el costo amortizado por operación es $ O(1) $.\n",
    "3. Propon un conjunto de datos experimentales simulados que evidencien el comportamiento amortizado y explica cómo validarías empíricamente el análisis.\n",
    "\n",
    "*Sugerencia:* Define una función potencial adecuada que capture la “carga” acumulada antes y después de cada reestructuración.\n",
    "\n",
    "\n",
    " 4 . Considera que se tiene un conjunto de datos experimentales que registra únicamente el tiempo de ejecución $ T(n) $ para distintos tamaños de entrada. Sin embargo, se sospecha que el rendimiento del algoritmo también está influido por características específicas de la entrada (por ejemplo, la distribución de los datos).\n",
    "\n",
    "- Propón un modelo estadístico en el que el tiempo de ejecución dependa de $ n $ y de una variable adicional $ d $ (que represente, por ejemplo, la \"densidad\" o \"ordenación\" de la entrada).\n",
    "- Diseña un experimento en el que recolectes datos para distintos valores de $ n $ y $ d $.\n",
    "- Utiliza regresión multivariable para ajustar el modelo y discute la significancia de cada parámetro en la predicción de $ T(n) $.\n",
    "\n",
    "*Sugerencia:* Considera un modelo de la forma $ T(n, d) = c_1 \\cdot n^k + c_2 \\cdot d + \\epsilon $, donde $ \\epsilon $ es el término de error.\n",
    "\n",
    "  \n",
    "5 . Se dispone de datos experimentales que muestran que un algoritmo utiliza:\n",
    "\n",
    "- Tiempo de ejecución: $ T(n) \\approx c_1 \\cdot n \\log n $\n",
    "- Uso de memoria: $ S(n) \\approx c_2 \\cdot n $\n",
    "\n",
    "Propón un modelo que relacione $ T(n) $ y $ S(n) $ de manera conjunta, por ejemplo:  \n",
    "   $$\n",
    "   T(n) \\approx c_1 \\cdot n \\log n \\cdot [S(n)]^p\n",
    "   $$\n",
    "   y determina el exponente $ p $ a partir de los datos.\n",
    "\n",
    "Discute cómo el aumento en el uso de memoria podría optimizar (o empeorar) el tiempo de ejecución en contextos de cache y jerarquía de memoria.\n",
    "\n",
    "Propón un experimento que permita validar este modelo y analiza posibles fuentes de error.\n",
    "\n",
    "*Sugerencia:* Considera la posibilidad de que $ p $ sea cercano a cero si el uso de memoria optimiza el acceso a datos.\n",
    "\n",
    "  \n",
    "6 . Un algoritmo se implementa en un entorno paralelo y se han medido los siguientes parámetros:\n",
    "- Trabajo total (work): $ T_1(n) = O(n) $\n",
    "- Cadena crítica (span): $ T_\\infty(n) = O(\\log n) $\n",
    "\n",
    "1. Utiliza la ley de Amdahl y la ley de Gustafson para predecir el tiempo de ejecución $ T_p(n) $ en un sistema con $ p $ procesadores.\n",
    "2. Diseña un experimento (real o simulado) para comparar las predicciones teóricas con la ejecución real en diferentes valores de $ p $.\n",
    "3. Discute los desafíos que surgen al integrar la complejidad espacial en este contexto, especialmente en lo referente a la comunicación y sincronización en sistemas paralelos.\n",
    "\n",
    "*Sugerencia:* Formula $ T_p(n) \\approx \\frac{T_1(n)}{p} + T_\\infty(n) $ y analiza cómo varía con $ p $.\n",
    "\n",
    "\n",
    "7 . Considera un algoritmo cuya complejidad depende de dos parámetros: el tamaño de la entrada $ n $ y un parámetro estructural $ k $ (por ejemplo, la \"treewidth\" de un grafo). La complejidad se expresa como:\n",
    "\n",
    "$$\n",
    "T(n, k) = f(k) \\cdot n^{O(1)}\n",
    "$$\n",
    "\n",
    "1. Define, teóricamente, posibles formas de la función $ f(k) $ en base a la literatura (por ejemplo, $ f(k) = 2^k $ o $ f(k) = k! $).\n",
    "2. Diseña un conjunto de experimentos en el que varíes tanto $ n $ como $ k $, y registra los tiempos de ejecución.\n",
    "3. Utiliza técnicas de regresión multivariable para determinar la forma funcional de $ f(k) $ y discute cómo el parámetro $ k $ afecta la escalabilidad del algoritmo.\n",
    "\n",
    "*Sugerencia:* Evalúa la sensibilidad del tiempo de ejecución ante cambios en $ k $ manteniendo $ n $ constante y viceversa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a99ec0-8a6e-461f-b19e-63b05545dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
