{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOq6r6_Qtqy8"
   },
   "source": [
    "## **El clustering aglomerativo**\n",
    "\n",
    "El clustering aglomerativo  es un algoritmo jerárquico de clustering que comienza tratando cada punto como un clúster independiente y, a continuación, fusiona de forma iterativa los dos grupos más cercanos hasta alcanzar el número de clústeres deseado. En scikit-learn, ese parámetro se llama `n_clusters`, de modo que el proceso de fusión continúa hasta que solo quedan tantos grupos como se haya especificado al instanciar `AgglomerativeClustering`.\n",
    "\n",
    "Para decidir qué dos clústeres son \"los más cercanos\", scikit-learn ofrece distintos criterios de enlace (linkage). El enlace por defecto, `ward`, selecciona en cada paso el par cuya fusión incrementa lo menos posible la suma de varianzas internas, lo cual suele dar lugar a grupos de tamaño y dispersión relativamente homogéneos. Si, por el contrario, los clústeres presentan un número de puntos muy desigual o formas irregulares, puede convenir probar el enlace `average`, que calcula la distancia media entre todos los pares de puntos de dos clústeres, o el enlace `complete` (también llamado enlace máximo), que se basa en la distancia máxima entre puntos y tiende a producir clústeres más compactos y bien separados.\n",
    "\n",
    "Además de `linkage`, otro parámetro importante es la métrica de distancia, que en versiones recientes de scikit-learn se denomina `metric` (antes `affinity`). Con `ward` solo es válida la métrica euclídea, pero si usamos `average` o `complete` podemos optar por “manhattan”, “cosine” u otras distancias soportadas. También existe la opción `connectivity`, que permite imponer un grafo de vecindad sobre los datos; resulta útil cuando sabemos que solo ciertos puntos deberían considerarse vecinos (por ejemplo, datos espaciales o estructurados en red).\n",
    "\n",
    "En la práctica, la implementación es muy sencilla: basta crear una instancia de `AgglomerativeClustering` con `n_clusters=4`, `linkage=\"average\"` y `metric=\"manhattan\"`, y luego llamar a `fit_predict(X)` sobre nuestra matriz de datos `X`. Si queremos entender mejor la jerarquía resultante, podemos pasar los mismos datos a `scipy.cluster.hierarchy.linkage` (por ejemplo con `method=\"complete\"`) y luego dibujar un dendrograma con `dendrogram(Z)`, lo cual proporciona una representación gráfica muy intuitiva de las sucesivas fusiones.\n",
    "\n",
    "El principal beneficio del clustering aglomerativo es que no requiere asumir ninguna forma concreta de clúster y permite explorar distintos niveles de granularidad gracias a la jerarquía inherente. Sin embargo, su coste es cuadrático tanto en tiempo como en memoria, lo cual lo hace poco escalable para conjuntos de datos muy grandes. Además, resulta sensible al ruido y a los valores atípicos, que pueden modificar drásticamente la estructura de fusiones tempranas.\n",
    "\n",
    "En resumen, para datos de tamaño moderado y con cierta homogeneidad, `ward` suele ser la mejor opción, mientras que `average` o `complete` pueden funcionar mejor cuando esperamos clústeres de tamaños muy distintos o formas no esféricas. Incorporar métricas distintas o restricciones de conectividad amplía aún más las posibilidades, permitiéndonos adaptar el algoritmo a la naturaleza particular de nuestros datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6uu5okztqzF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def dibuja_algoritmo_aglomerativo():\n",
    "    # generamos datos artificiales dos dimensionales\n",
    "    X, y = make_blobs(random_state=0, n_samples=12)\n",
    "    agg = AgglomerativeClustering(n_clusters=X.shape[0], compute_full_tree=True).fit(X)\n",
    "    fig, axes = plt.subplots(X.shape[0] // 5, 5, subplot_kw={'xticks': (),\n",
    "                                                             'yticks': ()},\n",
    "                             figsize=(20, 8))\n",
    "\n",
    "    eps = X.std() / 2\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
    "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    puntos_grid = np.c_[xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)]\n",
    "\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        agg.n_clusters = X.shape[0] - i\n",
    "        agg.fit(X)\n",
    "        ax.set_title(\"Paso %d\" % i)\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=60, c='grey')\n",
    "        bins = np.bincount(agg.labels_)\n",
    "        for cluster in range(agg.n_clusters):\n",
    "            if bins[cluster] > 1:\n",
    "                puntos = X[agg.labels_ == cluster]\n",
    "                otros_puntos = X[agg.labels_ != cluster]\n",
    "\n",
    "                kde = KernelDensity(bandwidth=.5).fit(puntos)\n",
    "                puntuacion = kde.score_samples(puntos_grid)\n",
    "                puntuacion_1 = np.min(kde.score_samples(puntos))\n",
    "                puntuacion_2 = np.max(kde.score_samples(otros_puntos))\n",
    "                niveles = .8 * puntuacion_1 + .2 * puntuacion_2\n",
    "                ax.contour(xx, yy, puntuacion.reshape(100, 100), levels=[niveles],\n",
    "                           colors='k', linestyles='solid', linewidths=2)\n",
    "\n",
    "    axes[0, 0].set_title(\"Inicializacion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzavuhqItqzd",
    "outputId": "6393b441-244d-4091-c69f-617fe496befd"
   },
   "outputs": [],
   "source": [
    "dibuja_algoritmo_aglomerativo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5viF8M5tqzx"
   },
   "source": [
    "Al principio, cada punto forma su propio clúster. Luego, en cada iteración, se fusionan los dos grupos más cercanos. En los cuatro primeros pasos, siempre se unen dos clústeres de un solo punto para formar clústeres de dos puntos. En el paso 5, uno de esos clústeres de dos puntos incorpora un tercer punto, y así sucesivamente. Cuando llegamos al paso 9, solo quedan tres clústeres; como habíamos fijado `n_clusters=3`, el algoritmo se detiene.\n",
    "\n",
    "Veamos ahora cómo opera el clustering aglomerativo con un conjunto muy simple de tres clústeres de datos. Dado que este método es puramente jerárquico, **no** puede asignar nuevos puntos tras el entrenamiento; por ello, la clase `AgglomerativeClustering` de scikit-learn **no** implementa un método `predict`. En su lugar, para ajustar el modelo y obtener las etiquetas de cada muestra, usamos `fit_predict(X)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función auxiliar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLjMAM1Mtqzz"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.colors import  colorConverter\n",
    "\n",
    "def dibuja_dispersion_discreta(x1, x2, y=None, marcadores=None, s=10, ax=None,\n",
    "                     etiquetas=None, relleno=.2, alfa=1, c=None, anchobordemarcador=None):\n",
    "    \"\"\"Adaptacion de la funcion matplotlib.pyplot.scatter a dibujar clases o  clusters\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if y is None:\n",
    "        y = np.zeros(len(x1))\n",
    "\n",
    "    unico_y = np.unique(y)\n",
    "\n",
    "    if marcadores is None:\n",
    "        marcadores= ['o', '^', 'v', 'D', 's', '*', 'p', 'h', 'H', '8', '<', '>'] * 10\n",
    "\n",
    "    if len(marcadores) == 1:\n",
    "        marcadores = marcadores * len(unico_y)\n",
    "\n",
    "    if etiquetas is None:\n",
    "        etiquetas = unico_y\n",
    "\n",
    "    # lineas en el sentido de matplotlib\n",
    "    lineas = []\n",
    "\n",
    "    actual_cycler = mpl.rcParams['axes.prop_cycle']\n",
    "\n",
    "    for i, (yy, cycle) in enumerate(zip(unico_y, actual_cycler())):\n",
    "        mascara = y == yy\n",
    "        # if c is ninguno, use color cycle\n",
    "        if c is None:\n",
    "            color = cycle['color']\n",
    "        elif len(c) > 1:\n",
    "            color = c[i]\n",
    "        else:\n",
    "            color = c\n",
    "        # bordes claros para marcadores oscuros\n",
    "        if np.mean(colorConverter.to_rgb(color)) < .4:\n",
    "            colorbordemarcador = \"grey\"\n",
    "        else:\n",
    "            colorbordemarcador = \"black\"\n",
    "\n",
    "        lineas.append(ax.plot(x1[mascara], x2[mascara], marcadores[i], markersize=s,\n",
    "                             label=etiquetas[i], alpha=alfa, c=color,\n",
    "                             markeredgewidth=anchobordemarcador,\n",
    "                             markeredgecolor=colorbordemarcador)[0])\n",
    "\n",
    "    if relleno != 0:\n",
    "        pad1 = x1.std() * relleno\n",
    "        pad2 = x2.std() * relleno\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "        ax.set_xlim(min(x1.min() - pad1, xlim[0]), max(x1.max() + pad1, xlim[1]))\n",
    "        ax.set_ylim(min(x2.min() - pad2, ylim[0]), max(x2.max() + pad2, ylim[1]))\n",
    "\n",
    "    return lineas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kifMJNjDtq0A",
    "outputId": "91df15ae-88e9-4568-8500-4d32945aad32"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "X, y = make_blobs(random_state=1)\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "asignamiento = agg.fit_predict(X)\n",
    "dibuja_dispersion_discreta(X[:, 0], X[:, 1], asignamiento)\n",
    "plt.legend([\"Cluster 0\", \"Cluster 1\", \"Cluster 2\"], loc=\"best\")\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VvruwTYtq0K"
   },
   "source": [
    "Como era de esperar, el algoritmo recupera la agrupación de forma perfecta. Aunque la implementación de clustering aglomerativo en scikit-learn exige especificar de antemano el número de clústeres que debe encontrar, existen métodos que nos ayudan a elegir dicho número, como veremos a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dwukGCPtq0M"
   },
   "source": [
    "### **Clustering jerárquico y dendrograma**\n",
    "\n",
    "El clustering aglomerativo genera lo que llamamos un **clustering jerárquico**, en el que cada punto inicia formando su propio clúster y, paso a paso, se va incorporando a grupos cada vez más grandes hasta llegar a la partición final. En cada iteración obtenemos una partición intermedia con distinto número de clústeres, que refleja hasta qué nivel de detalle estamos observando la estructura de los datos.\n",
    "\n",
    "Para visualizar de un solo vistazo todas esas fusiones sucesivas resulta muy útil el **dendrograma**, un diagrama en forma de árbol invertido que muestra en el eje vertical la distancia (o disimilitud) a la que se unen los clústeres y, en el horizontal, los elementos o subgrupos resultantes. De este modo, podemos identificar de forma intuitiva:\n",
    "\n",
    "* **El umbral de corte**: basta trazar una línea horizontal a la altura deseada para elegir el número de clústeres que nos interese.\n",
    "* **La cohesión interna**: la altura de las ramas del dendrograma indica qué tan similares eran dos clústeres antes de fusionarse; fusiones a menor altura significan grupos muy homogéneos.\n",
    "* **La estabilidad de los grupos**: clústeres que permanecen unidos hasta niveles altos del dendrograma son más robustos frente a variaciones en la métrica de distancia.\n",
    "\n",
    "En conjunto, el dendrograma no solo facilita la determinación del número óptimo de clústeres, sino que también proporciona una visión clara de la estructura jerárquica y de la forma en que los datos se agrupan a distintas escalas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvcc7SNktq0O"
   },
   "outputs": [],
   "source": [
    "# Asignación de cluster jerarquico (mostrado como lineas) generado con clustering aglomerativo, con puntos de datos numerados\n",
    "def dibuja_cluster_aglomerativo():\n",
    "    X, y = make_blobs(random_state=0, n_samples=12)\n",
    "    agg = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "    eps = X.std() / 2.\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n",
    "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    puntos_grid = np.c_[xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)]\n",
    "\n",
    "    ax = plt.gca()\n",
    "    for i, x in enumerate(X):\n",
    "        ax.text(x[0] + .1, x[1], \"%d\" % i, horizontalalignment='left', verticalalignment='center')\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=60, c='grey')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "    for i in range(11):\n",
    "        agg.n_clusters = X.shape[0] - i\n",
    "        agg.fit(X)\n",
    "\n",
    "        bins = np.bincount(agg.labels_)\n",
    "        for cluster in range(agg.n_clusters):\n",
    "            if bins[cluster] > 1:\n",
    "                puntos = X[agg.labels_ == cluster]\n",
    "                otros_puntos = X[agg.labels_ != cluster]\n",
    "\n",
    "                kde = KernelDensity(bandwidth=.5).fit(puntos)\n",
    "                puntuaciones = kde.score_samples(puntos_grid)\n",
    "                puntuaciones1 = np.min(kde.score_samples(puntos))\n",
    "                puntuaciones2 = np.max(kde.score_samples(otros_puntos))\n",
    "                niveles = .8 * puntuaciones1 + .2 * puntuaciones2\n",
    "                ax.contour(xx, yy, puntuaciones.reshape(100, 100), levels=[niveles],\n",
    "                           colors='k', linestyles='solid', linewidths=1)\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pezatyuAtq0a",
    "outputId": "16ca4b5f-7aa4-4a74-8707-eb972c4b1980"
   },
   "outputs": [],
   "source": [
    "dibuja_cluster_aglomerativo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GY3wZghtq0j"
   },
   "source": [
    "Aunque esta visualización ofrece un detalle muy fino del clustering jerárquico, se apoya en la representación bidimensional de los datos y, por tanto, no resulta válida cuando tenemos más de dos variables. Sin embargo, existe otra forma de visualizar la estructura jerárquica que sí admite datos multidimensionales: el dendrograma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQJofbaOtq0l"
   },
   "source": [
    "Los algoritmos de clustering de SciPy utilizan una interfaz ligeramente distinta a la de scikit-learn y están diseñados, entre otras cosas, para generar dendrogramas. SciPy ofrece una función que recibe la matriz de datos `X` y devuelve una **matriz de enlaces** (linkage matrix), en la que se codifica la similitud jerárquica entre clústeres. Seguidamente, esa matriz de enlaces se pasa a la función `dendrogram` de SciPy para dibujar el dendrograma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "294XV7zGtq0n",
    "outputId": "0bb7688f-a644-46e9-847b-dfa2c12318c0"
   },
   "outputs": [],
   "source": [
    "# Importamos la funcion  dendrogram  y la funcion ward desde SciPy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "# Aplique el clustering  ward a la matriz de datos X. \n",
    "matriz_enlace = ward(X)\n",
    "\n",
    "# Graficamos el dendrograma para la matriz enlace que contiene las distancias entre clusters\n",
    "dendrogram(matriz_enlace)\n",
    "\n",
    "# Marca los cortes en el árbol que significan dos o tres grupos\n",
    "\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
    "ax.plot(bounds, [4, 4], '--', c='k')\n",
    "ax.text(bounds[1], 7.25, ' dos clusters', va='center', fontdict={'size': 15})\n",
    "ax.text(bounds[1], 4, ' tres clusters', va='center', fontdict={'size': 15})\n",
    "plt.xlabel(\"Indice de muestras\")\n",
    "plt.ylabel(\"Distancia de clusters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMNlwbHdtq0v"
   },
   "source": [
    "El dendrograma muestra los puntos de datos como hojas numeradas de 0 a 11 en la base, y sobre ellas se construye un árbol en el que cada fusión genera un nodo padre que une dos clústeres. El eje vertical (`y`) indica tanto el orden de fusiones en el algoritmo como la distancia entre los clústeres fusionados: cuanto más larga es una rama, mayor es la separación original entre esos grupos.\n",
    "\n",
    "En este ejemplo, las tres ramas más largas (marcadas con la línea discontinua y la etiqueta \"tres grupos\") señalan que pasar de tres a dos clústeres implica unir subgrupos muy distantes. Esta idea se refuerza en la parte superior del dendrograma, donde la última fusión, la que consolida los dos clústeres restantes en uno solo ocurre a una distancia especialmente elevada.\n",
    "\n",
    "A pesar de su utilidad para visualizar estructuras jerárquicas, la agrupación aglomerativa puede fallar al separar formas complejas, como el conjunto de datos `two_moons`. Para esos casos, algoritmos basados en densidad, como DBSCAN, suelen ofrecer resultados mucho mejores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP6uol4Ttq0x"
   },
   "source": [
    "### **DBSCAN**\n",
    "\n",
    "DBSCAN (del inglés *Density-Based Spatial Clustering of Applications with Noise*) es un algoritmo de clustering que no exige especificar de antemano el número de clústeres. Además, es capaz de detectar grupos de formas arbitrarias y de identificar puntos aislados (\"ruido\") que no pertenecen a ningún clúster. Aunque suele ser algo más lento que K-means o el clustering aglomerativo, escala razonablemente bien a conjuntos de datos de tamaño medio.\n",
    "\n",
    "La idea central de DBSCAN es que los clústeres son regiones densas de puntos, separadas por zonas de baja densidad. Para ello define dos parámetros:\n",
    "\n",
    "* `eps`: radio de búsqueda, la distancia máxima para considerar dos puntos vecinos.\n",
    "* `min_samples`: número mínimo de puntos que debe haber dentro de la bola de radio `eps` para que un punto se considere “central”.\n",
    "\n",
    "Con estos parámetros, DBSCAN clasifica cada punto en uno de tres tipos:\n",
    "\n",
    "1. **Core sample** (punto central)\n",
    "   Tiene al menos `min_samples` vecinos a distancia ≤ `eps`.\n",
    "2. **Border point** (punto frontera)\n",
    "   No alcanza `min_samples` vecinos por sí solo, pero está dentro de `eps` de algún core sample.\n",
    "3. **Noise** (ruido)\n",
    "   No es ni core sample ni frontera; no pertenece a ningún clúster.\n",
    "\n",
    "**Proceso de DBSCAN**\n",
    "\n",
    "1. Se elige un punto no visitado.\n",
    "2. Se cuentan sus vecinos (distancia ≤ `eps`).\n",
    "\n",
    "   * Si son < `min_samples`, se etiqueta como ruido.\n",
    "   * Si son ≥ `min_samples`, se crea un nuevo clúster y se etiqueta como core sample.\n",
    "3. Se expanden los clústeres recorriendo recursivamente los vecinos de cada core sample:\n",
    "\n",
    "   * A cada vecino no asignado se le pone la etiqueta del clúster.\n",
    "   * Si ese vecino también es core sample, se analizan sus vecinos, y así hasta agotar la expansión.\n",
    "4. Se repite el proceso con otro punto no visitado hasta cubrir todo el conjunto.\n",
    "\n",
    "Al final obtenemos clústeres que reflejan la densidad real de los datos y un conjunto de puntos etiquetados como ruido. Ten en cuenta que, aunque la clasificación de los core samples y del ruido es determinista, la asignación de los puntos frontera puede depender del orden en que se exploran los vecinos. Sin embargo, en la práctica esto suele afectar solo a un pequeño número de puntos y rara vez altera de forma significativa la partición final.\n",
    "\n",
    "En scikit-learn, la clase `DBSCAN` no implementa un método `predict` para nuevos datos; en su lugar se utiliza `fit_predict(X)` para ajustar y etiquetar en un solo paso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDyORG5xtq01",
    "outputId": "9afc3e42-0ea1-49d4-ddc8-aa834f94adf6"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(\"Miembros de los clustering:\\n{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88vYUwpmtq0-"
   },
   "source": [
    "Todos los puntos de datos se les asignó la etiqueta `-1`, que significa ruido. Esto es una consecuencia de los ajustes de parámetros predeterminados para `eps` y `min_samples`, que no están ajustados para conjuntos de datos pequeños. \n",
    "\n",
    "Las asignaciones de clúster para diferentes valores de `min_samples` y `eps` se muestran a continuación y se visualizan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OO5W2XyDtq1A"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cm3 = ListedColormap(['#0000aa', '#ff2020', '#50ff50'])\n",
    "cm2 = ListedColormap(['#0000aa', '#ff2020'])\n",
    "\n",
    "\n",
    "def dibuja_dbscan():\n",
    "    X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "    dbscan = DBSCAN()\n",
    "    clusters = dbscan.fit_predict(X)\n",
    "    clusters\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(11, 8),\n",
    "                             subplot_kw={'xticks': (), 'yticks': ()})\n",
    "    \n",
    "    # dibujamos  los clústeres como rojo, verde y azul, y los valores atípicos (-1) con blanco\n",
    "\n",
    "    colores = [cm3(1), cm3(0), cm3(2)]\n",
    "    marcadores = ['o', '^', 'v']\n",
    "\n",
    "    # iterar sobre la configuración de min_samples y eps\n",
    "    for i, min_samples in enumerate([2, 3, 5]):\n",
    "        for j, eps in enumerate([1, 1.5, 2, 3]):\n",
    "            # instanciado  DBSCAN con una configuracion particular\n",
    "            dbscan = DBSCAN(min_samples=min_samples, eps=eps)\n",
    "            # obtenemos asignacion de cluster\n",
    "            clusters = dbscan.fit_predict(X)\n",
    "            print(\"min_samples: %d eps: %f  cluster: %s\"\n",
    "                  % (min_samples, eps, clusters))\n",
    "            if np.any(clusters == -1):\n",
    "                c = ['w'] + colores\n",
    "                m = ['o'] + marcadores\n",
    "            else:\n",
    "                c = colores\n",
    "                m = marcadores\n",
    "            dibuja_dispersion_discreta(X[:, 0], X[:, 1], clusters, ax=axes[i, j], c=c,\n",
    "                             s=8, marcadores=m)\n",
    "            inds = dbscan.core_sample_indices_\n",
    "            # vizualizamos los  core samples y clusters.\n",
    "            if len(inds):\n",
    "                dibuja_dispersion_discreta(X[inds, 0], X[inds, 1], clusters[inds], ax=axes[i, j], s=15, c=colores,\n",
    "                                 marcadores=marcadores)\n",
    "            axes[i, j].set_title(\"min_samples: %d eps: %.1f\"\n",
    "                                 % (min_samples, eps))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SmJYQpJtq1I",
    "outputId": "c37b65d8-2057-440c-cada-69d14890fc91"
   },
   "outputs": [],
   "source": [
    "dibuja_dbscan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1oVdCWVtq1P"
   },
   "source": [
    "En este gráfico, los puntos que pertenecen a los clústeres aparecen rellenos, mientras que los de ruido se muestran en blanco. Los *core samples* se dibujan con marcadores grandes y los puntos frontera con marcadores más pequeños. Al aumentar el valor de `eps` (de izquierda a derecha), más puntos entran en la vecindad de cada core sample, lo que hace crecer los clústeres, aunque también puede provocar que varios se fusionen en uno solo. Por otro lado, incrementar `min_samples` (de arriba abajo) exige un mayor número de vecinos para definir un core sample, de modo que menos puntos se clasifican como centrales y crece la cantidad de ruido.\n",
    "\n",
    "El parámetro `eps` es crucial porque determina qué significa \"cercanía\": un `eps` demasiado pequeño hará que ningún punto alcance el umbral de core sample y que casi todos acaben etiquetados como ruido; un `eps` muy grande, en cambio, tenderá a agrupar todos los puntos en un único clúster.\n",
    "\n",
    "Mientras tanto, `min_samples` regula el tamaño mínimo de los clústeres: valores mayores convierten en ruido los puntos en regiones menos densas, y valores menores permiten formar clústeres más pequeños antes de considerarlos ruido.\n",
    "\n",
    "Aunque DBSCAN no requiere especificar el número de clústeres, la elección de `eps` influye indirectamente en cuántos se detectan. Por ello, resulta habitual escalar primero los datos con `StandardScaler` o `MinMaxScaler`, de modo que todas las características compartan un rango similar y resulte más fácil encontrar un valor de `eps` adecuado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PUo_WS3tq1S",
    "outputId": "f21dad93-26df-4c00-f4c4-e720723f7f6a"
   },
   "outputs": [],
   "source": [
    "# Corremos DBSCAN con el conjunto de datos two_moons\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# rescalamos  los datos a una media cero y varianza 1 \n",
    "escalador = StandardScaler()\n",
    "escalador.fit(X)\n",
    "X_escalado = escalador.transform(X)\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X_escalado)\n",
    "\n",
    "# dibujamos el asignamiento de clusters\n",
    "plt.scatter(X_escalado[:, 0], X_escalado[:, 1], c=clusters, cmap=cm2, s=60)\n",
    "plt.xlabel(\"Caracteristica 0\")\n",
    "plt.ylabel(\"Caracteristica 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVpNdywztq1c"
   },
   "source": [
    "Como el algoritmo produjo el número deseado de clústeres (dos), la configuración de parámetros parece adecuada. Si reducimos `eps` a 0.2 (frente al valor por defecto de 0.5), obtenemos ocho clústeres, lo cual es excesivo. Por el contrario, aumentar `eps` a 0.7 da como resultado un único clúster.\n",
    "\n",
    "Al usar DBSCAN, hay que prestar atención al manejo de las etiquetas devueltas. DBSCAN emplea `-1` para indicar ruido, y esto puede causar comportamientos inesperados si usamos esas etiquetas para indexar otras estructuras (como arrays o DataFrames).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Clustering-ensamblado.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
